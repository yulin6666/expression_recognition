{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "882 99\n",
      "882 99\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.886 | Acc: 24.000% (212/882)        7/7 \n",
      "mainpro_CK+_mobilenetv2.py:149: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
      " [============================>.] | Loss: 1.879 | Acc: 21.000% (21/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 21.000\n",
      "\n",
      "Epoch: 1\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.835 | Acc: 27.000% (242/882)        7/7 \n",
      " [============================>.] | Loss: 1.874 | Acc: 21.000% (21/99)          20/20 \n",
      "\n",
      "Epoch: 2\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.799 | Acc: 30.000% (270/882)        7/7 \n",
      " [============================>.] | Loss: 1.907 | Acc: 12.000% (12/99)          20/20 \n",
      "\n",
      "Epoch: 3\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.709 | Acc: 36.000% (319/882)        7/7 \n",
      " [============================>.] | Loss: 1.893 | Acc: 18.000% (18/99)          20/20 \n",
      "\n",
      "Epoch: 4\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.481 | Acc: 44.000% (393/882)        7/7 \n",
      " [============================>.] | Loss: 1.886 | Acc: 24.000% (24/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 24.000\n",
      "\n",
      "Epoch: 5\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.334 | Acc: 48.000% (429/882)        7/7 \n",
      " [============================>.] | Loss: 1.984 | Acc: 21.000% (21/99)          20/20 \n",
      "\n",
      "Epoch: 6\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.210 | Acc: 53.000% (475/882)        7/7 \n",
      " [============================>.] | Loss: 1.966 | Acc: 18.000% (18/99)          20/20 \n",
      "\n",
      "Epoch: 7\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.046 | Acc: 59.000% (528/882)        7/7 \n",
      " [============================>.] | Loss: 1.984 | Acc: 21.000% (21/99)          20/20 \n",
      "\n",
      "Epoch: 8\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.921 | Acc: 66.000% (586/882)        7/7 \n",
      " [============================>.] | Loss: 1.881 | Acc: 34.000% (34/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 34.000\n",
      "\n",
      "Epoch: 9\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.781 | Acc: 70.000% (620/882)        7/7 \n",
      " [============================>.] | Loss: 1.809 | Acc: 28.000% (28/99)          20/20 \n",
      "\n",
      "Epoch: 10\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.726 | Acc: 74.000% (659/882)        7/7 \n",
      " [============================>.] | Loss: 1.808 | Acc: 39.000% (39/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 39.000\n",
      "\n",
      "Epoch: 11\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.495 | Acc: 81.000% (723/882)        7/7 \n",
      " [============================>.] | Loss: 1.692 | Acc: 44.000% (44/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 44.000\n",
      "\n",
      "Epoch: 12\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.401 | Acc: 85.000% (754/882)        7/7 \n",
      " [============================>.] | Loss: 1.814 | Acc: 40.000% (40/99)          20/20 \n",
      "\n",
      "Epoch: 13\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.327 | Acc: 89.000% (793/882)        7/7 \n",
      " [============================>.] | Loss: 2.207 | Acc: 42.000% (42/99)          20/20 \n",
      "\n",
      "Epoch: 14\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.380 | Acc: 87.000% (771/882)        7/7 \n",
      " [============================>.] | Loss: 1.529 | Acc: 52.000% (52/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 52.000\n",
      "\n",
      "Epoch: 15\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.290 | Acc: 89.000% (785/882)        7/7 \n",
      " [============================>.] | Loss: 0.890 | Acc: 66.000% (66/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 66.000\n",
      "\n",
      "Epoch: 16\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.226 | Acc: 92.000% (814/882)        7/7 \n",
      " [============================>.] | Loss: 1.531 | Acc: 60.000% (60/99)          20/20 \n",
      "\n",
      "Epoch: 17\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.182 | Acc: 93.000% (827/882)        7/7 \n",
      " [============================>.] | Loss: 1.093 | Acc: 68.000% (68/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 68.000\n",
      "\n",
      "Epoch: 18\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.158 | Acc: 94.000% (835/882)        7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 74.000% (74/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 74.000\n",
      "\n",
      "Epoch: 19\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.173 | Acc: 93.000% (827/882)        7/7 \n",
      " [============================>.] | Loss: 1.145 | Acc: 63.000% (63/99)          20/20 \n",
      "\n",
      "Epoch: 20\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.134 | Acc: 95.000% (839/882)        7/7 \n",
      " [============================>.] | Loss: 0.892 | Acc: 80.000% (80/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 80.000\n",
      "\n",
      "Epoch: 21\n",
      "learning_rate: 0.008\n",
      " [=========================>....] | Loss: 0.112 | Acc: 96.000% (848/882)        7/7 \n",
      " [============================>.] | Loss: 0.842 | Acc: 76.000% (76/99)          20/20 \n",
      "\n",
      "Epoch: 22\n",
      "learning_rate: 0.006400000000000001\n",
      " [=========================>....] | Loss: 0.087 | Acc: 96.000% (855/882)        7/7 \n",
      " [============================>.] | Loss: 0.841 | Acc: 76.000% (76/99)          20/20 \n",
      "\n",
      "Epoch: 23\n",
      "learning_rate: 0.005120000000000001\n",
      " [=========================>....] | Loss: 0.063 | Acc: 97.000% (864/882)        7/7 \n",
      " [============================>.] | Loss: 0.848 | Acc: 75.000% (75/99)          20/20 \n",
      "\n",
      "Epoch: 24\n",
      "learning_rate: 0.004096000000000001\n",
      " [=========================>....] | Loss: 0.039 | Acc: 98.000% (873/882)        7/7 \n",
      " [============================>.] | Loss: 0.836 | Acc: 73.000% (73/99)          20/20 \n",
      "\n",
      "Epoch: 25\n",
      "learning_rate: 0.0032768000000000007\n",
      " [=========================>....] | Loss: 0.021 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.817 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 26\n",
      "learning_rate: 0.002621440000000001\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.833 | Acc: 69.000% (69/99)          20/20 \n",
      "\n",
      "Epoch: 27\n",
      "learning_rate: 0.002097152000000001\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.821 | Acc: 70.000% (70/99)          20/20 \n",
      "\n",
      "Epoch: 28\n",
      "learning_rate: 0.001677721600000001\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.832 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 29\n",
      "learning_rate: 0.0013421772800000006\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.831 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 30\n",
      "learning_rate: 0.0010737418240000006\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.841 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 31\n",
      "learning_rate: 0.0008589934592000006\n",
      " [=========================>....] | Loss: 0.012 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.816 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 32\n",
      "learning_rate: 0.0006871947673600004\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.819 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 33\n",
      "learning_rate: 0.0005497558138880004\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 34\n",
      "learning_rate: 0.00043980465111040037\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 35\n",
      "learning_rate: 0.0003518437208883203\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 36\n",
      "learning_rate: 0.00028147497671065624\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 37\n",
      "learning_rate: 0.00022517998136852504\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 38\n",
      "learning_rate: 0.00018014398509482002\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 39\n",
      "learning_rate: 0.00014411518807585602\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 40\n",
      "learning_rate: 0.00011529215046068484\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 41\n",
      "learning_rate: 9.223372036854788e-05\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.790 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 42\n",
      "learning_rate: 7.37869762948383e-05\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 43\n",
      "learning_rate: 5.902958103587064e-05\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 44\n",
      "learning_rate: 4.722366482869652e-05\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 45\n",
      "learning_rate: 3.777893186295722e-05\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.796 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 46\n",
      "learning_rate: 3.0223145490365776e-05\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 70.000% (70/99)          20/20 \n",
      "\n",
      "Epoch: 47\n",
      "learning_rate: 2.417851639229262e-05\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 48\n",
      "learning_rate: 1.9342813113834096e-05\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.795 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 49\n",
      "learning_rate: 1.547425049106728e-05\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.802 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 50\n",
      "learning_rate: 1.2379400392853824e-05\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 51\n",
      "learning_rate: 9.903520314283058e-06\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.812 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 52\n",
      "learning_rate: 7.922816251426448e-06\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 53\n",
      "learning_rate: 6.338253001141158e-06\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 54\n",
      "learning_rate: 5.0706024009129275e-06\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 55\n",
      "learning_rate: 4.056481920730342e-06\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.813 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 56\n",
      "learning_rate: 3.2451855365842735e-06\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 57\n",
      "learning_rate: 2.5961484292674196e-06\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.816 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 58\n",
      "learning_rate: 2.0769187434139356e-06\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.817 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 59\n",
      "learning_rate: 1.6615349947311485e-06\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.810 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 60\n",
      "learning_rate: 1.3292279957849189e-06\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 61\n",
      "learning_rate: 1.0633823966279351e-06\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 62\n",
      "learning_rate: 8.507059173023481e-07\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.812 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 63\n",
      "learning_rate: 6.805647338418786e-07\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 64\n",
      "learning_rate: 5.444517870735029e-07\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 65\n",
      "learning_rate: 4.3556142965880233e-07\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 66\n",
      "learning_rate: 3.484491437270419e-07\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 67\n",
      "learning_rate: 2.787593149816335e-07\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 68\n",
      "learning_rate: 2.2300745198530684e-07\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 69\n",
      "learning_rate: 1.784059615882455e-07\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 70\n",
      "learning_rate: 1.4272476927059639e-07\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 71\n",
      "learning_rate: 1.1417981541647711e-07\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 72\n",
      "learning_rate: 9.13438523331817e-08\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 73\n",
      "learning_rate: 7.307508186654536e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 74\n",
      "learning_rate: 5.846006549323629e-08\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 75\n",
      "learning_rate: 4.676805239458904e-08\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 76\n",
      "learning_rate: 3.741444191567123e-08\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 77\n",
      "learning_rate: 2.9931553532536985e-08\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 78\n",
      "learning_rate: 2.3945242826029592e-08\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.810 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 79\n",
      "learning_rate: 1.9156194260823674e-08\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.813 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 80\n",
      "learning_rate: 1.532495540865894e-08\n",
      " [=========================>....] | Loss: 0.014 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 81\n",
      "learning_rate: 1.2259964326927151e-08\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 82\n",
      "learning_rate: 9.807971461541723e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 83\n",
      "learning_rate: 7.846377169233378e-09\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 84\n",
      "learning_rate: 6.277101735386703e-09\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 85\n",
      "learning_rate: 5.0216813883093625e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 86\n",
      "learning_rate: 4.017345110647491e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 87\n",
      "learning_rate: 3.2138760885179924e-09\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.793 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 88\n",
      "learning_rate: 2.5711008708143944e-09\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 89\n",
      "learning_rate: 2.0568806966515157e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.802 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 90\n",
      "learning_rate: 1.6455045573212124e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 91\n",
      "learning_rate: 1.31640364585697e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 92\n",
      "learning_rate: 1.0531229166855762e-09\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 93\n",
      "learning_rate: 8.42498333348461e-10\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.802 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 94\n",
      "learning_rate: 6.739986666787687e-10\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 95\n",
      "learning_rate: 5.39198933343015e-10\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 96\n",
      "learning_rate: 4.3135914667441205e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 97\n",
      "learning_rate: 3.450873173395297e-10\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.792 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 98\n",
      "learning_rate: 2.7606985387162373e-10\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.790 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 99\n",
      "learning_rate: 2.2085588309729901e-10\n",
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 100\n",
      "learning_rate: 1.7668470647783923e-10\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 101\n",
      "learning_rate: 1.413477651822714e-10\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 102\n",
      "learning_rate: 1.130782121458171e-10\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 103\n",
      "learning_rate: 9.04625697166537e-11\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 104\n",
      "learning_rate: 7.237005577332295e-11\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 105\n",
      "learning_rate: 5.789604461865837e-11\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 106\n",
      "learning_rate: 4.63168356949267e-11\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 107\n",
      "learning_rate: 3.7053468555941365e-11\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 108\n",
      "learning_rate: 2.964277484475309e-11\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.802 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 109\n",
      "learning_rate: 2.3714219875802474e-11\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 110\n",
      "learning_rate: 1.8971375900641982e-11\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 111\n",
      "learning_rate: 1.5177100720513584e-11\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 112\n",
      "learning_rate: 1.2141680576410869e-11\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 113\n",
      "learning_rate: 9.713344461128697e-12\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 114\n",
      "learning_rate: 7.770675568902958e-12\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.796 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 115\n",
      "learning_rate: 6.216540455122366e-12\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 116\n",
      "learning_rate: 4.973232364097893e-12\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.814 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 117\n",
      "learning_rate: 3.978585891278314e-12\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 118\n",
      "learning_rate: 3.182868713022652e-12\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.822 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 119\n",
      "learning_rate: 2.5462949704181215e-12\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 120\n",
      "learning_rate: 2.0370359763344977e-12\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 121\n",
      "learning_rate: 1.629628781067598e-12\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 122\n",
      "learning_rate: 1.3037030248540785e-12\n",
      " [=========================>....] | Loss: 0.004 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 123\n",
      "learning_rate: 1.0429624198832629e-12\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 124\n",
      "learning_rate: 8.343699359066104e-13\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 125\n",
      "learning_rate: 6.674959487252882e-13\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.795 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 126\n",
      "learning_rate: 5.339967589802307e-13\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 127\n",
      "learning_rate: 4.2719740718418454e-13\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 128\n",
      "learning_rate: 3.4175792574734765e-13\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 129\n",
      "learning_rate: 2.7340634059787813e-13\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 130\n",
      "learning_rate: 2.1872507247830254e-13\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 131\n",
      "learning_rate: 1.7498005798264204e-13\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 132\n",
      "learning_rate: 1.3998404638611363e-13\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.793 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 133\n",
      "learning_rate: 1.1198723710889092e-13\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 134\n",
      "learning_rate: 8.958978968711274e-14\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 135\n",
      "learning_rate: 7.167183174969019e-14\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 136\n",
      "learning_rate: 5.733746539975217e-14\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 137\n",
      "learning_rate: 4.5869972319801734e-14\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 138\n",
      "learning_rate: 3.669597785584139e-14\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.824 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 139\n",
      "learning_rate: 2.935678228467311e-14\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 140\n",
      "learning_rate: 2.3485425827738488e-14\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 141\n",
      "learning_rate: 1.878834066219079e-14\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 142\n",
      "learning_rate: 1.5030672529752636e-14\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 70.000% (70/99)          20/20 \n",
      "\n",
      "Epoch: 143\n",
      "learning_rate: 1.2024538023802108e-14\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.810 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 144\n",
      "learning_rate: 9.619630419041687e-15\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 145\n",
      "learning_rate: 7.69570433523335e-15\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.795 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 146\n",
      "learning_rate: 6.15656346818668e-15\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 147\n",
      "learning_rate: 4.925250774549344e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 148\n",
      "learning_rate: 3.940200619639476e-15\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.817 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 149\n",
      "learning_rate: 3.1521604957115808e-15\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.813 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 150\n",
      "learning_rate: 2.521728396569265e-15\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 151\n",
      "learning_rate: 2.017382717255412e-15\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 152\n",
      "learning_rate: 1.6139061738043298e-15\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.786 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 153\n",
      "learning_rate: 1.2911249390434638e-15\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.790 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 154\n",
      "learning_rate: 1.0328999512347711e-15\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 70.000% (70/99)          20/20 \n",
      "\n",
      "Epoch: 155\n",
      "learning_rate: 8.263199609878169e-16\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 156\n",
      "learning_rate: 6.610559687902537e-16\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.810 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 157\n",
      "learning_rate: 5.288447750322029e-16\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.810 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 158\n",
      "learning_rate: 4.2307582002576235e-16\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 159\n",
      "learning_rate: 3.384606560206099e-16\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 160\n",
      "learning_rate: 2.7076852481648796e-16\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.815 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 161\n",
      "learning_rate: 2.1661481985319035e-16\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.819 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 162\n",
      "learning_rate: 1.732918558825523e-16\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.824 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 163\n",
      "learning_rate: 1.3863348470604184e-16\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.814 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 164\n",
      "learning_rate: 1.1090678776483348e-16\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.814 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 165\n",
      "learning_rate: 8.87254302118668e-17\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 166\n",
      "learning_rate: 7.098034416949344e-17\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 167\n",
      "learning_rate: 5.678427533559476e-17\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 168\n",
      "learning_rate: 4.5427420268475807e-17\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.792 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 169\n",
      "learning_rate: 3.6341936214780644e-17\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 170\n",
      "learning_rate: 2.907354897182452e-17\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 171\n",
      "learning_rate: 2.3258839177459616e-17\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.802 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 172\n",
      "learning_rate: 1.8607071341967693e-17\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 173\n",
      "learning_rate: 1.4885657073574156e-17\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.792 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 174\n",
      "learning_rate: 1.1908525658859325e-17\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.793 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 175\n",
      "learning_rate: 9.52682052708746e-18\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 176\n",
      "learning_rate: 7.62145642166997e-18\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 177\n",
      "learning_rate: 6.0971651373359754e-18\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 178\n",
      "learning_rate: 4.877732109868781e-18\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 179\n",
      "learning_rate: 3.902185687895025e-18\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 180\n",
      "learning_rate: 3.12174855031602e-18\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.795 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 181\n",
      "learning_rate: 2.497398840252816e-18\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 182\n",
      "learning_rate: 1.997919072202253e-18\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 183\n",
      "learning_rate: 1.5983352577618025e-18\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 184\n",
      "learning_rate: 1.278668206209442e-18\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 185\n",
      "learning_rate: 1.0229345649675538e-18\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.792 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 186\n",
      "learning_rate: 8.18347651974043e-19\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 187\n",
      "learning_rate: 6.546781215792345e-19\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 188\n",
      "learning_rate: 5.237424972633876e-19\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.790 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 189\n",
      "learning_rate: 4.189939978107101e-19\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 190\n",
      "learning_rate: 3.3519519824856807e-19\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 191\n",
      "learning_rate: 2.681561585988545e-19\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 192\n",
      "learning_rate: 2.145249268790836e-19\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.781 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 193\n",
      "learning_rate: 1.7161994150326688e-19\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.793 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 194\n",
      "learning_rate: 1.3729595320261352e-19\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 195\n",
      "learning_rate: 1.098367625620908e-19\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 196\n",
      "learning_rate: 8.786941004967267e-20\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.797 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 197\n",
      "learning_rate: 7.029552803973813e-20\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 198\n",
      "learning_rate: 5.623642243179051e-20\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 199\n",
      "learning_rate: 4.498913794543241e-20\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.793 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 200\n",
      "learning_rate: 3.5991310356345934e-20\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 201\n",
      "learning_rate: 2.8793048285076746e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.803 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 202\n",
      "learning_rate: 2.30344386280614e-20\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 203\n",
      "learning_rate: 1.8427550902449122e-20\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.788 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 204\n",
      "learning_rate: 1.4742040721959298e-20\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 205\n",
      "learning_rate: 1.1793632577567439e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.818 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 206\n",
      "learning_rate: 9.43490606205395e-21\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 207\n",
      "learning_rate: 7.547924849643161e-21\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.796 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 208\n",
      "learning_rate: 6.0383398797145295e-21\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.804 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 209\n",
      "learning_rate: 4.8306719037716234e-21\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.801 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 210\n",
      "learning_rate: 3.864537523017299e-21\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.792 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 211\n",
      "learning_rate: 3.0916300184138396e-21\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 212\n",
      "learning_rate: 2.4733040147310717e-21\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.815 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 213\n",
      "learning_rate: 1.9786432117848575e-21\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.822 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 214\n",
      "learning_rate: 1.5829145694278862e-21\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.810 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 215\n",
      "learning_rate: 1.266331655542309e-21\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.815 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 216\n",
      "learning_rate: 1.0130653244338472e-21\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 217\n",
      "learning_rate: 8.104522595470779e-22\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 218\n",
      "learning_rate: 6.483618076376623e-22\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 219\n",
      "learning_rate: 5.186894461101298e-22\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.792 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 220\n",
      "learning_rate: 4.149515568881039e-22\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 221\n",
      "learning_rate: 3.3196124551048316e-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.787 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 222\n",
      "learning_rate: 2.6556899640838656e-22\n",
      " [=========================>....] | Loss: 0.010 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 223\n",
      "learning_rate: 2.1245519712670924e-22\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.816 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 224\n",
      "learning_rate: 1.699641577013674e-22\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.813 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 225\n",
      "learning_rate: 1.3597132616109392e-22\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 226\n",
      "learning_rate: 1.0877706092887513e-22\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 227\n",
      "learning_rate: 8.702164874310012e-23\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 228\n",
      "learning_rate: 6.96173189944801e-23\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.789 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 229\n",
      "learning_rate: 5.569385519558409e-23\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 230\n",
      "learning_rate: 4.455508415646727e-23\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.795 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 231\n",
      "learning_rate: 3.5644067325173816e-23\n",
      " [=========================>....] | Loss: 0.015 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 232\n",
      "learning_rate: 2.851525386013906e-23\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.802 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 233\n",
      "learning_rate: 2.2812203088111247e-23\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.796 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 234\n",
      "learning_rate: 1.8249762470489e-23\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 235\n",
      "learning_rate: 1.45998099763912e-23\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.814 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 236\n",
      "learning_rate: 1.167984798111296e-23\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.809 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 237\n",
      "learning_rate: 9.343878384890368e-24\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.794 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 238\n",
      "learning_rate: 7.475102707912296e-24\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.800 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 239\n",
      "learning_rate: 5.9800821663298366e-24\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.808 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 240\n",
      "learning_rate: 4.784065733063869e-24\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.799 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 241\n",
      "learning_rate: 3.827252586451096e-24\n",
      " [=========================>....] | Loss: 0.005 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.805 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 242\n",
      "learning_rate: 3.061802069160877e-24\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.813 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 243\n",
      "learning_rate: 2.4494416553287016e-24\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.813 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 244\n",
      "learning_rate: 1.9595533242629614e-24\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 71.000% (71/99)          20/20 \n",
      "\n",
      "Epoch: 245\n",
      "learning_rate: 1.5676426594103693e-24\n",
      " [=========================>....] | Loss: 0.003 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.806 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 246\n",
      "learning_rate: 1.2541141275282953e-24\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.791 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 247\n",
      "learning_rate: 1.0032913020226365e-24\n",
      " [=========================>....] | Loss: 0.005 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.796 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 248\n",
      "learning_rate: 8.026330416181091e-25\n",
      " [=========================>....] | Loss: 0.004 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.812 | Acc: 72.000% (72/99)          20/20 \n",
      "\n",
      "Epoch: 249\n",
      "learning_rate: 6.421064332944874e-25\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.000% (72/99)          20/20 \n",
      "best_Test_acc: 80.000\n",
      "best_Test_acc_epoch: 20\n"
     ]
    }
   ],
   "source": [
    "!python mainpro_CK+_mobilenetv2.py --model mobilenetv2 --bs 128 --lr 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin      4.570\n",
      "origin      1.976\n",
      "origin      2.724\n",
      "origin     -0.200\n",
      "origin     -0.855\n",
      "origin     -2.763\n",
      "origin     -5.388\n",
      "     0.802\n",
      "     0.060\n",
      "     0.127\n",
      "     0.007\n",
      "     0.004\n",
      "     0.001\n",
      "     0.000\n",
      "The Expression is Angry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\"\"\"\n",
    "visualize results for test image\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenet_v1 import mobilenet, MobileNet, mobilenet_05\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "img = io.imread('images/anger_rgb.png')\n",
    "# img = raw_img[:, :, np.newaxis]\n",
    "# img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "img.save('images/anger_rgb_32.png')\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "net = mobilenetv2(num_classes=7,input_size=32)\n",
    "checkpoint = torch.load(os.path.join('CK+_mobilenetv2/1/', 'Test_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1, c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "outputs = net(inputs)\n",
    "for i in range(7):\n",
    "  print('origin %10.3f' % outputs[0][i])\n",
    "\n",
    "score = F.softmax(outputs,1)\n",
    "max = score[0][0]\n",
    "maxindex = 0\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "  if(score[0][i] > max):\n",
    "        max = score[0][i]\n",
    "        maxindex = i\n",
    "print(\"The Expression is %s\" %str(class_names[maxindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (1.5.0)\r\n",
      "Requirement already satisfied: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (41.0.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%data : Float(1, 3, 32, 32),\n",
      "      %1 : Float(32, 3, 3, 3),\n",
      "      %2 : Float(32),\n",
      "      %3 : Float(32),\n",
      "      %4 : Float(32),\n",
      "      %5 : Float(32),\n",
      "      %6 : Long(),\n",
      "      %7 : Float(32, 1, 3, 3),\n",
      "      %8 : Float(32),\n",
      "      %9 : Float(32),\n",
      "      %10 : Float(32),\n",
      "      %11 : Float(32),\n",
      "      %12 : Long(),\n",
      "      %13 : Float(16, 32, 1, 1),\n",
      "      %14 : Float(16),\n",
      "      %15 : Float(16),\n",
      "      %16 : Float(16),\n",
      "      %17 : Float(16),\n",
      "      %18 : Long(),\n",
      "      %19 : Float(96, 16, 1, 1),\n",
      "      %20 : Float(96),\n",
      "      %21 : Float(96),\n",
      "      %22 : Float(96),\n",
      "      %23 : Float(96),\n",
      "      %24 : Long(),\n",
      "      %25 : Float(96, 1, 3, 3),\n",
      "      %26 : Float(96),\n",
      "      %27 : Float(96),\n",
      "      %28 : Float(96),\n",
      "      %29 : Float(96),\n",
      "      %30 : Long(),\n",
      "      %31 : Float(24, 96, 1, 1),\n",
      "      %32 : Float(24),\n",
      "      %33 : Float(24),\n",
      "      %34 : Float(24),\n",
      "      %35 : Float(24),\n",
      "      %36 : Long(),\n",
      "      %37 : Float(144, 24, 1, 1),\n",
      "      %38 : Float(144),\n",
      "      %39 : Float(144),\n",
      "      %40 : Float(144),\n",
      "      %41 : Float(144),\n",
      "      %42 : Long(),\n",
      "      %43 : Float(144, 1, 3, 3),\n",
      "      %44 : Float(144),\n",
      "      %45 : Float(144),\n",
      "      %46 : Float(144),\n",
      "      %47 : Float(144),\n",
      "      %48 : Long(),\n",
      "      %49 : Float(24, 144, 1, 1),\n",
      "      %50 : Float(24),\n",
      "      %51 : Float(24),\n",
      "      %52 : Float(24),\n",
      "      %53 : Float(24),\n",
      "      %54 : Long(),\n",
      "      %55 : Float(144, 24, 1, 1),\n",
      "      %56 : Float(144),\n",
      "      %57 : Float(144),\n",
      "      %58 : Float(144),\n",
      "      %59 : Float(144),\n",
      "      %60 : Long(),\n",
      "      %61 : Float(144, 1, 3, 3),\n",
      "      %62 : Float(144),\n",
      "      %63 : Float(144),\n",
      "      %64 : Float(144),\n",
      "      %65 : Float(144),\n",
      "      %66 : Long(),\n",
      "      %67 : Float(32, 144, 1, 1),\n",
      "      %68 : Float(32),\n",
      "      %69 : Float(32),\n",
      "      %70 : Float(32),\n",
      "      %71 : Float(32),\n",
      "      %72 : Long(),\n",
      "      %73 : Float(192, 32, 1, 1),\n",
      "      %74 : Float(192),\n",
      "      %75 : Float(192),\n",
      "      %76 : Float(192),\n",
      "      %77 : Float(192),\n",
      "      %78 : Long(),\n",
      "      %79 : Float(192, 1, 3, 3),\n",
      "      %80 : Float(192),\n",
      "      %81 : Float(192),\n",
      "      %82 : Float(192),\n",
      "      %83 : Float(192),\n",
      "      %84 : Long(),\n",
      "      %85 : Float(32, 192, 1, 1),\n",
      "      %86 : Float(32),\n",
      "      %87 : Float(32),\n",
      "      %88 : Float(32),\n",
      "      %89 : Float(32),\n",
      "      %90 : Long(),\n",
      "      %91 : Float(192, 32, 1, 1),\n",
      "      %92 : Float(192),\n",
      "      %93 : Float(192),\n",
      "      %94 : Float(192),\n",
      "      %95 : Float(192),\n",
      "      %96 : Long(),\n",
      "      %97 : Float(192, 1, 3, 3),\n",
      "      %98 : Float(192),\n",
      "      %99 : Float(192),\n",
      "      %100 : Float(192),\n",
      "      %101 : Float(192),\n",
      "      %102 : Long(),\n",
      "      %103 : Float(32, 192, 1, 1),\n",
      "      %104 : Float(32),\n",
      "      %105 : Float(32),\n",
      "      %106 : Float(32),\n",
      "      %107 : Float(32),\n",
      "      %108 : Long(),\n",
      "      %109 : Float(192, 32, 1, 1),\n",
      "      %110 : Float(192),\n",
      "      %111 : Float(192),\n",
      "      %112 : Float(192),\n",
      "      %113 : Float(192),\n",
      "      %114 : Long(),\n",
      "      %115 : Float(192, 1, 3, 3),\n",
      "      %116 : Float(192),\n",
      "      %117 : Float(192),\n",
      "      %118 : Float(192),\n",
      "      %119 : Float(192),\n",
      "      %120 : Long(),\n",
      "      %121 : Float(64, 192, 1, 1),\n",
      "      %122 : Float(64),\n",
      "      %123 : Float(64),\n",
      "      %124 : Float(64),\n",
      "      %125 : Float(64),\n",
      "      %126 : Long(),\n",
      "      %127 : Float(384, 64, 1, 1),\n",
      "      %128 : Float(384),\n",
      "      %129 : Float(384),\n",
      "      %130 : Float(384),\n",
      "      %131 : Float(384),\n",
      "      %132 : Long(),\n",
      "      %133 : Float(384, 1, 3, 3),\n",
      "      %134 : Float(384),\n",
      "      %135 : Float(384),\n",
      "      %136 : Float(384),\n",
      "      %137 : Float(384),\n",
      "      %138 : Long(),\n",
      "      %139 : Float(64, 384, 1, 1),\n",
      "      %140 : Float(64),\n",
      "      %141 : Float(64),\n",
      "      %142 : Float(64),\n",
      "      %143 : Float(64),\n",
      "      %144 : Long(),\n",
      "      %145 : Float(384, 64, 1, 1),\n",
      "      %146 : Float(384),\n",
      "      %147 : Float(384),\n",
      "      %148 : Float(384),\n",
      "      %149 : Float(384),\n",
      "      %150 : Long(),\n",
      "      %151 : Float(384, 1, 3, 3),\n",
      "      %152 : Float(384),\n",
      "      %153 : Float(384),\n",
      "      %154 : Float(384),\n",
      "      %155 : Float(384),\n",
      "      %156 : Long(),\n",
      "      %157 : Float(64, 384, 1, 1),\n",
      "      %158 : Float(64),\n",
      "      %159 : Float(64),\n",
      "      %160 : Float(64),\n",
      "      %161 : Float(64),\n",
      "      %162 : Long(),\n",
      "      %163 : Float(384, 64, 1, 1),\n",
      "      %164 : Float(384),\n",
      "      %165 : Float(384),\n",
      "      %166 : Float(384),\n",
      "      %167 : Float(384),\n",
      "      %168 : Long(),\n",
      "      %169 : Float(384, 1, 3, 3),\n",
      "      %170 : Float(384),\n",
      "      %171 : Float(384),\n",
      "      %172 : Float(384),\n",
      "      %173 : Float(384),\n",
      "      %174 : Long(),\n",
      "      %175 : Float(64, 384, 1, 1),\n",
      "      %176 : Float(64),\n",
      "      %177 : Float(64),\n",
      "      %178 : Float(64),\n",
      "      %179 : Float(64),\n",
      "      %180 : Long(),\n",
      "      %181 : Float(384, 64, 1, 1),\n",
      "      %182 : Float(384),\n",
      "      %183 : Float(384),\n",
      "      %184 : Float(384),\n",
      "      %185 : Float(384),\n",
      "      %186 : Long(),\n",
      "      %187 : Float(384, 1, 3, 3),\n",
      "      %188 : Float(384),\n",
      "      %189 : Float(384),\n",
      "      %190 : Float(384),\n",
      "      %191 : Float(384),\n",
      "      %192 : Long(),\n",
      "      %193 : Float(96, 384, 1, 1),\n",
      "      %194 : Float(96),\n",
      "      %195 : Float(96),\n",
      "      %196 : Float(96),\n",
      "      %197 : Float(96),\n",
      "      %198 : Long(),\n",
      "      %199 : Float(576, 96, 1, 1),\n",
      "      %200 : Float(576),\n",
      "      %201 : Float(576),\n",
      "      %202 : Float(576),\n",
      "      %203 : Float(576),\n",
      "      %204 : Long(),\n",
      "      %205 : Float(576, 1, 3, 3),\n",
      "      %206 : Float(576),\n",
      "      %207 : Float(576),\n",
      "      %208 : Float(576),\n",
      "      %209 : Float(576),\n",
      "      %210 : Long(),\n",
      "      %211 : Float(96, 576, 1, 1),\n",
      "      %212 : Float(96),\n",
      "      %213 : Float(96),\n",
      "      %214 : Float(96),\n",
      "      %215 : Float(96),\n",
      "      %216 : Long(),\n",
      "      %217 : Float(576, 96, 1, 1),\n",
      "      %218 : Float(576),\n",
      "      %219 : Float(576),\n",
      "      %220 : Float(576),\n",
      "      %221 : Float(576),\n",
      "      %222 : Long(),\n",
      "      %223 : Float(576, 1, 3, 3),\n",
      "      %224 : Float(576),\n",
      "      %225 : Float(576),\n",
      "      %226 : Float(576),\n",
      "      %227 : Float(576),\n",
      "      %228 : Long(),\n",
      "      %229 : Float(96, 576, 1, 1),\n",
      "      %230 : Float(96),\n",
      "      %231 : Float(96),\n",
      "      %232 : Float(96),\n",
      "      %233 : Float(96),\n",
      "      %234 : Long(),\n",
      "      %235 : Float(576, 96, 1, 1),\n",
      "      %236 : Float(576),\n",
      "      %237 : Float(576),\n",
      "      %238 : Float(576),\n",
      "      %239 : Float(576),\n",
      "      %240 : Long(),\n",
      "      %241 : Float(576, 1, 3, 3),\n",
      "      %242 : Float(576),\n",
      "      %243 : Float(576),\n",
      "      %244 : Float(576),\n",
      "      %245 : Float(576),\n",
      "      %246 : Long(),\n",
      "      %247 : Float(160, 576, 1, 1),\n",
      "      %248 : Float(160),\n",
      "      %249 : Float(160),\n",
      "      %250 : Float(160),\n",
      "      %251 : Float(160),\n",
      "      %252 : Long(),\n",
      "      %253 : Float(960, 160, 1, 1),\n",
      "      %254 : Float(960),\n",
      "      %255 : Float(960),\n",
      "      %256 : Float(960),\n",
      "      %257 : Float(960),\n",
      "      %258 : Long(),\n",
      "      %259 : Float(960, 1, 3, 3),\n",
      "      %260 : Float(960),\n",
      "      %261 : Float(960),\n",
      "      %262 : Float(960),\n",
      "      %263 : Float(960),\n",
      "      %264 : Long(),\n",
      "      %265 : Float(160, 960, 1, 1),\n",
      "      %266 : Float(160),\n",
      "      %267 : Float(160),\n",
      "      %268 : Float(160),\n",
      "      %269 : Float(160),\n",
      "      %270 : Long(),\n",
      "      %271 : Float(960, 160, 1, 1),\n",
      "      %272 : Float(960),\n",
      "      %273 : Float(960),\n",
      "      %274 : Float(960),\n",
      "      %275 : Float(960),\n",
      "      %276 : Long(),\n",
      "      %277 : Float(960, 1, 3, 3),\n",
      "      %278 : Float(960),\n",
      "      %279 : Float(960),\n",
      "      %280 : Float(960),\n",
      "      %281 : Float(960),\n",
      "      %282 : Long(),\n",
      "      %283 : Float(160, 960, 1, 1),\n",
      "      %284 : Float(160),\n",
      "      %285 : Float(160),\n",
      "      %286 : Float(160),\n",
      "      %287 : Float(160),\n",
      "      %288 : Long(),\n",
      "      %289 : Float(960, 160, 1, 1),\n",
      "      %290 : Float(960),\n",
      "      %291 : Float(960),\n",
      "      %292 : Float(960),\n",
      "      %293 : Float(960),\n",
      "      %294 : Long(),\n",
      "      %295 : Float(960, 1, 3, 3),\n",
      "      %296 : Float(960),\n",
      "      %297 : Float(960),\n",
      "      %298 : Float(960),\n",
      "      %299 : Float(960),\n",
      "      %300 : Long(),\n",
      "      %301 : Float(320, 960, 1, 1),\n",
      "      %302 : Float(320),\n",
      "      %303 : Float(320),\n",
      "      %304 : Float(320),\n",
      "      %305 : Float(320),\n",
      "      %306 : Long(),\n",
      "      %307 : Float(1280, 320, 1, 1),\n",
      "      %308 : Float(1280),\n",
      "      %309 : Float(1280),\n",
      "      %310 : Float(1280),\n",
      "      %311 : Float(1280),\n",
      "      %312 : Long(),\n",
      "      %313 : Float(7, 1280),\n",
      "      %314 : Float(7)):\n",
      "  %315 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%data, %1), scope: MobileNetV2/Sequential[features]/Sequential[0]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %316 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%315, %2, %3, %4, %5), scope: MobileNetV2/Sequential[features]/Sequential[0]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %317 : Float(1, 32, 16, 16) = onnx::Clip[max=6, min=0](%316), scope: MobileNetV2/Sequential[features]/Sequential[0]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %318 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%317, %7), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %319 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%318, %8, %9, %10, %11), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %320 : Float(1, 32, 16, 16) = onnx::Clip[max=6, min=0](%319), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %321 : Float(1, 16, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%320, %13), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %322 : Float(1, 16, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%321, %14, %15, %16, %17), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %323 : Float(1, 96, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%322, %19), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %324 : Float(1, 96, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%323, %20, %21, %22, %23), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %325 : Float(1, 96, 16, 16) = onnx::Clip[max=6, min=0](%324), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %326 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%325, %25), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %327 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%326, %26, %27, %28, %29), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %328 : Float(1, 96, 8, 8) = onnx::Clip[max=6, min=0](%327), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %329 : Float(1, 24, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%328, %31), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %330 : Float(1, 24, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%329, %32, %33, %34, %35), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %331 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%330, %37), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %332 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%331, %38, %39, %40, %41), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %333 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%332), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %334 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%333, %43), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %335 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%334, %44, %45, %46, %47), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %336 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%335), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %337 : Float(1, 24, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%336, %49), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %338 : Float(1, 24, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%337, %50, %51, %52, %53), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %339 : Float(1, 24, 8, 8) = onnx::Add(%330, %338), scope: MobileNetV2/Sequential[features]/InvertedResidual[3] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %340 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%339, %55), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %341 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%340, %56, %57, %58, %59), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %342 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%341), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %343 : Float(1, 144, 4, 4) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%342, %61), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %344 : Float(1, 144, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%343, %62, %63, %64, %65), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %345 : Float(1, 144, 4, 4) = onnx::Clip[max=6, min=0](%344), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %346 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%345, %67), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %347 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%346, %68, %69, %70, %71), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %348 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%347, %73), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %349 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%348, %74, %75, %76, %77), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %350 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%349), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %351 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%350, %79), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %352 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%351, %80, %81, %82, %83), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %353 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%352), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %354 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%353, %85), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %355 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%354, %86, %87, %88, %89), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %356 : Float(1, 32, 4, 4) = onnx::Add(%347, %355), scope: MobileNetV2/Sequential[features]/InvertedResidual[5] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %357 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%356, %91), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %358 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%357, %92, %93, %94, %95), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %359 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%358), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %360 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%359, %97), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %361 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%360, %98, %99, %100, %101), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %362 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%361), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %363 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%362, %103), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %364 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%363, %104, %105, %106, %107), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %365 : Float(1, 32, 4, 4) = onnx::Add(%356, %364), scope: MobileNetV2/Sequential[features]/InvertedResidual[6] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %366 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %109), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %367 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%366, %110, %111, %112, %113), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %368 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%367), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %369 : Float(1, 192, 2, 2) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%368, %115), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %370 : Float(1, 192, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%369, %116, %117, %118, %119), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %371 : Float(1, 192, 2, 2) = onnx::Clip[max=6, min=0](%370), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %372 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%371, %121), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %373 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%372, %122, %123, %124, %125), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %374 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%373, %127), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %375 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%374, %128, %129, %130, %131), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %376 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%375), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %377 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%376, %133), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %378 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%377, %134, %135, %136, %137), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %379 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%378), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %380 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%379, %139), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %381 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%380, %140, %141, %142, %143), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %382 : Float(1, 64, 2, 2) = onnx::Add(%373, %381), scope: MobileNetV2/Sequential[features]/InvertedResidual[8] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %383 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%382, %145), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %384 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%383, %146, %147, %148, %149), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %385 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%384), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %386 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%385, %151), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %387 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%386, %152, %153, %154, %155), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %388 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%387), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %389 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%388, %157), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %390 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%389, %158, %159, %160, %161), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %391 : Float(1, 64, 2, 2) = onnx::Add(%382, %390), scope: MobileNetV2/Sequential[features]/InvertedResidual[9] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %392 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%391, %163), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %393 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%392, %164, %165, %166, %167), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %394 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%393), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %395 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%394, %169), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %396 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%395, %170, %171, %172, %173), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %397 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%396), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %398 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%397, %175), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %399 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%398, %176, %177, %178, %179), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %400 : Float(1, 64, 2, 2) = onnx::Add(%391, %399), scope: MobileNetV2/Sequential[features]/InvertedResidual[10] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %401 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%400, %181), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %402 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%401, %182, %183, %184, %185), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %403 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%402), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %404 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%403, %187), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %405 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%404, %188, %189, %190, %191), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %406 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%405), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %407 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%406, %193), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %408 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%407, %194, %195, %196, %197), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %409 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%408, %199), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %410 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%409, %200, %201, %202, %203), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %411 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%410), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %412 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%411, %205), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %413 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%412, %206, %207, %208, %209), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %414 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%413), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %415 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%414, %211), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %416 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%415, %212, %213, %214, %215), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %417 : Float(1, 96, 2, 2) = onnx::Add(%408, %416), scope: MobileNetV2/Sequential[features]/InvertedResidual[12] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %418 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%417, %217), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %419 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%418, %218, %219, %220, %221), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %420 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%419), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %421 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%420, %223), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %422 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%421, %224, %225, %226, %227), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %423 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%422), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %424 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%423, %229), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %425 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%424, %230, %231, %232, %233), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %426 : Float(1, 96, 2, 2) = onnx::Add(%417, %425), scope: MobileNetV2/Sequential[features]/InvertedResidual[13] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %427 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%426, %235), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %428 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%427, %236, %237, %238, %239), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %429 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%428), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %430 : Float(1, 576, 1, 1) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%429, %241), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %431 : Float(1, 576, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%430, %242, %243, %244, %245), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %432 : Float(1, 576, 1, 1) = onnx::Clip[max=6, min=0](%431), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %433 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%432, %247), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %434 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%433, %248, %249, %250, %251), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %435 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%434, %253), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %436 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%435, %254, %255, %256, %257), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %437 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%436), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %438 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%437, %259), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %439 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%438, %260, %261, %262, %263), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %440 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%439), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %441 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %265), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %442 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%441, %266, %267, %268, %269), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %443 : Float(1, 160, 1, 1) = onnx::Add(%434, %442), scope: MobileNetV2/Sequential[features]/InvertedResidual[15] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %444 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%443, %271), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %445 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%444, %272, %273, %274, %275), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %446 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%445), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %447 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%446, %277), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %448 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%447, %278, %279, %280, %281), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %449 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%448), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %450 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %283), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %451 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%450, %284, %285, %286, %287), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %452 : Float(1, 160, 1, 1) = onnx::Add(%443, %451), scope: MobileNetV2/Sequential[features]/InvertedResidual[16] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %453 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%452, %289), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %454 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%453, %290, %291, %292, %293), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %455 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%454), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %456 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%455, %295), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %457 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%456, %296, %297, %298, %299), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %458 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%457), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %459 : Float(1, 320, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%458, %301), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %460 : Float(1, 320, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%459, %302, %303, %304, %305), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %461 : Float(1, 1280, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%460, %307), scope: MobileNetV2/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %462 : Float(1, 1280, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%461, %308, %309, %310, %311), scope: MobileNetV2/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %463 : Float(1, 1280, 1, 1) = onnx::Clip[max=6, min=0](%462), scope: MobileNetV2/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %464 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%463), scope: MobileNetV2/AvgPool2d[avgpool]\n",
      "  %465 : Float(1, 1280, 1, 1) = onnx::AveragePool[kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%464), scope: MobileNetV2/AvgPool2d[avgpool] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py:551:0\n",
      "  %466 : Long() = onnx::Constant[value={0}](), scope: MobileNetV2\n",
      "  %467 : Tensor = onnx::Shape(%465), scope: MobileNetV2\n",
      "  %468 : Long() = onnx::Gather[axis=0](%467, %466), scope: MobileNetV2 # /storage/expression_recognition/mobilenetv2.py:132:0\n",
      "  %469 : Long() = onnx::Constant[value={-1}](), scope: MobileNetV2\n",
      "  %470 : Tensor = onnx::Unsqueeze[axes=[0]](%468)\n",
      "  %471 : Tensor = onnx::Unsqueeze[axes=[0]](%469)\n",
      "  %472 : Tensor = onnx::Concat[axis=0](%470, %471)\n",
      "  %473 : Float(1, 1280) = onnx::Reshape(%465, %472), scope: MobileNetV2 # /storage/expression_recognition/mobilenetv2.py:132:0\n",
      "  %outTensor : Float(1, 7) = onnx::Gemm[alpha=1, beta=1, transB=1](%473, %313, %314), scope: MobileNetV2/Linear[classifier] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1369:0\n",
      "  return (%outTensor)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "\n",
    "#onnx\n",
    "\"\"\"\n",
    "visualize results for test image\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "img = io.imread('images/anger_rgb.png')\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "#\n",
    "net = mobilenetv2(num_classes=7,input_size=32)\n",
    "checkpoint = torch.load(os.path.join('CK+_mobilenetv2/1/', 'Test_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.train(False)\n",
    "\n",
    "#input\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "#\n",
    "torch_out = torch.onnx._export(net,  # model being run\n",
    "                               inputs,  # model input (or a tuple for multiple inputs)\n",
    "                               \"CK+_mobilenetv2_privateTest.onnx\",  # where to save the model\n",
    "                               verbose=True,\n",
    "                               input_names=['data'], \n",
    "                               output_names=['outTensor'], \n",
    "                               export_params=True, \n",
    "                               training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.569552   1.9761721  2.7238057 -0.2002076 -0.8548509 -2.763262\n",
      "  -5.387613 ]]\n",
      "     0.802\n",
      "     0.060\n",
      "     0.127\n",
      "     0.007\n",
      "     0.004\n",
      "     0.001\n",
      "     0.000\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "# onnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"CK+_mobilenetv2_privateTest.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "\n",
    "img = io.imread('images/anger_rgb.png')\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "max = score[0][0]\n",
    "maxindex = 0\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "  if(score[0][i] > max):\n",
    "        max = score[0][i]\n",
    "        maxindex = i\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[maxindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx-simplifier in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.5.0)\n",
      "Requirement already satisfied: onnxruntime>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (3.9.1)\n",
      "Requirement already satisfied: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (3.7.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (3.7.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (1.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.7.0->onnx-simplifier) (41.0.1)\n",
      "Simplifying...\n",
      "Ok!\n",
      "[[ 4.569552    1.9761757   2.7238045  -0.20020331 -0.8548558  -2.763267\n",
      "  -5.387617  ]]\n",
      "     0.802\n",
      "     0.060\n",
      "     0.127\n",
      "     0.007\n",
      "     0.004\n",
      "     0.001\n",
      "     0.000\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx-simplifier\n",
    "# onnx\n",
    "!python -m onnxsim \"CK+_mobilenetv2_privateTest.onnx\" \"CK+_mobilenetv2_privateTest_sim.onnx\"\n",
    "\n",
    "# onnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"CK+_mobilenetv2_privateTest_sim.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "\n",
    "img = io.imread('images/anger_rgb.png')\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "max = score[0][0]\n",
    "maxindex = 0\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "  if(score[0][i] > max):\n",
    "        max = score[0][i]\n",
    "        maxindex = i\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[maxindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: onnx-coreml in /usr/local/lib/python3.6/dist-packages (1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: coremltools==3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.0)\n",
      "Requirement already satisfied, skipping upgrade: onnx==1.5.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from coremltools==3.0->onnx-coreml) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from coremltools==3.0->onnx-coreml) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->coremltools==3.0->onnx-coreml) (41.0.1)\n",
      "1/100: Converting Node Type Conv\n",
      "2/100: Converting Node Type Clip\n",
      "3/100: Converting Node Type Conv\n",
      "4/100: Converting Node Type Clip\n",
      "5/100: Converting Node Type Conv\n",
      "6/100: Converting Node Type Conv\n",
      "7/100: Converting Node Type Clip\n",
      "8/100: Converting Node Type Conv\n",
      "9/100: Converting Node Type Clip\n",
      "10/100: Converting Node Type Conv\n",
      "11/100: Converting Node Type Conv\n",
      "12/100: Converting Node Type Clip\n",
      "13/100: Converting Node Type Conv\n",
      "14/100: Converting Node Type Clip\n",
      "15/100: Converting Node Type Conv\n",
      "16/100: Converting Node Type Add\n",
      "17/100: Converting Node Type Conv\n",
      "18/100: Converting Node Type Clip\n",
      "19/100: Converting Node Type Conv\n",
      "20/100: Converting Node Type Clip\n",
      "21/100: Converting Node Type Conv\n",
      "22/100: Converting Node Type Conv\n",
      "23/100: Converting Node Type Clip\n",
      "24/100: Converting Node Type Conv\n",
      "25/100: Converting Node Type Clip\n",
      "26/100: Converting Node Type Conv\n",
      "27/100: Converting Node Type Add\n",
      "28/100: Converting Node Type Conv\n",
      "29/100: Converting Node Type Clip\n",
      "30/100: Converting Node Type Conv\n",
      "31/100: Converting Node Type Clip\n",
      "32/100: Converting Node Type Conv\n",
      "33/100: Converting Node Type Add\n",
      "34/100: Converting Node Type Conv\n",
      "35/100: Converting Node Type Clip\n",
      "36/100: Converting Node Type Conv\n",
      "37/100: Converting Node Type Clip\n",
      "38/100: Converting Node Type Conv\n",
      "39/100: Converting Node Type Conv\n",
      "40/100: Converting Node Type Clip\n",
      "41/100: Converting Node Type Conv\n",
      "42/100: Converting Node Type Clip\n",
      "43/100: Converting Node Type Conv\n",
      "44/100: Converting Node Type Add\n",
      "45/100: Converting Node Type Conv\n",
      "46/100: Converting Node Type Clip\n",
      "47/100: Converting Node Type Conv\n",
      "48/100: Converting Node Type Clip\n",
      "49/100: Converting Node Type Conv\n",
      "50/100: Converting Node Type Add\n",
      "51/100: Converting Node Type Conv\n",
      "52/100: Converting Node Type Clip\n",
      "53/100: Converting Node Type Conv\n",
      "54/100: Converting Node Type Clip\n",
      "55/100: Converting Node Type Conv\n",
      "56/100: Converting Node Type Add\n",
      "57/100: Converting Node Type Conv\n",
      "58/100: Converting Node Type Clip\n",
      "59/100: Converting Node Type Conv\n",
      "60/100: Converting Node Type Clip\n",
      "61/100: Converting Node Type Conv\n",
      "62/100: Converting Node Type Conv\n",
      "63/100: Converting Node Type Clip\n",
      "64/100: Converting Node Type Conv\n",
      "65/100: Converting Node Type Clip\n",
      "66/100: Converting Node Type Conv\n",
      "67/100: Converting Node Type Add\n",
      "68/100: Converting Node Type Conv\n",
      "69/100: Converting Node Type Clip\n",
      "70/100: Converting Node Type Conv\n",
      "71/100: Converting Node Type Clip\n",
      "72/100: Converting Node Type Conv\n",
      "73/100: Converting Node Type Add\n",
      "74/100: Converting Node Type Conv\n",
      "75/100: Converting Node Type Clip\n",
      "76/100: Converting Node Type Conv\n",
      "77/100: Converting Node Type Clip\n",
      "78/100: Converting Node Type Conv\n",
      "79/100: Converting Node Type Conv\n",
      "80/100: Converting Node Type Clip\n",
      "81/100: Converting Node Type Conv\n",
      "82/100: Converting Node Type Clip\n",
      "83/100: Converting Node Type Conv\n",
      "84/100: Converting Node Type Add\n",
      "85/100: Converting Node Type Conv\n",
      "86/100: Converting Node Type Clip\n",
      "87/100: Converting Node Type Conv\n",
      "88/100: Converting Node Type Clip\n",
      "89/100: Converting Node Type Conv\n",
      "90/100: Converting Node Type Add\n",
      "91/100: Converting Node Type Conv\n",
      "92/100: Converting Node Type Clip\n",
      "93/100: Converting Node Type Conv\n",
      "94/100: Converting Node Type Clip\n",
      "95/100: Converting Node Type Conv\n",
      "96/100: Converting Node Type Conv\n",
      "97/100: Converting Node Type Clip\n",
      "98/100: Converting Node Type AveragePool\n",
      "99/100: Converting Node Type Reshape\n",
      "100/100: Converting Node Type Gemm\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U onnx-coreml\n",
    "#corml\n",
    "import onnx;\n",
    "from onnx_coreml import convert\n",
    "\n",
    "onnx_model = onnx.load(\"CK+_mobilenetv2_privateTest_sim.onnx\")\n",
    "cml_model= convert(onnx_model,image_input_names='data')\n",
    "cml_model.save(\"CK+_mobilenetv2_privateTest_sim.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
