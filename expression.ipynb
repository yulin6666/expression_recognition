{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/expression_recognition\n"
     ]
    }
   ],
   "source": [
    "cd /storage/expression_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [=============================>] | Loss: 1.656 | Acc: 32.786% (9364/28561)     224/224 \n",
      "mainpro_FER_moblienetv2-tencopy.py:234: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
      " [============================>.] | Loss: 1.601 | Acc: 32.998% (1181/3579)      28/28 \n",
      "mainpro_FER_moblienetv2-tencopy.py:277: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
      " [============================>.] | Loss: 1.615 | Acc: 31.925% (1141/3574)      28/28 \n",
      "\n",
      "Epoch: 1\n",
      " [=============================>] | Loss: 1.627 | Acc: 34.151% (9754/28561)     224/224 \n",
      " [============================>.] | Loss: 1.576 | Acc: 39.061% (1398/3579)      28/28 \n",
      " [============================>.] | Loss: 1.616 | Acc: 36.514% (1305/3574)      28/28 \n",
      "\n",
      "Epoch: 2\n",
      " [=============================>] | Loss: 1.613 | Acc: 35.335% (10092/28561)    224/224 \n",
      " [============================>.] | Loss: 1.540 | Acc: 38.419% (1375/3579)      28/28 \n",
      " [============================>.] | Loss: 1.555 | Acc: 38.220% (1366/3574)      28/28 \n",
      "\n",
      "Epoch: 3\n",
      " [=============================>] | Loss: 1.596 | Acc: 35.741% (10208/28561)    224/224 \n",
      " [============================>.] | Loss: 1.543 | Acc: 38.894% (1392/3579)      28/28 \n",
      " [============================>.] | Loss: 1.559 | Acc: 37.353% (1335/3574)      28/28 \n",
      "\n",
      "Epoch: 4\n",
      " [=============================>] | Loss: 1.569 | Acc: 36.823% (10517/28561)    224/224 \n",
      " [============================>.] | Loss: 1.525 | Acc: 39.480% (1413/3579)      28/28 \n",
      " [============================>.] | Loss: 1.542 | Acc: 38.081% (1361/3574)      28/28 \n",
      "\n",
      "Epoch: 5\n",
      " [=============================>] | Loss: 1.534 | Acc: 39.410% (11256/28561)    224/224 \n",
      " [============================>.] | Loss: 1.491 | Acc: 43.588% (1560/3579)      28/28 \n",
      " [============================>.] | Loss: 1.504 | Acc: 43.341% (1549/3574)      28/28 \n",
      "\n",
      "Epoch: 6\n",
      " [=============================>] | Loss: 1.501 | Acc: 41.459% (11841/28561)    224/224 \n",
      " [============================>.] | Loss: 1.450 | Acc: 45.543% (1630/3579)      28/28 \n",
      " [============================>.] | Loss: 1.470 | Acc: 43.201% (1544/3574)      28/28 \n",
      "\n",
      "Epoch: 7\n",
      " [=============================>] | Loss: 1.464 | Acc: 43.797% (12509/28561)    224/224 \n",
      " [============================>.] | Loss: 1.494 | Acc: 40.961% (1466/3579)      28/28 \n",
      " [============================>.] | Loss: 1.511 | Acc: 40.739% (1456/3574)      28/28 \n",
      "\n",
      "Epoch: 8\n",
      " [=============================>] | Loss: 1.435 | Acc: 45.573% (13016/28561)    224/224 \n",
      " [============================>.] | Loss: 1.419 | Acc: 45.376% (1624/3579)      28/28 \n",
      " [============================>.] | Loss: 1.442 | Acc: 44.264% (1582/3574)      28/28 \n",
      "\n",
      "Epoch: 9\n",
      " [=============================>] | Loss: 1.397 | Acc: 47.558% (13583/28561)    224/224 \n",
      " [============================>.] | Loss: 1.641 | Acc: 34.647% (1240/3579)      28/28 \n",
      " [============================>.] | Loss: 1.642 | Acc: 35.227% (1259/3574)      28/28 \n",
      "\n",
      "Epoch: 10\n",
      " [=============================>] | Loss: 1.343 | Acc: 50.047% (14294/28561)    224/224 \n",
      " [============================>.] | Loss: 1.356 | Acc: 46.913% (1679/3579)      28/28 \n",
      " [============================>.] | Loss: 1.372 | Acc: 45.663% (1632/3574)      28/28 \n",
      "\n",
      "Epoch: 11\n",
      " [=============================>] | Loss: 1.307 | Acc: 51.546% (14722/28561)    224/224 \n",
      " [============================>.] | Loss: 1.257 | Acc: 54.652% (1956/3579)      28/28 \n",
      " [============================>.] | Loss: 1.277 | Acc: 53.973% (1929/3574)      28/28 \n",
      "\n",
      "Epoch: 12\n",
      " [=============================>] | Loss: 1.273 | Acc: 53.258% (15211/28561)    224/224 \n",
      " [============================>.] | Loss: 1.263 | Acc: 55.155% (1974/3579)      28/28 \n",
      " [============================>.] | Loss: 1.279 | Acc: 53.553% (1914/3574)      28/28 \n",
      "\n",
      "Epoch: 13\n",
      " [=============================>] | Loss: 1.255 | Acc: 54.007% (15425/28561)    224/224 \n",
      " [============================>.] | Loss: 1.195 | Acc: 55.462% (1985/3579)      28/28 \n",
      " [============================>.] | Loss: 1.226 | Acc: 54.225% (1938/3574)      28/28 \n",
      "\n",
      "Epoch: 14\n",
      " [=============================>] | Loss: 1.226 | Acc: 55.306% (15796/28561)    224/224 \n",
      " [============================>.] | Loss: 1.231 | Acc: 57.446% (2056/3579)      28/28 \n",
      " [============================>.] | Loss: 1.239 | Acc: 56.659% (2025/3574)      28/28 \n",
      "\n",
      "Epoch: 15\n",
      " [=============================>] | Loss: 1.196 | Acc: 56.423% (16115/28561)    224/224 \n",
      " [============================>.] | Loss: 1.178 | Acc: 57.949% (2074/3579)      28/28 \n",
      " [============================>.] | Loss: 1.192 | Acc: 56.072% (2004/3574)      28/28 \n",
      "\n",
      "Epoch: 16\n",
      " [=============================>] | Loss: 1.182 | Acc: 56.980% (16274/28561)    224/224 \n",
      " [============================>.] | Loss: 1.118 | Acc: 61.749% (2210/3579)      28/28 \n",
      " [============================>.] | Loss: 1.128 | Acc: 60.828% (2174/3574)      28/28 \n",
      "\n",
      "Epoch: 17\n",
      " [=============================>] | Loss: 1.153 | Acc: 58.114% (16598/28561)    224/224 \n",
      " [============================>.] | Loss: 1.275 | Acc: 50.293% (1800/3579)      28/28 \n",
      " [============================>.] | Loss: 1.287 | Acc: 49.664% (1775/3574)      28/28 \n",
      "\n",
      "Epoch: 18\n",
      " [=============================>] | Loss: 1.138 | Acc: 58.443% (16692/28561)    224/224 \n",
      " [============================>.] | Loss: 1.116 | Acc: 62.699% (2244/3579)      28/28 \n",
      " [============================>.] | Loss: 1.122 | Acc: 62.031% (2217/3574)      28/28 \n",
      "\n",
      "Epoch: 19\n",
      " [=============================>] | Loss: 1.115 | Acc: 59.070% (16871/28561)    224/224 \n",
      " [============================>.] | Loss: 1.120 | Acc: 60.855% (2178/3579)      28/28 \n",
      " [============================>.] | Loss: 1.131 | Acc: 59.597% (2130/3574)      28/28 \n",
      "\n",
      "Epoch: 20\n",
      " [=============================>] | Loss: 1.098 | Acc: 59.718% (17056/28561)    224/224 \n",
      " [============================>.] | Loss: 1.039 | Acc: 63.677% (2279/3579)      28/28 \n",
      " [============================>.] | Loss: 1.057 | Acc: 63.402% (2266/3574)      28/28 \n",
      "\n",
      "Epoch: 21\n",
      " [=============================>] | Loss: 1.077 | Acc: 61.115% (17455/28561)    224/224 \n",
      " [============================>.] | Loss: 1.071 | Acc: 61.749% (2210/3579)      28/28 \n",
      " [============================>.] | Loss: 1.090 | Acc: 59.905% (2141/3574)      28/28 \n",
      "\n",
      "Epoch: 22\n",
      " [=============================>] | Loss: 1.070 | Acc: 61.311% (17511/28561)    224/224 \n",
      " [============================>.] | Loss: 1.061 | Acc: 62.168% (2225/3579)      28/28 \n",
      " [============================>.] | Loss: 1.078 | Acc: 60.996% (2180/3574)      28/28 \n",
      "\n",
      "Epoch: 23\n",
      " [=============================>] | Loss: 1.043 | Acc: 61.836% (17661/28561)    224/224 \n",
      " [============================>.] | Loss: 0.969 | Acc: 67.924% (2431/3579)      28/28 \n",
      " [============================>.] | Loss: 0.988 | Acc: 66.704% (2384/3574)      28/28 \n",
      "\n",
      "Epoch: 24\n",
      " [=============================>] | Loss: 1.034 | Acc: 62.386% (17818/28561)    224/224 \n",
      " [============================>.] | Loss: 0.993 | Acc: 64.990% (2326/3579)      28/28 \n",
      " [============================>.] | Loss: 1.012 | Acc: 64.438% (2303/3574)      28/28 \n",
      "\n",
      "Epoch: 25\n",
      " [=============================>] | Loss: 1.026 | Acc: 62.445% (17835/28561)    224/224 \n",
      " [============================>.] | Loss: 1.068 | Acc: 62.140% (2224/3579)      28/28 \n",
      " [============================>.] | Loss: 1.087 | Acc: 61.780% (2208/3574)      28/28 \n",
      "\n",
      "Epoch: 26\n",
      " [=============================>] | Loss: 1.007 | Acc: 63.310% (18082/28561)    224/224 \n",
      " [============================>.] | Loss: 1.015 | Acc: 66.303% (2373/3579)      28/28 \n",
      " [============================>.] | Loss: 1.024 | Acc: 65.781% (2351/3574)      28/28 \n",
      "\n",
      "Epoch: 27\n",
      " [=============================>] | Loss: 0.999 | Acc: 63.296% (18078/28561)    224/224 \n",
      " [============================>.] | Loss: 0.923 | Acc: 66.723% (2388/3579)      28/28 \n",
      " [============================>.] | Loss: 0.955 | Acc: 65.781% (2351/3574)      28/28 \n",
      "\n",
      "Epoch: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.991 | Acc: 63.790% (18219/28561)    224/224 \n",
      " [============================>.] | Loss: 0.931 | Acc: 67.980% (2433/3579)      28/28 \n",
      " [============================>.] | Loss: 0.942 | Acc: 67.152% (2400/3574)      28/28 \n",
      "\n",
      "Epoch: 29\n",
      " [=============================>] | Loss: 0.978 | Acc: 64.269% (18356/28561)    224/224 \n",
      " [============================>.] | Loss: 0.898 | Acc: 69.796% (2498/3579)      28/28 \n",
      " [============================>.] | Loss: 0.915 | Acc: 68.830% (2460/3574)      28/28 \n",
      "\n",
      "Epoch: 30\n",
      " [=============================>] | Loss: 0.963 | Acc: 64.777% (18501/28561)    224/224 \n",
      " [============================>.] | Loss: 0.956 | Acc: 68.511% (2452/3579)      28/28 \n",
      " [============================>.] | Loss: 0.969 | Acc: 66.928% (2392/3574)      28/28 \n",
      "\n",
      "Epoch: 31\n",
      " [=============================>] | Loss: 0.960 | Acc: 64.816% (18512/28561)    224/224 \n",
      " [============================>.] | Loss: 0.931 | Acc: 68.399% (2448/3579)      28/28 \n",
      " [============================>.] | Loss: 0.951 | Acc: 66.872% (2390/3574)      28/28 \n",
      "\n",
      "Epoch: 32\n",
      " [=============================>] | Loss: 0.951 | Acc: 65.306% (18652/28561)    224/224 \n",
      " [============================>.] | Loss: 0.907 | Acc: 67.477% (2415/3579)      28/28 \n",
      " [============================>.] | Loss: 0.940 | Acc: 66.088% (2362/3574)      28/28 \n",
      "\n",
      "Epoch: 33\n",
      " [=============================>] | Loss: 0.952 | Acc: 65.362% (18668/28561)    224/224 \n",
      " [============================>.] | Loss: 0.895 | Acc: 67.561% (2418/3579)      28/28 \n",
      " [============================>.] | Loss: 0.928 | Acc: 66.396% (2373/3574)      28/28 \n",
      "\n",
      "Epoch: 34\n",
      " [=============================>] | Loss: 0.932 | Acc: 66.101% (18879/28561)    224/224 \n",
      " [============================>.] | Loss: 0.876 | Acc: 70.047% (2507/3579)      28/28 \n",
      " [============================>.] | Loss: 0.896 | Acc: 69.054% (2468/3574)      28/28 \n",
      "\n",
      "Epoch: 35\n",
      " [=============================>] | Loss: 0.926 | Acc: 66.321% (18942/28561)    224/224 \n",
      " [============================>.] | Loss: 0.903 | Acc: 67.393% (2412/3579)      28/28 \n",
      " [============================>.] | Loss: 0.922 | Acc: 66.172% (2365/3574)      28/28 \n",
      "\n",
      "Epoch: 36\n",
      " [=============================>] | Loss: 0.918 | Acc: 66.423% (18971/28561)    224/224 \n",
      " [============================>.] | Loss: 0.881 | Acc: 68.874% (2465/3579)      28/28 \n",
      " [============================>.] | Loss: 0.905 | Acc: 68.383% (2444/3574)      28/28 \n",
      "\n",
      "Epoch: 37\n",
      " [=============================>] | Loss: 0.914 | Acc: 66.650% (19036/28561)    224/224 \n",
      " [============================>.] | Loss: 0.836 | Acc: 71.892% (2573/3579)      28/28 \n",
      " [============================>.] | Loss: 0.860 | Acc: 70.425% (2517/3574)      28/28 \n",
      "\n",
      "Epoch: 38\n",
      " [=============================>] | Loss: 0.905 | Acc: 67.049% (19150/28561)    224/224 \n",
      " [============================>.] | Loss: 0.836 | Acc: 70.383% (2519/3579)      28/28 \n",
      " [============================>.] | Loss: 0.864 | Acc: 69.390% (2480/3574)      28/28 \n",
      "\n",
      "Epoch: 39\n",
      " [=============================>] | Loss: 0.899 | Acc: 67.270% (19213/28561)    224/224 \n",
      " [============================>.] | Loss: 0.837 | Acc: 71.193% (2548/3579)      28/28 \n",
      " [============================>.] | Loss: 0.866 | Acc: 69.110% (2470/3574)      28/28 \n",
      "\n",
      "Epoch: 40\n",
      " [=============================>] | Loss: 0.890 | Acc: 67.330% (19230/28561)    224/224 \n",
      " [============================>.] | Loss: 0.819 | Acc: 72.618% (2599/3579)      28/28 \n",
      " [============================>.] | Loss: 0.845 | Acc: 70.957% (2536/3574)      28/28 \n",
      "\n",
      "Epoch: 41\n",
      " [=============================>] | Loss: 0.891 | Acc: 67.442% (19262/28561)    224/224 \n",
      " [============================>.] | Loss: 0.846 | Acc: 71.892% (2573/3579)      28/28 \n",
      " [============================>.] | Loss: 0.876 | Acc: 70.537% (2521/3574)      28/28 \n",
      "\n",
      "Epoch: 42\n",
      " [=============================>] | Loss: 0.886 | Acc: 67.340% (19233/28561)    224/224 \n",
      " [============================>.] | Loss: 0.840 | Acc: 69.908% (2502/3579)      28/28 \n",
      " [============================>.] | Loss: 0.873 | Acc: 68.467% (2447/3574)      28/28 \n",
      "\n",
      "Epoch: 43\n",
      " [=============================>] | Loss: 0.887 | Acc: 67.781% (19359/28561)    224/224 \n",
      " [============================>.] | Loss: 0.803 | Acc: 71.556% (2561/3579)      28/28 \n",
      " [============================>.] | Loss: 0.827 | Acc: 70.257% (2511/3574)      28/28 \n",
      "\n",
      "Epoch: 44\n",
      " [=============================>] | Loss: 0.867 | Acc: 68.310% (19510/28561)    224/224 \n",
      " [============================>.] | Loss: 0.778 | Acc: 73.484% (2630/3579)      28/28 \n",
      " [============================>.] | Loss: 0.807 | Acc: 72.104% (2577/3574)      28/28 \n",
      "\n",
      "Epoch: 45\n",
      " [=============================>] | Loss: 0.855 | Acc: 68.496% (19563/28561)    224/224 \n",
      " [============================>.] | Loss: 0.818 | Acc: 70.578% (2526/3579)      28/28 \n",
      " [============================>.] | Loss: 0.845 | Acc: 69.446% (2482/3574)      28/28 \n",
      "\n",
      "Epoch: 46\n",
      " [=============================>] | Loss: 0.861 | Acc: 69.052% (19722/28561)    224/224 \n",
      " [============================>.] | Loss: 0.788 | Acc: 73.708% (2638/3579)      28/28 \n",
      " [============================>.] | Loss: 0.809 | Acc: 71.824% (2567/3574)      28/28 \n",
      "\n",
      "Epoch: 47\n",
      " [=============================>] | Loss: 0.855 | Acc: 68.888% (19675/28561)    224/224 \n",
      " [============================>.] | Loss: 0.786 | Acc: 73.708% (2638/3579)      28/28 \n",
      " [============================>.] | Loss: 0.811 | Acc: 72.020% (2574/3574)      28/28 \n",
      "\n",
      "Epoch: 48\n",
      " [=============================>] | Loss: 0.851 | Acc: 68.912% (19682/28561)    224/224 \n",
      " [============================>.] | Loss: 0.797 | Acc: 73.456% (2629/3579)      28/28 \n",
      " [=============================>] | Loss: 0.827 | Acc: 69.893% (19962/28561)    224/224 \n",
      " [============================>.] | Loss: 0.759 | Acc: 74.378% (2662/3579)      28/28 \n",
      " [============================>.] | Loss: 0.788 | Acc: 72.664% (2597/3574)      28/28 \n",
      "\n",
      "Epoch: 54\n",
      " [=============================>] | Loss: 0.822 | Acc: 69.921% (19970/28561)    224/224 \n",
      " [============================>.] | Loss: 0.815 | Acc: 71.808% (2570/3579)      28/28 \n",
      " [============================>.] | Loss: 0.843 | Acc: 70.062% (2504/3574)      28/28 \n",
      "\n",
      "Epoch: 55\n",
      " [=============================>] | Loss: 0.812 | Acc: 70.026% (20000/28561)    224/224 \n",
      " [============================>.] | Loss: 0.757 | Acc: 74.769% (2676/3579)      28/28 \n",
      " [============================>.] | Loss: 0.784 | Acc: 73.475% (2626/3574)      28/28 \n",
      "\n",
      "Epoch: 56\n",
      " [=============================>] | Loss: 0.814 | Acc: 70.099% (20021/28561)    224/224 \n",
      " [============================>.] | Loss: 0.749 | Acc: 74.015% (2649/3579)      28/28 \n",
      " [============================>.] | Loss: 0.777 | Acc: 72.468% (2590/3574)      28/28 \n",
      "\n",
      "Epoch: 57\n",
      " [=============================>] | Loss: 0.805 | Acc: 70.526% (20143/28561)    224/224 \n",
      " [============================>.] | Loss: 0.729 | Acc: 74.602% (2670/3579)      28/28 \n",
      " [============================>.] | Loss: 0.757 | Acc: 73.195% (2616/3574)      28/28 \n",
      "\n",
      "Epoch: 58\n",
      " [=============================>] | Loss: 0.803 | Acc: 70.295% (20077/28561)    224/224 \n",
      " [============================>.] | Loss: 0.742 | Acc: 73.847% (2643/3579)      28/28 \n",
      " [============================>.] | Loss: 0.776 | Acc: 72.188% (2580/3574)      28/28 \n",
      "\n",
      "Epoch: 59\n",
      " [=============================>] | Loss: 0.798 | Acc: 70.929% (20258/28561)    224/224 \n",
      " [============================>.] | Loss: 0.771 | Acc: 74.434% (2664/3579)      28/28 \n",
      " [============================>.] | Loss: 0.798 | Acc: 72.720% (2599/3574)      28/28 \n",
      "\n",
      "Epoch: 60\n",
      " [=============================>] | Loss: 0.793 | Acc: 70.890% (20247/28561)    224/224 \n",
      " [============================>.] | Loss: 0.709 | Acc: 75.636% (2707/3579)      28/28 \n",
      " [============================>.] | Loss: 0.738 | Acc: 74.203% (2652/3574)      28/28 \n",
      "\n",
      "Epoch: 61\n",
      " [=============================>] | Loss: 0.787 | Acc: 71.167% (20326/28561)    224/224 \n",
      " [============================>.] | Loss: 0.712 | Acc: 76.222% (2728/3579)      28/28 \n",
      " [============================>.] | Loss: 0.738 | Acc: 74.147% (2650/3574)      28/28 \n",
      "\n",
      "Epoch: 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.787 | Acc: 71.177% (20329/28561)    224/224 \n",
      " [============================>.] | Loss: 0.724 | Acc: 75.915% (2717/3579)      28/28 \n",
      " [============================>.] | Loss: 0.756 | Acc: 73.587% (2630/3574)      28/28 \n",
      "\n",
      "Epoch: 63\n",
      " [=============================>] | Loss: 0.778 | Acc: 71.248% (20349/28561)    224/224 \n",
      " [============================>.] | Loss: 0.713 | Acc: 75.189% (2691/3579)      28/28 \n",
      " [============================>.] | Loss: 0.749 | Acc: 72.412% (2588/3574)      28/28 \n",
      "\n",
      "Epoch: 64\n",
      " [=============================>] | Loss: 0.778 | Acc: 71.440% (20404/28561)    224/224 \n",
      " [============================>.] | Loss: 0.710 | Acc: 75.049% (2686/3579)      28/28 \n",
      " [============================>.] | Loss: 0.741 | Acc: 73.727% (2635/3574)      28/28 \n",
      "\n",
      "Epoch: 65\n",
      " [=============================>] | Loss: 0.774 | Acc: 71.787% (20503/28561)    224/224 \n",
      " [============================>.] | Loss: 0.695 | Acc: 76.530% (2739/3579)      28/28 \n",
      " [============================>.] | Loss: 0.726 | Acc: 74.231% (2653/3574)      28/28 \n",
      "\n",
      "Epoch: 66\n",
      " [=============================>] | Loss: 0.765 | Acc: 71.920% (20541/28561)    224/224 \n",
      " [============================>.] | Loss: 0.723 | Acc: 73.987% (2648/3579)      28/28 \n",
      " [============================>.] | Loss: 0.753 | Acc: 73.419% (2624/3574)      28/28 \n",
      "\n",
      "Epoch: 67\n",
      " [=============================>] | Loss: 0.761 | Acc: 72.238% (20632/28561)    224/224 \n",
      " [============================>.] | Loss: 0.691 | Acc: 76.139% (2725/3579)      28/28 \n",
      " [============================>.] | Loss: 0.727 | Acc: 74.762% (2672/3574)      28/28 \n",
      "\n",
      "Epoch: 68\n",
      " [=============================>] | Loss: 0.760 | Acc: 72.235% (20631/28561)    224/224 \n",
      " [============================>.] | Loss: 0.678 | Acc: 75.412% (2699/3579)      28/28 \n",
      " [============================>.] | Loss: 0.721 | Acc: 74.091% (2648/3574)      28/28 \n",
      "\n",
      "Epoch: 69\n",
      " [=============================>] | Loss: 0.763 | Acc: 72.245% (20634/28561)    224/224 \n",
      " [============================>.] | Loss: 0.725 | Acc: 73.820% (2642/3579)      28/28 \n",
      " [============================>.] | Loss: 0.763 | Acc: 71.740% (2564/3574)      28/28 \n",
      "\n",
      "Epoch: 70\n",
      " [=============================>] | Loss: 0.752 | Acc: 72.466% (20697/28561)    224/224 \n",
      " [============================>.] | Loss: 0.690 | Acc: 76.502% (2738/3579)      28/28 \n",
      " [============================>.] | Loss: 0.721 | Acc: 74.510% (2663/3574)      28/28 \n",
      "\n",
      "Epoch: 71\n",
      " [=============================>] | Loss: 0.745 | Acc: 72.704% (20765/28561)    224/224 \n",
      " [============================>.] | Loss: 0.675 | Acc: 76.194% (2727/3579)      28/28 \n",
      " [============================>.] | Loss: 0.706 | Acc: 75.518% (2699/3574)      28/28 \n",
      "\n",
      "Epoch: 72\n",
      " [=============================>] | Loss: 0.741 | Acc: 72.645% (20748/28561)    224/224 \n",
      " [============================>.] | Loss: 0.671 | Acc: 76.278% (2730/3579)      28/28 \n",
      " [============================>.] | Loss: 0.709 | Acc: 74.762% (2672/3574)      28/28 \n",
      "\n",
      "Epoch: 73\n",
      " [=============================>] | Loss: 0.744 | Acc: 72.588% (20732/28561)    224/224 \n",
      " [============================>.] | Loss: 0.709 | Acc: 75.524% (2703/3579)      28/28 \n",
      " [============================>.] | Loss: 0.743 | Acc: 73.811% (2638/3574)      28/28 \n",
      "\n",
      "Epoch: 74\n",
      " [=============================>] | Loss: 0.745 | Acc: 72.497% (20706/28561)    224/224 \n",
      " [============================>.] | Loss: 0.705 | Acc: 74.546% (2668/3579)      28/28 \n",
      " [============================>.] | Loss: 0.743 | Acc: 72.692% (2598/3574)      28/28 \n",
      "\n",
      "Epoch: 75\n",
      " [=============================>] | Loss: 0.737 | Acc: 72.806% (20794/28561)    224/224 \n",
      " [============================>.] | Loss: 0.679 | Acc: 75.328% (2696/3579)      28/28 \n",
      " [============================>.] | Loss: 0.708 | Acc: 74.175% (2651/3574)      28/28 \n",
      "\n",
      "Epoch: 76\n",
      " [=============================>] | Loss: 0.735 | Acc: 72.883% (20816/28561)    224/224 \n",
      " [============================>.] | Loss: 0.692 | Acc: 76.250% (2729/3579)      28/28 \n",
      " [============================>.] | Loss: 0.726 | Acc: 74.063% (2647/3574)      28/28 \n",
      "\n",
      "Epoch: 77\n",
      " [=============================>] | Loss: 0.724 | Acc: 73.376% (20957/28561)    224/224 \n",
      " [============================>.] | Loss: 0.662 | Acc: 77.144% (2761/3579)      28/28 \n",
      " [============================>.] | Loss: 0.697 | Acc: 75.042% (2682/3574)      28/28 \n",
      "\n",
      "Epoch: 78\n",
      " [=============================>] | Loss: 0.730 | Acc: 73.418% (20969/28561)    224/224 \n",
      " [============================>.] | Loss: 0.662 | Acc: 76.027% (2721/3579)      28/28 \n",
      " [============================>.] | Loss: 0.700 | Acc: 74.203% (2652/3574)      28/28 \n",
      "\n",
      "Epoch: 79\n",
      " [=============================>] | Loss: 0.730 | Acc: 73.555% (21008/28561)    224/224 \n",
      " [============================>.] | Loss: 0.664 | Acc: 76.949% (2754/3579)      28/28 \n",
      " [============================>.] | Loss: 0.697 | Acc: 74.874% (2676/3574)      28/28 \n",
      "\n",
      "Epoch: 80\n",
      " [=============================>] | Loss: 0.722 | Acc: 73.271% (20927/28561)    224/224 \n",
      " [============================>.] | Loss: 0.647 | Acc: 77.117% (2760/3579)      28/28 \n",
      " [============================>.] | Loss: 0.681 | Acc: 75.350% (2693/3574)      28/28 \n",
      "\n",
      "Epoch: 81\n",
      " [=============================>] | Loss: 0.718 | Acc: 73.779% (21072/28561)    224/224 \n",
      " [============================>.] | Loss: 0.651 | Acc: 76.977% (2755/3579)      28/28 \n",
      " [============================>.] | Loss: 0.688 | Acc: 75.909% (2713/3574)      28/28 \n",
      "\n",
      "Epoch: 82\n",
      " [=============================>] | Loss: 0.716 | Acc: 73.859% (21095/28561)    224/224 \n",
      " [============================>.] | Loss: 0.652 | Acc: 77.200% (2763/3579)      28/28 \n",
      " [============================>.] | Loss: 0.689 | Acc: 75.406% (2695/3574)      28/28 \n",
      "\n",
      "Epoch: 83\n",
      " [=============================>] | Loss: 0.713 | Acc: 73.621% (21027/28561)    224/224 \n",
      " [============================>.] | Loss: 0.663 | Acc: 76.725% (2746/3579)      28/28 \n",
      " [============================>.] | Loss: 0.700 | Acc: 75.126% (2685/3574)      28/28 \n",
      "\n",
      "Epoch: 84\n",
      " [=============================>] | Loss: 0.702 | Acc: 74.035% (21145/28561)    224/224 \n",
      " [============================>.] | Loss: 0.652 | Acc: 78.430% (2807/3579)      28/28 \n",
      " [============================>.] | Loss: 0.677 | Acc: 76.329% (2728/3574)      28/28 \n",
      "\n",
      "Epoch: 85\n",
      " [=============================>] | Loss: 0.706 | Acc: 73.768% (21069/28561)    224/224 \n",
      " [============================>.] | Loss: 0.638 | Acc: 78.262% (2801/3579)      28/28 \n",
      " [============================>.] | Loss: 0.675 | Acc: 75.378% (2694/3574)      28/28 \n",
      "\n",
      "Epoch: 86\n",
      " [=============================>] | Loss: 0.701 | Acc: 74.203% (21193/28561)    224/224 \n",
      " [============================>.] | Loss: 0.632 | Acc: 78.793% (2820/3579)      28/28 \n",
      " [============================>.] | Loss: 0.671 | Acc: 75.909% (2713/3574)      28/28 \n",
      "\n",
      "Epoch: 87\n",
      " [=============================>] | Loss: 0.693 | Acc: 74.406% (21251/28561)    224/224 \n",
      " [============================>.] | Loss: 0.653 | Acc: 77.005% (2756/3579)      28/28 \n",
      " [============================>.] | Loss: 0.680 | Acc: 75.769% (2708/3574)      28/28 \n",
      "\n",
      "Epoch: 88\n",
      " [=============================>] | Loss: 0.703 | Acc: 74.224% (21199/28561)    224/224 \n",
      " [============================>.] | Loss: 0.650 | Acc: 78.039% (2793/3579)      28/28 \n",
      " [============================>.] | Loss: 0.680 | Acc: 75.909% (2713/3574)      28/28 \n",
      "\n",
      "Epoch: 89\n",
      " [=============================>] | Loss: 0.699 | Acc: 74.157% (21180/28561)    224/224 \n",
      " [============================>.] | Loss: 0.632 | Acc: 78.150% (2797/3579)      28/28 \n",
      " [============================>.] | Loss: 0.665 | Acc: 76.273% (2726/3574)      28/28 \n",
      "\n",
      "Epoch: 90\n",
      " [=============================>] | Loss: 0.693 | Acc: 74.381% (21244/28561)    224/224 \n",
      " [============================>.] | Loss: 0.649 | Acc: 77.508% (2774/3579)      28/28 \n",
      " [============================>.] | Loss: 0.680 | Acc: 75.881% (2712/3574)      28/28 \n",
      "\n",
      "Epoch: 91\n",
      " [=============================>] | Loss: 0.685 | Acc: 74.658% (21323/28561)    224/224 \n",
      " [============================>.] | Loss: 0.637 | Acc: 77.256% (2765/3579)      28/28 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================>.] | Loss: 0.666 | Acc: 76.357% (2729/3574)      28/28 \n",
      "\n",
      "Epoch: 92\n",
      " [=============================>] | Loss: 0.683 | Acc: 74.749% (21349/28561)    224/224 \n",
      " [============================>.] | Loss: 0.661 | Acc: 76.921% (2753/3579)      28/28 \n",
      " [============================>.] | Loss: 0.690 | Acc: 74.566% (2665/3574)      28/28 \n",
      "\n",
      "Epoch: 93\n",
      " [=============================>] | Loss: 0.690 | Acc: 74.892% (21390/28561)    224/224 \n",
      " [============================>.] | Loss: 0.641 | Acc: 77.368% (2769/3579)      28/28 \n",
      " [============================>.] | Loss: 0.667 | Acc: 76.105% (2720/3574)      28/28 \n",
      "\n",
      "Epoch: 94\n",
      " [=============================>] | Loss: 0.687 | Acc: 74.871% (21384/28561)    224/224 \n",
      " [============================>.] | Loss: 0.620 | Acc: 78.402% (2806/3579)      28/28 \n",
      " [============================>.] | Loss: 0.651 | Acc: 76.469% (2733/3574)      28/28 \n",
      "\n",
      "Epoch: 95\n",
      " [=============================>] | Loss: 0.679 | Acc: 75.039% (21432/28561)    224/224 \n",
      " [============================>.] | Loss: 0.632 | Acc: 77.647% (2779/3579)      28/28 \n",
      " [============================>.] | Loss: 0.668 | Acc: 76.105% (2720/3574)      28/28 \n",
      "\n",
      "Epoch: 96\n",
      " [=============================>] | Loss: 0.672 | Acc: 75.162% (21467/28561)    224/224 \n",
      " [============================>.] | Loss: 0.627 | Acc: 77.871% (2787/3579)      28/28 \n",
      " [============================>.] | Loss: 0.665 | Acc: 75.294% (2691/3574)      28/28 \n",
      "\n",
      "Epoch: 97\n",
      " [=============================>] | Loss: 0.673 | Acc: 75.074% (21442/28561)    224/224 \n",
      " [============================>.] | Loss: 0.620 | Acc: 78.178% (2798/3579)      28/28 \n",
      " [============================>.] | Loss: 0.649 | Acc: 76.329% (2728/3574)      28/28 \n",
      "\n",
      "Epoch: 98\n",
      " [=============================>] | Loss: 0.667 | Acc: 75.326% (21514/28561)    224/224 \n",
      " [============================>.] | Loss: 0.628 | Acc: 77.843% (2786/3579)      28/28 \n",
      " [============================>.] | Loss: 0.662 | Acc: 76.413% (2731/3574)      28/28 \n",
      "\n",
      "Epoch: 99\n",
      " [=============================>] | Loss: 0.673 | Acc: 75.134% (21459/28561)    224/224 \n",
      " [============================>.] | Loss: 0.634 | Acc: 78.541% (2811/3579)      28/28 \n",
      " [============================>.] | Loss: 0.670 | Acc: 76.385% (2730/3574)      28/28 \n",
      "\n",
      "Epoch: 100\n",
      " [=============================>] | Loss: 0.663 | Acc: 75.351% (21521/28561)    224/224 \n",
      " [============================>.] | Loss: 0.625 | Acc: 77.983% (2791/3579)      28/28 \n",
      " [============================>.] | Loss: 0.654 | Acc: 76.385% (2730/3574)      28/28 \n",
      "\n",
      "Epoch: 101\n",
      " [=============================>] | Loss: 0.664 | Acc: 75.260% (21495/28561)    224/224 \n",
      " [============================>.] | Loss: 0.609 | Acc: 78.653% (2815/3579)      28/28 \n",
      " [============================>.] | Loss: 0.642 | Acc: 77.308% (2763/3574)      28/28 \n",
      "\n",
      "Epoch: 102\n",
      " [=============================>] | Loss: 0.664 | Acc: 75.435% (21545/28561)    224/224 \n",
      " [============================>.] | Loss: 0.619 | Acc: 79.100% (2831/3579)      28/28 \n",
      " [============================>.] | Loss: 0.641 | Acc: 76.497% (2734/3574)      28/28 \n",
      "\n",
      "Epoch: 103\n",
      " [=============================>] | Loss: 0.652 | Acc: 75.642% (21604/28561)    224/224 \n",
      " [============================>.] | Loss: 0.620 | Acc: 78.150% (2797/3579)      28/28 \n",
      " [============================>.] | Loss: 0.650 | Acc: 75.741% (2707/3574)      28/28 \n",
      "\n",
      "Epoch: 104\n",
      " [=============================>] | Loss: 0.658 | Acc: 75.340% (21518/28561)    224/224 \n",
      " [============================>.] | Loss: 0.622 | Acc: 78.346% (2804/3579)      28/28 \n",
      " [============================>.] | Loss: 0.656 | Acc: 76.777% (2744/3574)      28/28 \n",
      "\n",
      "Epoch: 105\n",
      " [=============================>] | Loss: 0.650 | Acc: 75.897% (21677/28561)    224/224 \n",
      " [============================>.] | Loss: 0.608 | Acc: 78.262% (2801/3579)      28/28 \n",
      " [============================>.] | Loss: 0.637 | Acc: 76.441% (2732/3574)      28/28 \n",
      "\n",
      "Epoch: 106\n",
      " [=============================>] | Loss: 0.653 | Acc: 75.880% (21672/28561)    224/224 \n",
      " [============================>.] | Loss: 0.606 | Acc: 78.625% (2814/3579)      28/28 \n",
      " [============================>.] | Loss: 0.642 | Acc: 76.385% (2730/3574)      28/28 \n",
      "\n",
      "Epoch: 107\n",
      " [=============================>] | Loss: 0.654 | Acc: 75.918% (21683/28561)    224/224 \n",
      " [============================>.] | Loss: 0.605 | Acc: 78.737% (2818/3579)      28/28 \n",
      " [============================>.] | Loss: 0.641 | Acc: 76.693% (2741/3574)      28/28 \n",
      "\n",
      "Epoch: 108\n",
      " [=============================>] | Loss: 0.648 | Acc: 75.876% (21671/28561)    224/224 \n",
      " [============================>.] | Loss: 0.598 | Acc: 79.128% (2832/3579)      28/28 \n",
      " [============================>.] | Loss: 0.631 | Acc: 77.588% (2773/3574)      28/28 \n",
      "\n",
      "Epoch: 109\n",
      " [=============================>] | Loss: 0.644 | Acc: 76.139% (21746/28561)    224/224 \n",
      " [============================>.] | Loss: 0.613 | Acc: 79.324% (2839/3579)      28/28 \n",
      " [============================>.] | Loss: 0.650 | Acc: 76.581% (2737/3574)      28/28 \n",
      "\n",
      "Epoch: 110\n",
      " [=============================>] | Loss: 0.646 | Acc: 76.398% (21820/28561)    224/224 \n",
      " [============================>.] | Loss: 0.596 | Acc: 79.072% (2830/3579)      28/28 \n",
      " [============================>.] | Loss: 0.636 | Acc: 76.581% (2737/3574)      28/28 \n",
      "\n",
      "Epoch: 111\n",
      " [=============================>] | Loss: 0.638 | Acc: 76.163% (21753/28561)    224/224 \n",
      " [============================>.] | Loss: 0.608 | Acc: 79.240% (2836/3579)      28/28 \n",
      " [============================>.] | Loss: 0.639 | Acc: 76.917% (2749/3574)      28/28 \n",
      "\n",
      "Epoch: 112\n",
      " [=============================>] | Loss: 0.645 | Acc: 75.764% (21639/28561)    224/224 \n",
      " [============================>.] | Loss: 0.607 | Acc: 78.402% (2806/3579)      28/28 \n",
      " [============================>.] | Loss: 0.640 | Acc: 76.777% (2744/3574)      28/28 \n",
      "\n",
      "Epoch: 113\n",
      " [=============================>] | Loss: 0.631 | Acc: 76.615% (21882/28561)    224/224 \n",
      " [============================>.] | Loss: 0.605 | Acc: 79.296% (2838/3579)      28/28 \n",
      " [============================>.] | Loss: 0.639 | Acc: 76.805% (2745/3574)      28/28 \n",
      "\n",
      "Epoch: 114\n",
      " [=============================>] | Loss: 0.637 | Acc: 76.548% (21863/28561)    224/224 \n",
      " [============================>.] | Loss: 0.607 | Acc: 78.765% (2819/3579)      28/28 \n",
      " [============================>.] | Loss: 0.640 | Acc: 76.721% (2742/3574)      28/28 \n",
      "\n",
      "Epoch: 115\n",
      " [=============================>] | Loss: 0.630 | Acc: 76.688% (21903/28561)    224/224 \n",
      " [============================>.] | Loss: 0.593 | Acc: 79.156% (2833/3579)      28/28 \n",
      " [============================>.] | Loss: 0.623 | Acc: 77.308% (2763/3574)      28/28 \n",
      "\n",
      "Epoch: 116\n",
      " [=============================>] | Loss: 0.633 | Acc: 76.342% (21804/28561)    224/224 \n",
      " [============================>.] | Loss: 0.587 | Acc: 79.799% (2856/3579)      28/28 \n",
      " [============================>.] | Loss: 0.617 | Acc: 77.196% (2759/3574)      28/28 \n",
      "\n",
      "Epoch: 117\n",
      " [=============================>] | Loss: 0.630 | Acc: 76.384% (21816/28561)    224/224 \n",
      " [============================>.] | Loss: 0.596 | Acc: 79.324% (2839/3579)      28/28 \n",
      " [============================>.] | Loss: 0.622 | Acc: 77.896% (2784/3574)      28/28 \n",
      "\n",
      "Epoch: 118\n",
      " [=============================>] | Loss: 0.625 | Acc: 76.843% (21947/28561)    224/224 \n",
      " [============================>.] | Loss: 0.598 | Acc: 79.240% (2836/3579)      28/28 \n",
      " [============================>.] | Loss: 0.628 | Acc: 76.945% (2750/3574)      28/28 \n",
      "\n",
      "Epoch: 119\n",
      " [=============================>] | Loss: 0.625 | Acc: 76.727% (21914/28561)    224/224 \n",
      " [============================>.] | Loss: 0.582 | Acc: 79.380% (2841/3579)      28/28 \n",
      " [============================>.] | Loss: 0.612 | Acc: 77.448% (2768/3574)      28/28 \n",
      "\n",
      "Epoch: 120\n",
      " [=============================>] | Loss: 0.623 | Acc: 77.228% (22057/28561)    224/224 \n",
      " [============================>.] | Loss: 0.586 | Acc: 79.464% (2844/3579)      28/28 \n",
      " [============================>.] | Loss: 0.614 | Acc: 77.336% (2764/3574)      28/28 \n",
      "\n",
      "Epoch: 121\n",
      " [============================>.] | Loss: 0.585 | Acc: 79.883% (2859/3579)      28/28 4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================>.] | Loss: 0.619 | Acc: 77.644% (2775/3574)      28/28 \n",
      "\n",
      "Epoch: 125\n",
      " [=============================>] | Loss: 0.610 | Acc: 77.546% (22148/28561)    224/224 \n",
      " [============================>.] | Loss: 0.580 | Acc: 79.575% (2848/3579)      28/28 \n",
      " [============================>.] | Loss: 0.614 | Acc: 77.700% (2777/3574)      28/28 \n",
      "\n",
      "Epoch: 126\n",
      " [=============================>] | Loss: 0.608 | Acc: 77.448% (22120/28561)    224/224 \n",
      " [============================>.] | Loss: 0.584 | Acc: 79.827% (2857/3579)      28/28 \n",
      " [============================>.] | Loss: 0.623 | Acc: 77.084% (2755/3574)      28/28 \n",
      "\n",
      "Epoch: 127\n",
      " [=============================>] | Loss: 0.617 | Acc: 77.091% (22018/28561)    224/224 \n",
      " [============================>.] | Loss: 0.586 | Acc: 79.156% (2833/3579)      28/28 \n",
      " [============================>.] | Loss: 0.623 | Acc: 77.084% (2755/3574)      28/28 \n",
      "\n",
      "Epoch: 128\n",
      " [=============================>] | Loss: 0.605 | Acc: 77.448% (22120/28561)    224/224 \n",
      " [============================>.] | Loss: 0.580 | Acc: 79.436% (2843/3579)      28/28 \n",
      " [============================>.] | Loss: 0.616 | Acc: 77.420% (2767/3574)      28/28 \n",
      "\n",
      "Epoch: 129\n",
      " [=============================>] | Loss: 0.610 | Acc: 77.525% (22142/28561)    224/224 \n",
      " [============================>.] | Loss: 0.581 | Acc: 79.966% (2862/3579)      28/28 \n",
      " [============================>.] | Loss: 0.620 | Acc: 77.924% (2785/3574)      28/28 \n",
      "\n",
      "Epoch: 130\n",
      " [=============================>] | Loss: 0.606 | Acc: 77.431% (22115/28561)    224/224 \n",
      " [============================>.] | Loss: 0.575 | Acc: 79.603% (2849/3579)      28/28 \n",
      " [============================>.] | Loss: 0.614 | Acc: 77.029% (2753/3574)      28/28 \n",
      "\n",
      "Epoch: 131\n",
      " [=============================>] | Loss: 0.595 | Acc: 77.970% (22269/28561)    224/224 \n",
      " [============================>.] | Loss: 0.576 | Acc: 79.855% (2858/3579)      28/28 \n",
      " [============================>.] | Loss: 0.612 | Acc: 77.812% (2781/3574)      28/28 \n",
      "\n",
      "Epoch: 132\n",
      " [=============================>] | Loss: 0.600 | Acc: 77.900% (22249/28561)    224/224 \n",
      " [============================>.] | Loss: 0.585 | Acc: 79.715% (2853/3579)      28/28 \n",
      " [============================>.] | Loss: 0.618 | Acc: 78.092% (2791/3574)      28/28 \n",
      "\n",
      "Epoch: 133\n",
      " [=============================>] | Loss: 0.603 | Acc: 77.294% (22076/28561)    224/224 \n",
      " [============================>.] | Loss: 0.579 | Acc: 79.380% (2841/3579)      28/28 \n",
      " [============================>.] | Loss: 0.610 | Acc: 77.812% (2781/3574)      28/28 \n",
      "\n",
      "Epoch: 134\n",
      " [=============================>] | Loss: 0.597 | Acc: 77.658% (22180/28561)    224/224 \n",
      " [============================>.] | Loss: 0.579 | Acc: 79.966% (2862/3579)      28/28 \n",
      " [============================>.] | Loss: 0.609 | Acc: 77.588% (2773/3574)      28/28 \n",
      "\n",
      "Epoch: 135\n",
      " [=============================>] | Loss: 0.595 | Acc: 77.973% (22270/28561)    224/224 \n",
      " [============================>.] | Loss: 0.566 | Acc: 80.414% (2878/3579)      28/28 \n",
      " [============================>.] | Loss: 0.605 | Acc: 77.728% (2778/3574)      28/28 \n",
      "\n",
      "Epoch: 136\n",
      " [=============================>] | Loss: 0.593 | Acc: 77.966% (22268/28561)    224/224 \n",
      " [============================>.] | Loss: 0.577 | Acc: 79.436% (2843/3579)      28/28 \n",
      " [============================>.] | Loss: 0.619 | Acc: 77.112% (2756/3574)      28/28 \n",
      "\n",
      "Epoch: 137\n",
      " [=============================>] | Loss: 0.593 | Acc: 77.693% (22190/28561)    224/224 \n",
      " [============================>.] | Loss: 0.570 | Acc: 79.827% (2857/3579)      28/28 \n",
      " [============================>.] | Loss: 0.609 | Acc: 77.560% (2772/3574)      28/28 \n",
      "\n",
      "Epoch: 138\n",
      " [=============================>] | Loss: 0.590 | Acc: 77.742% (22204/28561)    224/224 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.687% (2852/3579)      28/28 \n",
      " [============================>.] | Loss: 0.604 | Acc: 78.148% (2793/3574)      28/28 \n",
      "\n",
      "Epoch: 139\n",
      " [=============================>] | Loss: 0.582 | Acc: 78.492% (22418/28561)    224/224 \n",
      " [============================>.] | Loss: 0.575 | Acc: 79.408% (2842/3579)      28/28 \n",
      " [============================>.] | Loss: 0.609 | Acc: 77.812% (2781/3574)      28/28 \n",
      "\n",
      "Epoch: 140\n",
      " [=============================>] | Loss: 0.585 | Acc: 78.289% (22360/28561)    224/224 \n",
      " [============================>.] | Loss: 0.569 | Acc: 79.659% (2851/3579)      28/28 \n",
      " [============================>.] | Loss: 0.600 | Acc: 78.008% (2788/3574)      28/28 \n",
      "\n",
      "Epoch: 141\n",
      " [=============================>] | Loss: 0.587 | Acc: 77.938% (22260/28561)    224/224 \n",
      " [============================>.] | Loss: 0.573 | Acc: 79.939% (2861/3579)      28/28 \n",
      " [============================>.] | Loss: 0.600 | Acc: 77.980% (2787/3574)      28/28 \n",
      "\n",
      "Epoch: 142\n",
      " [=============================>] | Loss: 0.585 | Acc: 78.310% (22366/28561)    224/224 \n",
      " [============================>.] | Loss: 0.564 | Acc: 79.659% (2851/3579)      28/28 \n",
      " [============================>.] | Loss: 0.603 | Acc: 78.148% (2793/3574)      28/28 \n",
      "\n",
      "Epoch: 143\n",
      " [=============================>] | Loss: 0.581 | Acc: 78.502% (22421/28561)    224/224 \n",
      " [============================>.] | Loss: 0.572 | Acc: 79.603% (2849/3579)      28/28 \n",
      " [============================>.] | Loss: 0.607 | Acc: 77.672% (2776/3574)      28/28 \n",
      "\n",
      "Epoch: 144\n",
      " [=============================>] | Loss: 0.589 | Acc: 78.208% (22337/28561)    224/224 \n",
      " [============================>.] | Loss: 0.572 | Acc: 79.939% (2861/3579)      28/28 \n",
      " [============================>.] | Loss: 0.609 | Acc: 77.476% (2769/3574)      28/28 \n",
      "\n",
      "Epoch: 145\n",
      " [=============================>] | Loss: 0.578 | Acc: 78.558% (22437/28561)    224/224 \n",
      " [============================>.] | Loss: 0.566 | Acc: 79.966% (2862/3579)      28/28 \n",
      " [============================>.] | Loss: 0.610 | Acc: 77.812% (2781/3574)      28/28 \n",
      "\n",
      "Epoch: 146\n",
      " [=============================>] | Loss: 0.572 | Acc: 79.031% (22572/28561)    224/224 \n",
      " [============================>.] | Loss: 0.565 | Acc: 79.883% (2859/3579)      28/28 \n",
      " [============================>.] | Loss: 0.597 | Acc: 78.511% (2806/3574)      28/28 \n",
      "\n",
      "Epoch: 147\n",
      " [=============================>] | Loss: 0.567 | Acc: 78.740% (22489/28561)    224/224 \n",
      " [============================>.] | Loss: 0.561 | Acc: 79.994% (2863/3579)      28/28 \n",
      " [============================>.] | Loss: 0.600 | Acc: 78.008% (2788/3574)      28/28 \n",
      "\n",
      "Epoch: 148\n",
      " [=============================>] | Loss: 0.573 | Acc: 78.632% (22458/28561)    224/224 \n",
      " [============================>.] | Loss: 0.560 | Acc: 80.134% (2868/3579)      28/28 \n",
      " [============================>.] | Loss: 0.595 | Acc: 77.896% (2784/3574)      28/28 \n",
      "\n",
      "Epoch: 149\n",
      " [=============================>] | Loss: 0.572 | Acc: 78.583% (22444/28561)    224/224 \n",
      " [============================>.] | Loss: 0.555 | Acc: 80.525% (2882/3579)      28/28 \n",
      " [============================>.] | Loss: 0.589 | Acc: 78.400% (2802/3574)      28/28 \n",
      "\n",
      "Epoch: 150\n",
      " [=============================>] | Loss: 0.573 | Acc: 78.544% (22433/28561)    224/224 \n",
      " [============================>.] | Loss: 0.575 | Acc: 79.603% (2849/3579)      28/28 \n",
      " [============================>.] | Loss: 0.606 | Acc: 77.756% (2779/3574)      28/28 \n",
      "\n",
      "Epoch: 151\n",
      " [=============================>] | Loss: 0.569 | Acc: 78.558% (22437/28561)    224/224 \n",
      " [============================>.] | Loss: 0.558 | Acc: 80.218% (2871/3579)      28/28 \n",
      " [============================>.] | Loss: 0.589 | Acc: 78.316% (2799/3574)      28/28 \n",
      "\n",
      "Epoch: 152\n",
      " [=============================>] | Loss: 0.567 | Acc: 79.094% (22590/28561)    224/224 \n",
      " [============================>.] | Loss: 0.554 | Acc: 80.665% (2887/3579)      28/28 \n",
      " [============================>.] | Loss: 0.589 | Acc: 78.567% (2808/3574)      28/28 \n",
      "\n",
      "Epoch: 153\n",
      " [=============================>] | Loss: 0.565 | Acc: 79.066% (22582/28561)    224/224 \n",
      " [============================>.] | Loss: 0.561 | Acc: 79.715% (2853/3579)      28/28 \n",
      " [============================>.] | Loss: 0.597 | Acc: 78.176% (2794/3574)      28/28 \n",
      "\n",
      "Epoch: 154\n",
      " [=============================>] | Loss: 0.566 | Acc: 78.800% (22506/28561)    224/224 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================>.] | Loss: 0.551 | Acc: 80.749% (2890/3579)      28/28 \n",
      " [============================>.] | Loss: 0.584 | Acc: 78.595% (2809/3574)      28/28 \n",
      "\n",
      "Epoch: 155\n",
      " [=============================>] | Loss: 0.555 | Acc: 79.262% (22638/28561)    224/224 \n",
      " [============================>.] | Loss: 0.556 | Acc: 80.050% (2865/3579)      28/28 \n",
      " [============================>.] | Loss: 0.590 | Acc: 78.511% (2806/3574)      28/28 \n",
      "\n",
      "Epoch: 156\n",
      " [=============================>] | Loss: 0.556 | Acc: 79.311% (22652/28561)    224/224 \n",
      " [============================>.] | Loss: 0.570 | Acc: 79.883% (2859/3579)      28/28 \n",
      " [============================>.] | Loss: 0.604 | Acc: 78.036% (2789/3574)      28/28 \n",
      "\n",
      "Epoch: 157\n",
      " [=============================>] | Loss: 0.547 | Acc: 79.553% (22721/28561)    224/224 \n",
      " [============================>.] | Loss: 0.552 | Acc: 80.525% (2882/3579)      28/28 \n",
      " [============================>.] | Loss: 0.584 | Acc: 78.372% (2801/3574)      28/28 \n",
      "\n",
      "Epoch: 158\n",
      " [=============================>] | Loss: 0.551 | Acc: 79.469% (22697/28561)    224/224 \n",
      " [============================>.] | Loss: 0.558 | Acc: 80.162% (2869/3579)      28/28 \n",
      " [============================>.] | Loss: 0.591 | Acc: 78.064% (2790/3574)      28/28 \n",
      "\n",
      "Epoch: 159\n",
      " [=============================>] | Loss: 0.558 | Acc: 79.181% (22615/28561)    224/224 \n",
      " [============================>.] | Loss: 0.556 | Acc: 79.911% (2860/3579)      28/28 \n",
      " [============================>.] | Loss: 0.596 | Acc: 78.092% (2791/3574)      28/28 \n",
      "\n",
      "Epoch: 160\n",
      " [=============================>] | Loss: 0.554 | Acc: 79.220% (22626/28561)    224/224 \n",
      " [============================>.] | Loss: 0.559 | Acc: 80.497% (2881/3579)      28/28 \n",
      " [============================>.] | Loss: 0.591 | Acc: 78.232% (2796/3574)      28/28 \n",
      "\n",
      "Epoch: 161\n",
      " [=============================>] | Loss: 0.549 | Acc: 79.462% (22695/28561)    224/224 \n",
      " [============================>.] | Loss: 0.559 | Acc: 79.911% (2860/3579)      28/28 \n",
      " [============================>.] | Loss: 0.590 | Acc: 78.232% (2796/3574)      28/28 \n",
      "\n",
      "Epoch: 162\n",
      " [=============================>] | Loss: 0.552 | Acc: 79.556% (22722/28561)    224/224 \n",
      " [============================>.] | Loss: 0.555 | Acc: 80.441% (2879/3579)      28/28 \n",
      " [============================>.] | Loss: 0.590 | Acc: 78.428% (2803/3574)      28/28 \n",
      "\n",
      "Epoch: 163\n",
      " [=============================>] | Loss: 0.544 | Acc: 79.707% (22765/28561)    224/224 \n",
      " [============================>.] | Loss: 0.556 | Acc: 79.939% (2861/3579)      28/28 \n",
      " [============================>.] | Loss: 0.585 | Acc: 78.288% (2798/3574)      28/28 \n",
      "\n",
      "Epoch: 164\n",
      " [=============================>] | Loss: 0.548 | Acc: 79.738% (22774/28561)    224/224 \n",
      " [============================>.] | Loss: 0.560 | Acc: 80.246% (2872/3579)      28/28 \n",
      " [============================>.] | Loss: 0.601 | Acc: 77.672% (2776/3574)      28/28 \n",
      "\n",
      "Epoch: 165\n",
      " [=============================>] | Loss: 0.546 | Acc: 79.447% (22691/28561)    224/224 \n",
      " [============================>.] | Loss: 0.552 | Acc: 80.134% (2868/3579)      28/28 \n",
      " [============================>.] | Loss: 0.591 | Acc: 78.288% (2798/3574)      28/28 \n",
      "\n",
      "Epoch: 166\n",
      " [=============================>] | Loss: 0.539 | Acc: 79.829% (22800/28561)    224/224 \n",
      " [============================>.] | Loss: 0.556 | Acc: 80.274% (2873/3579)      28/28 \n",
      " [============================>.] | Loss: 0.589 | Acc: 77.952% (2786/3574)      28/28 \n",
      "\n",
      "Epoch: 167\n",
      " [=============================>] | Loss: 0.534 | Acc: 79.826% (22799/28561)    224/224 \n",
      " [============================>.] | Loss: 0.540 | Acc: 80.637% (2886/3579)      28/28 \n",
      " [============================>.] | Loss: 0.579 | Acc: 78.316% (2799/3574)      28/28 \n",
      "\n",
      "Epoch: 168\n",
      " [=============================>] | Loss: 0.529 | Acc: 80.155% (22893/28561)    224/224 \n",
      " [============================>.] | Loss: 0.560 | Acc: 80.246% (2872/3579)      28/28 \n",
      " [============================>.] | Loss: 0.597 | Acc: 78.176% (2794/3574)      28/28 \n",
      "\n",
      "Epoch: 169\n",
      " [=============================>] | Loss: 0.545 | Acc: 79.773% (22784/28561)    224/224 \n",
      " [============================>.] | Loss: 0.551 | Acc: 80.525% (2882/3579)      28/28 \n",
      " [============================>.] | Loss: 0.579 | Acc: 78.931% (2821/3574)      28/28 \n",
      "\n",
      "Epoch: 170\n",
      " [=============================>] | Loss: 0.528 | Acc: 80.043% (22861/28561)    224/224 \n",
      " [============================>.] | Loss: 0.549 | Acc: 80.386% (2877/3579)      28/28 \n",
      " [============================>.] | Loss: 0.579 | Acc: 78.623% (2810/3574)      28/28 \n",
      "\n",
      "Epoch: 171\n",
      " [=============================>] | Loss: 0.528 | Acc: 80.165% (22896/28561)    224/224 \n",
      " [============================>.] | Loss: 0.549 | Acc: 80.497% (2881/3579)      28/28 \n",
      " [============================>.] | Loss: 0.582 | Acc: 78.595% (2809/3574)      28/28 \n",
      "\n",
      "Epoch: 172\n",
      " [=============================>] | Loss: 0.531 | Acc: 80.001% (22849/28561)    224/224 \n",
      " [============================>.] | Loss: 0.556 | Acc: 80.022% (2864/3579)      28/28 \n",
      " [============================>.] | Loss: 0.592 | Acc: 78.483% (2805/3574)      28/28 \n",
      "\n",
      "Epoch: 173\n",
      " [=============================>] | Loss: 0.529 | Acc: 80.232% (22915/28561)    224/224 \n",
      " [============================>.] | Loss: 0.552 | Acc: 80.386% (2877/3579)      28/28 \n",
      " [============================>.] | Loss: 0.582 | Acc: 78.372% (2801/3574)      28/28 \n",
      "\n",
      "Epoch: 174\n",
      " [=============================>] | Loss: 0.532 | Acc: 79.987% (22845/28561)    224/224 \n",
      " [============================>.] | Loss: 0.556 | Acc: 80.637% (2886/3579)      28/28 \n",
      " [============================>.] | Loss: 0.591 | Acc: 78.539% (2807/3574)      28/28 \n",
      "\n",
      "Epoch: 175\n",
      " [=============================>] | Loss: 0.522 | Acc: 80.463% (22981/28561)    224/224 \n",
      " [============================>.] | Loss: 0.553 | Acc: 80.330% (2875/3579)      28/28 \n",
      " [============================>.] | Loss: 0.580 | Acc: 78.847% (2818/3574)      28/28 \n",
      "\n",
      "Epoch: 176\n",
      " [=============================>] | Loss: 0.525 | Acc: 80.323% (22941/28561)    224/224 \n",
      " [============================>.] | Loss: 0.548 | Acc: 80.469% (2880/3579)      28/28 \n",
      " [============================>.] | Loss: 0.580 | Acc: 78.511% (2806/3574)      28/28 \n",
      "\n",
      "Epoch: 177\n",
      " [=============================>] | Loss: 0.527 | Acc: 80.120% (22883/28561)    224/224 \n",
      " [============================>.] | Loss: 0.547 | Acc: 80.721% (2889/3579)      28/28 \n",
      " [============================>.] | Loss: 0.579 | Acc: 78.567% (2808/3574)      28/28 \n",
      "\n",
      "Epoch: 178\n",
      " [=============================>] | Loss: 0.526 | Acc: 80.421% (22969/28561)    224/224 \n",
      " [============================>.] | Loss: 0.549 | Acc: 80.497% (2881/3579)      28/28 \n",
      " [============================>.] | Loss: 0.583 | Acc: 78.064% (2790/3574)      28/28 \n",
      "\n",
      "Epoch: 179\n",
      " [=============================>] | Loss: 0.527 | Acc: 80.253% (22921/28561)    224/224 \n",
      " [============================>.] | Loss: 0.549 | Acc: 80.386% (2877/3579)      28/28 \n",
      " [============================>.] | Loss: 0.585 | Acc: 78.539% (2807/3574)      28/28 \n",
      "\n",
      "Epoch: 180\n",
      " [=============================>] | Loss: 0.517 | Acc: 80.442% (22975/28561)    224/224 \n",
      " [============================>.] | Loss: 0.546 | Acc: 80.330% (2875/3579)      28/28 \n",
      " [============================>.] | Loss: 0.576 | Acc: 78.679% (2812/3574)      28/28 \n",
      "\n",
      "Epoch: 181\n",
      " [=============================>] | Loss: 0.518 | Acc: 80.225% (22913/28561)    224/224 \n",
      " [============================>.] | Loss: 0.540 | Acc: 81.224% (2907/3579)      28/28 \n",
      " [============================>.] | Loss: 0.568 | Acc: 79.491% (2841/3574)      28/28 \n",
      "\n",
      "Epoch: 182\n",
      " [=============================>] | Loss: 0.517 | Acc: 80.669% (23040/28561)    224/224 \n",
      " [============================>.] | Loss: 0.541 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.578 | Acc: 78.847% (2818/3574)      28/28 \n",
      "\n",
      "Epoch: 183\n",
      " [=============================>] | Loss: 0.527 | Acc: 80.239% (22917/28561)    224/224 \n",
      " [============================>.] | Loss: 0.547 | Acc: 80.777% (2891/3579)      28/28 \n",
      " [============================>.] | Loss: 0.576 | Acc: 78.931% (2821/3574)      28/28 \n",
      "\n",
      "Epoch: 184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.520 | Acc: 80.403% (22964/28561)    224/224 \n",
      " [============================>.] | Loss: 0.547 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.582 | Acc: 78.735% (2814/3574)      28/28 \n",
      "\n",
      "Epoch: 185\n",
      " [=============================>] | Loss: 0.522 | Acc: 80.358% (22951/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.805% (2892/3579)      28/28 \n",
      " [============================>.] | Loss: 0.573 | Acc: 79.183% (2830/3574)      28/28 \n",
      "\n",
      "Epoch: 186\n",
      " [=============================>] | Loss: 0.512 | Acc: 80.561% (23009/28561)    224/224 \n",
      " [============================>.] | Loss: 0.547 | Acc: 80.497% (2881/3579)      28/28 \n",
      " [============================>.] | Loss: 0.583 | Acc: 78.456% (2804/3574)      28/28 \n",
      "\n",
      "Epoch: 187\n",
      " [=============================>] | Loss: 0.515 | Acc: 80.641% (23032/28561)    224/224 \n",
      " [============================>.] | Loss: 0.535 | Acc: 80.944% (2897/3579)      28/28 \n",
      " [============================>.] | Loss: 0.577 | Acc: 78.735% (2814/3574)      28/28 \n",
      "\n",
      "Epoch: 188\n",
      " [=============================>] | Loss: 0.510 | Acc: 80.901% (23106/28561)    224/224 \n",
      " [============================>.] | Loss: 0.548 | Acc: 80.497% (2881/3579)      28/28 \n",
      " [============================>.] | Loss: 0.582 | Acc: 78.651% (2811/3574)      28/28 \n",
      "\n",
      "Epoch: 189\n",
      " [=============================>] | Loss: 0.512 | Acc: 80.508% (22994/28561)    224/224 \n",
      " [============================>.] | Loss: 0.540 | Acc: 80.749% (2890/3579)      28/28 \n",
      " [============================>.] | Loss: 0.574 | Acc: 78.679% (2812/3574)      28/28 \n",
      "\n",
      "Epoch: 190\n",
      " [=============================>] | Loss: 0.510 | Acc: 80.697% (23048/28561)    224/224 \n",
      " [============================>.] | Loss: 0.551 | Acc: 80.441% (2879/3579)      28/28 \n",
      " [============================>.] | Loss: 0.589 | Acc: 78.316% (2799/3574)      28/28 \n",
      "\n",
      "Epoch: 191\n",
      " [=============================>] | Loss: 0.508 | Acc: 80.855% (23093/28561)    224/224 \n",
      " [============================>.] | Loss: 0.539 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.575 | Acc: 78.847% (2818/3574)      28/28 \n",
      "\n",
      "Epoch: 192\n",
      " [=============================>] | Loss: 0.515 | Acc: 80.904% (23107/28561)    224/224 \n",
      " [============================>.] | Loss: 0.532 | Acc: 81.364% (2912/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.211% (2831/3574)      28/28 \n",
      "\n",
      "Epoch: 193\n",
      " [=============================>] | Loss: 0.508 | Acc: 80.848% (23091/28561)    224/224 \n",
      " [============================>.] | Loss: 0.540 | Acc: 81.336% (2911/3579)      28/28 \n",
      " [============================>.] | Loss: 0.570 | Acc: 79.099% (2827/3574)      28/28 \n",
      "\n",
      "Epoch: 194\n",
      " [=============================>] | Loss: 0.506 | Acc: 81.055% (23150/28561)    224/224 \n",
      " [============================>.] | Loss: 0.542 | Acc: 80.861% (2894/3579)      28/28 \n",
      " [============================>.] | Loss: 0.574 | Acc: 78.931% (2821/3574)      28/28 \n",
      "\n",
      "Epoch: 195\n",
      " [=============================>] | Loss: 0.504 | Acc: 80.967% (23125/28561)    224/224 \n",
      " [============================>.] | Loss: 0.543 | Acc: 80.581% (2884/3579)      28/28 \n",
      " [============================>.] | Loss: 0.573 | Acc: 79.015% (2824/3574)      28/28 \n",
      "\n",
      "Epoch: 196\n",
      " [=============================>] | Loss: 0.507 | Acc: 81.188% (23188/28561)    224/224 \n",
      " [============================>.] | Loss: 0.535 | Acc: 81.224% (2907/3579)      28/28 \n",
      " [============================>.] | Loss: 0.575 | Acc: 78.903% (2820/3574)      28/28 \n",
      "\n",
      "Epoch: 197\n",
      " [=============================>] | Loss: 0.504 | Acc: 81.128% (23171/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.889% (2895/3579)      28/28 \n",
      " [============================>.] | Loss: 0.575 | Acc: 78.595% (2809/3574)      28/28 \n",
      "\n",
      "Epoch: 198\n",
      " [=============================>] | Loss: 0.496 | Acc: 81.436% (23259/28561)    224/224 \n",
      " [============================>.] | Loss: 0.546 | Acc: 80.441% (2879/3579)      28/28 \n",
      " [============================>.] | Loss: 0.582 | Acc: 78.428% (2803/3574)      28/28 \n",
      "\n",
      "Epoch: 199\n",
      " [=============================>] | Loss: 0.493 | Acc: 81.454% (23264/28561)    224/224 \n",
      " [============================>.] | Loss: 0.537 | Acc: 80.525% (2882/3579)      28/28 \n",
      " [============================>.] | Loss: 0.574 | Acc: 78.707% (2813/3574)      28/28 \n",
      "\n",
      "Epoch: 200\n",
      " [=============================>] | Loss: 0.495 | Acc: 81.520% (23283/28561)    224/224 \n",
      " [============================>.] | Loss: 0.537 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.015% (2824/3574)      28/28 \n",
      "\n",
      "Epoch: 201\n",
      " [=============================>] | Loss: 0.498 | Acc: 81.450% (23263/28561)    224/224 \n",
      " [============================>.] | Loss: 0.544 | Acc: 80.581% (2884/3579)      28/28 \n",
      " [============================>.] | Loss: 0.580 | Acc: 78.819% (2817/3574)      28/28 \n",
      "\n",
      "Epoch: 202\n",
      " [=============================>] | Loss: 0.493 | Acc: 81.321% (23226/28561)    224/224 \n",
      " [============================>.] | Loss: 0.542 | Acc: 80.944% (2897/3579)      28/28 \n",
      " [============================>.] | Loss: 0.579 | Acc: 78.819% (2817/3574)      28/28 \n",
      "\n",
      "Epoch: 203\n",
      " [=============================>] | Loss: 0.499 | Acc: 81.254% (23207/28561)    224/224 \n",
      " [============================>.] | Loss: 0.540 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.574 | Acc: 78.735% (2814/3574)      28/28 \n",
      "\n",
      "Epoch: 204\n",
      " [=============================>] | Loss: 0.486 | Acc: 81.926% (23399/28561)    224/224 \n",
      " [============================>.] | Loss: 0.534 | Acc: 81.028% (2900/3579)      28/28 \n",
      " [============================>.] | Loss: 0.565 | Acc: 79.603% (2845/3574)      28/28 \n",
      "\n",
      "Epoch: 208\n",
      " [=============================>] | Loss: 0.489 | Acc: 81.706% (23336/28561)    224/224 \n",
      " [============================>.] | Loss: 0.535 | Acc: 81.000% (2899/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.295% (2834/3574)      28/28 \n",
      "\n",
      "Epoch: 209\n",
      " [=============================>] | Loss: 0.491 | Acc: 81.401% (23249/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.573 | Acc: 78.903% (2820/3574)      28/28 \n",
      "\n",
      "Epoch: 210\n",
      " [=============================>] | Loss: 0.484 | Acc: 81.811% (23366/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.497% (2881/3579)      28/28 \n",
      " [============================>.] | Loss: 0.570 | Acc: 79.043% (2825/3574)      28/28 \n",
      "\n",
      "Epoch: 211\n",
      " [=============================>] | Loss: 0.488 | Acc: 81.709% (23337/28561)    224/224 \n",
      " [============================>.] | Loss: 0.535 | Acc: 81.056% (2901/3579)      28/28 \n",
      " [============================>.] | Loss: 0.570 | Acc: 78.903% (2820/3574)      28/28 \n",
      "\n",
      "Epoch: 212\n",
      " [=============================>] | Loss: 0.480 | Acc: 82.000% (23420/28561)    224/224 \n",
      " [============================>.] | Loss: 0.542 | Acc: 80.805% (2892/3579)      28/28 \n",
      " [============================>.] | Loss: 0.575 | Acc: 79.239% (2832/3574)      28/28 \n",
      "\n",
      "Epoch: 213\n",
      " [=============================>] | Loss: 0.484 | Acc: 81.744% (23347/28561)    224/224 \n",
      " [============================>.] | Loss: 0.546 | Acc: 80.525% (2882/3579)      28/28 \n",
      " [============================>.] | Loss: 0.572 | Acc: 79.183% (2830/3574)      28/28 \n",
      "\n",
      "Epoch: 214\n",
      " [=============================>] | Loss: 0.482 | Acc: 81.870% (23383/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.972% (2898/3579)      28/28 \n",
      " [============================>.] | Loss: 0.569 | Acc: 79.323% (2835/3574)      28/28 \n",
      "\n",
      "Epoch: 215\n",
      " [=============================>] | Loss: 0.484 | Acc: 81.706% (23336/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.721% (2889/3579)      28/28 \n",
      " [============================>.] | Loss: 0.570 | Acc: 79.015% (2824/3574)      28/28 \n",
      "\n",
      "Epoch: 216\n",
      " [=============================>] | Loss: 0.479 | Acc: 81.923% (23398/28561)    224/224 \n",
      " [============================>.] | Loss: 0.533 | Acc: 81.419% (2914/3579)      28/28 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================>.] | Loss: 0.565 | Acc: 79.351% (2836/3574)      28/28 \n",
      "\n",
      "Epoch: 217\n",
      " [=============================>] | Loss: 0.483 | Acc: 81.797% (23362/28561)    224/224 \n",
      " [============================>.] | Loss: 0.531 | Acc: 81.531% (2918/3579)      28/28 \n",
      " [============================>.] | Loss: 0.563 | Acc: 79.351% (2836/3574)      28/28 \n",
      "\n",
      "Epoch: 218\n",
      " [=============================>] | Loss: 0.477 | Acc: 82.000% (23420/28561)    224/224 \n",
      " [============================>.] | Loss: 0.535 | Acc: 80.665% (2887/3579)      28/28 \n",
      " [============================>.] | Loss: 0.567 | Acc: 79.099% (2827/3574)      28/28 \n",
      "\n",
      "Epoch: 219\n",
      " [=============================>] | Loss: 0.481 | Acc: 82.049% (23434/28561)    224/224 \n",
      " [============================>.] | Loss: 0.542 | Acc: 81.056% (2901/3579)      28/28 \n",
      " [============================>.] | Loss: 0.577 | Acc: 78.931% (2821/3574)      28/28 \n",
      "\n",
      "Epoch: 220\n",
      " [=============================>] | Loss: 0.477 | Acc: 82.115% (23453/28561)    224/224 \n",
      " [============================>.] | Loss: 0.537 | Acc: 80.833% (2893/3579)      28/28 \n",
      " [============================>.] | Loss: 0.565 | Acc: 79.323% (2835/3574)      28/28 \n",
      "\n",
      "Epoch: 221\n",
      " [=============================>] | Loss: 0.471 | Acc: 82.119% (23454/28561)    224/224 \n",
      " [============================>.] | Loss: 0.535 | Acc: 80.805% (2892/3579)      28/28 \n",
      " [============================>.] | Loss: 0.564 | Acc: 80.022% (2860/3574)      28/28 \n",
      "\n",
      "Epoch: 222\n",
      " [=============================>] | Loss: 0.476 | Acc: 82.214% (23481/28561)    224/224 \n",
      " [============================>.] | Loss: 0.539 | Acc: 81.000% (2899/3579)      28/28 \n",
      " [============================>.] | Loss: 0.567 | Acc: 79.463% (2840/3574)      28/28 \n",
      "\n",
      "Epoch: 223\n",
      " [=============================>] | Loss: 0.476 | Acc: 82.063% (23438/28561)    224/224 \n",
      " [============================>.] | Loss: 0.532 | Acc: 81.140% (2904/3579)      28/28 \n",
      " [============================>.] | Loss: 0.565 | Acc: 79.323% (2835/3574)      28/28 \n",
      "\n",
      "Epoch: 224\n",
      " [=============================>] | Loss: 0.470 | Acc: 82.319% (23511/28561)    224/224 \n",
      " [============================>.] | Loss: 0.541 | Acc: 80.469% (2880/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.099% (2827/3574)      28/28 \n",
      "\n",
      "Epoch: 225\n",
      " [=============================>] | Loss: 0.469 | Acc: 82.431% (23543/28561)    224/224 \n",
      " [============================>.] | Loss: 0.534 | Acc: 80.861% (2894/3579)      28/28 \n",
      " [============================>.] | Loss: 0.570 | Acc: 79.183% (2830/3574)      28/28 \n",
      "\n",
      "Epoch: 226\n",
      " [=============================>] | Loss: 0.474 | Acc: 82.382% (23529/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.693% (2888/3579)      28/28 \n",
      " [============================>.] | Loss: 0.574 | Acc: 79.015% (2824/3574)      28/28 \n",
      "\n",
      "Epoch: 227\n",
      " [=============================>] | Loss: 0.467 | Acc: 82.270% (23497/28561)    224/224 \n",
      " [============================>.] | Loss: 0.536 | Acc: 80.889% (2895/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 78.959% (2822/3574)      28/28 \n",
      "\n",
      "Epoch: 228\n",
      " [=============================>] | Loss: 0.474 | Acc: 82.329% (23514/28561)    224/224 \n",
      " [============================>.] | Loss: 0.534 | Acc: 80.944% (2897/3579)      28/28 \n",
      " [============================>.] | Loss: 0.568 | Acc: 79.435% (2839/3574)      28/28 \n",
      "\n",
      "Epoch: 229\n",
      " [=============================>] | Loss: 0.477 | Acc: 82.171% (23469/28561)    224/224 \n",
      " [============================>.] | Loss: 0.534 | Acc: 81.056% (2901/3579)      28/28 \n",
      " [============================>.] | Loss: 0.569 | Acc: 79.855% (2854/3574)      28/28 \n",
      "\n",
      "Epoch: 230\n",
      " [=============================>] | Loss: 0.468 | Acc: 82.424% (23541/28561)    224/224 \n",
      " [============================>.] | Loss: 0.539 | Acc: 80.944% (2897/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.351% (2836/3574)      28/28 \n",
      "\n",
      "Epoch: 231\n",
      " [=============================>] | Loss: 0.467 | Acc: 82.119% (23454/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.553% (2883/3579)      28/28 \n",
      " [============================>.] | Loss: 0.569 | Acc: 79.015% (2824/3574)      28/28 \n",
      "\n",
      "Epoch: 232\n",
      " [=============================>] | Loss: 0.468 | Acc: 82.224% (23484/28561)    224/224 \n",
      " [============================>.] | Loss: 0.530 | Acc: 81.084% (2902/3579)      28/28 \n",
      " [============================>.] | Loss: 0.560 | Acc: 79.519% (2842/3574)      28/28 \n",
      "\n",
      "Epoch: 233\n",
      " [=============================>] | Loss: 0.465 | Acc: 82.511% (23566/28561)    224/224 \n",
      " [============================>.] | Loss: 0.532 | Acc: 81.336% (2911/3579)      28/28 \n",
      " [============================>.] | Loss: 0.561 | Acc: 79.463% (2840/3574)      28/28 \n",
      "\n",
      "Epoch: 234\n",
      " [=============================>] | Loss: 0.463 | Acc: 82.690% (23617/28561)    224/224 \n",
      " [============================>.] | Loss: 0.536 | Acc: 81.112% (2903/3579)      28/28 \n",
      " [============================>.] | Loss: 0.571 | Acc: 79.323% (2835/3574)      28/28 \n",
      "\n",
      "Epoch: 235\n",
      " [=============================>] | Loss: 0.463 | Acc: 82.679% (23614/28561)    224/224 \n",
      " [============================>.] | Loss: 0.532 | Acc: 80.916% (2896/3579)      28/28 \n",
      " [============================>.] | Loss: 0.567 | Acc: 79.295% (2834/3574)      28/28 \n",
      "\n",
      "Epoch: 236\n",
      " [=============================>] | Loss: 0.456 | Acc: 82.886% (23673/28561)    224/224 \n",
      " [============================>.] | Loss: 0.533 | Acc: 80.944% (2897/3579)      28/28 \n",
      " [============================>.] | Loss: 0.564 | Acc: 79.267% (2833/3574)      28/28 \n",
      "\n",
      "Epoch: 237\n",
      " [=============================>] | Loss: 0.467 | Acc: 82.413% (23538/28561)    224/224 \n",
      " [============================>.] | Loss: 0.536 | Acc: 80.972% (2898/3579)      28/28 \n",
      " [============================>.] | Loss: 0.569 | Acc: 79.099% (2827/3574)      28/28 \n",
      "\n",
      "Epoch: 238\n",
      " [=============================>] | Loss: 0.463 | Acc: 82.518% (23568/28561)    224/224 \n",
      " [============================>.] | Loss: 0.537 | Acc: 81.112% (2903/3579)      28/28 \n",
      " [============================>.] | Loss: 0.569 | Acc: 79.155% (2829/3574)      28/28 \n",
      "\n",
      "Epoch: 239\n",
      " [=============================>] | Loss: 0.467 | Acc: 82.658% (23608/28561)    224/224 \n",
      " [============================>.] | Loss: 0.529 | Acc: 81.447% (2915/3579)      28/28 \n",
      " [============================>.] | Loss: 0.563 | Acc: 79.715% (2849/3574)      28/28 \n",
      "\n",
      "Epoch: 240\n",
      " [=============================>] | Loss: 0.465 | Acc: 82.361% (23523/28561)    224/224 \n",
      " [============================>.] | Loss: 0.534 | Acc: 81.196% (2906/3579)      28/28 \n",
      " [============================>.] | Loss: 0.567 | Acc: 79.491% (2841/3574)      28/28 \n",
      "\n",
      "Epoch: 241\n",
      " [=============================>] | Loss: 0.465 | Acc: 82.756% (23636/28561)    224/224 \n",
      " [============================>.] | Loss: 0.536 | Acc: 81.531% (2918/3579)      28/28 \n",
      " [============================>.] | Loss: 0.572 | Acc: 79.435% (2839/3574)      28/28 \n",
      "\n",
      "Epoch: 242\n",
      " [=============================>] | Loss: 0.469 | Acc: 82.256% (23493/28561)    224/224 \n",
      " [============================>.] | Loss: 0.527 | Acc: 81.447% (2915/3579)      28/28 \n",
      " [============================>.] | Loss: 0.564 | Acc: 79.575% (2844/3574)      28/28 \n",
      "\n",
      "Epoch: 243\n",
      " [=============================>] | Loss: 0.456 | Acc: 82.676% (23613/28561)    224/224 \n",
      " [============================>.] | Loss: 0.531 | Acc: 81.419% (2914/3579)      28/28 \n",
      " [============================>.] | Loss: 0.563 | Acc: 79.323% (2835/3574)      28/28 \n",
      "\n",
      "Epoch: 244\n",
      " [=============================>] | Loss: 0.460 | Acc: 82.539% (23574/28561)    224/224 \n",
      " [============================>.] | Loss: 0.533 | Acc: 81.140% (2904/3579)      28/28 \n",
      " [============================>.] | Loss: 0.568 | Acc: 79.127% (2828/3574)      28/28 \n",
      "\n",
      "Epoch: 245\n",
      " [=============================>] | Loss: 0.459 | Acc: 82.812% (23652/28561)    224/224 \n",
      " [============================>.] | Loss: 0.538 | Acc: 80.889% (2895/3579)      28/28 \n",
      " [============================>.] | Loss: 0.574 | Acc: 78.763% (2815/3574)      28/28 \n",
      "\n",
      "Epoch: 246\n",
      " [=============================>] | Loss: 0.453 | Acc: 82.788% (23645/28561)    224/224 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================>.] | Loss: 0.534 | Acc: 81.336% (2911/3579)      28/28 \n",
      " [============================>.] | Loss: 0.567 | Acc: 79.435% (2839/3574)      28/28 \n",
      "\n",
      "Epoch: 247\n",
      " [=============================>] | Loss: 0.455 | Acc: 82.690% (23617/28561)    224/224 \n",
      " [============================>.] | Loss: 0.528 | Acc: 81.391% (2913/3579)      28/28 \n",
      " [============================>.] | Loss: 0.564 | Acc: 79.267% (2833/3574)      28/28 \n",
      "\n",
      "Epoch: 248\n",
      " [=============================>] | Loss: 0.459 | Acc: 82.774% (23641/28561)    224/224 \n",
      " [============================>.] | Loss: 0.530 | Acc: 81.168% (2905/3579)      28/28 \n",
      " [============================>.] | Loss: 0.567 | Acc: 79.687% (2848/3574)      28/28 \n",
      "\n",
      "Epoch: 249\n",
      " [=============================>] | Loss: 0.461 | Acc: 82.851% (23663/28561)    224/224 \n",
      " [============================>.] | Loss: 0.537 | Acc: 80.749% (2890/3579)      28/28 \n",
      " [============================>.] | Loss: 0.570 | Acc: 78.791% (2816/3574)      28/28 \n",
      "best_PublicTest_acc: 81.531\n",
      "best_PublicTest_acc_epoch: 217\n",
      "best_PrivateTest_acc: 80.022\n",
      "best_PrivateTest_acc_epoch: 221\n"
     ]
    }
   ],
   "source": [
    "!python mainpro_FER_moblienetv2-tencopy.py --model mobilenetv2 --bs 128 --lr 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Traceback (most recent call last):\n",
      "  File \"mainpro_FER_moblienetv2.py\", line 77, in <module>\n",
      "    trainset = FER2013(split = 'Training', filename='./data/data_plus.h5',transform=transform_train,resize_length=48)\n",
      "  File \"/storage/expression_recognition/fer.py\", line 28, in __init__\n",
      "    self.train_data = self.train_data.reshape((train_length, resize_length, resize_length))\n",
      "ValueError: cannot reshape array of size 65804544 into shape (28709,48,48)\n"
     ]
    }
   ],
   "source": [
    "!python mainpro_FER_moblienetv2.py --model mobilenetv2 --bs 128 --lr 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/bb/771800366f41d66eef51e4b80515f8ef7edab234a3f244fdce3bafe89b39/scikit_image-0.16.2-cp36-cp36m-manylinux1_x86_64.whl (26.5MB)\n",
      "\u001b[K     |████████████████████████████████| 26.5MB 20.4MB/s eta 0:00:01     |██████████████████████████▊     | 22.1MB 20.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.1.1)\n",
      "Collecting imageio>=2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/de/f7f985018f462ceeffada7f6e609919fbcc934acd9301929cba14bc2c24a/imageio-2.6.1-py3-none-any.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 27.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 14.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.3.0)\n",
      "Collecting PyWavelets>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4MB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (6.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (41.0.1)\n",
      "Installing collected packages: imageio, networkx, PyWavelets, scikit-image\n",
      "Successfully installed PyWavelets-1.1.1 imageio-2.6.1 networkx-2.4 scikit-image-0.16.2\n",
      "Traceback (most recent call last):\n",
      "  File \"test_visualization.py\", line 44, in <module>\n",
      "    net.load_state_dict(checkpoint['net'])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 845, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for MobileNet:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"dw2_1.conv_dw.weight\", \"dw2_1.bn_dw.weight\", \"dw2_1.bn_dw.bias\", \"dw2_1.bn_dw.running_mean\", \"dw2_1.bn_dw.running_var\", \"dw2_1.conv_sep.weight\", \"dw2_1.bn_sep.weight\", \"dw2_1.bn_sep.bias\", \"dw2_1.bn_sep.running_mean\", \"dw2_1.bn_sep.running_var\", \"dw2_2.conv_dw.weight\", \"dw2_2.bn_dw.weight\", \"dw2_2.bn_dw.bias\", \"dw2_2.bn_dw.running_mean\", \"dw2_2.bn_dw.running_var\", \"dw2_2.conv_sep.weight\", \"dw2_2.bn_sep.weight\", \"dw2_2.bn_sep.bias\", \"dw2_2.bn_sep.running_mean\", \"dw2_2.bn_sep.running_var\", \"dw3_1.conv_dw.weight\", \"dw3_1.bn_dw.weight\", \"dw3_1.bn_dw.bias\", \"dw3_1.bn_dw.running_mean\", \"dw3_1.bn_dw.running_var\", \"dw3_1.conv_sep.weight\", \"dw3_1.bn_sep.weight\", \"dw3_1.bn_sep.bias\", \"dw3_1.bn_sep.running_mean\", \"dw3_1.bn_sep.running_var\", \"dw3_2.conv_dw.weight\", \"dw3_2.bn_dw.weight\", \"dw3_2.bn_dw.bias\", \"dw3_2.bn_dw.running_mean\", \"dw3_2.bn_dw.running_var\", \"dw3_2.conv_sep.weight\", \"dw3_2.bn_sep.weight\", \"dw3_2.bn_sep.bias\", \"dw3_2.bn_sep.running_mean\", \"dw3_2.bn_sep.running_var\", \"dw4_1.conv_dw.weight\", \"dw4_1.bn_dw.weight\", \"dw4_1.bn_dw.bias\", \"dw4_1.bn_dw.running_mean\", \"dw4_1.bn_dw.running_var\", \"dw4_1.conv_sep.weight\", \"dw4_1.bn_sep.weight\", \"dw4_1.bn_sep.bias\", \"dw4_1.bn_sep.running_mean\", \"dw4_1.bn_sep.running_var\", \"dw4_2.conv_dw.weight\", \"dw4_2.bn_dw.weight\", \"dw4_2.bn_dw.bias\", \"dw4_2.bn_dw.running_mean\", \"dw4_2.bn_dw.running_var\", \"dw4_2.conv_sep.weight\", \"dw4_2.bn_sep.weight\", \"dw4_2.bn_sep.bias\", \"dw4_2.bn_sep.running_mean\", \"dw4_2.bn_sep.running_var\", \"dw5_1.conv_dw.weight\", \"dw5_1.bn_dw.weight\", \"dw5_1.bn_dw.bias\", \"dw5_1.bn_dw.running_mean\", \"dw5_1.bn_dw.running_var\", \"dw5_1.conv_sep.weight\", \"dw5_1.bn_sep.weight\", \"dw5_1.bn_sep.bias\", \"dw5_1.bn_sep.running_mean\", \"dw5_1.bn_sep.running_var\", \"dw5_2.conv_dw.weight\", \"dw5_2.bn_dw.weight\", \"dw5_2.bn_dw.bias\", \"dw5_2.bn_dw.running_mean\", \"dw5_2.bn_dw.running_var\", \"dw5_2.conv_sep.weight\", \"dw5_2.bn_sep.weight\", \"dw5_2.bn_sep.bias\", \"dw5_2.bn_sep.running_mean\", \"dw5_2.bn_sep.running_var\", \"dw5_3.conv_dw.weight\", \"dw5_3.bn_dw.weight\", \"dw5_3.bn_dw.bias\", \"dw5_3.bn_dw.running_mean\", \"dw5_3.bn_dw.running_var\", \"dw5_3.conv_sep.weight\", \"dw5_3.bn_sep.weight\", \"dw5_3.bn_sep.bias\", \"dw5_3.bn_sep.running_mean\", \"dw5_3.bn_sep.running_var\", \"dw5_4.conv_dw.weight\", \"dw5_4.bn_dw.weight\", \"dw5_4.bn_dw.bias\", \"dw5_4.bn_dw.running_mean\", \"dw5_4.bn_dw.running_var\", \"dw5_4.conv_sep.weight\", \"dw5_4.bn_sep.weight\", \"dw5_4.bn_sep.bias\", \"dw5_4.bn_sep.running_mean\", \"dw5_4.bn_sep.running_var\", \"dw5_5.conv_dw.weight\", \"dw5_5.bn_dw.weight\", \"dw5_5.bn_dw.bias\", \"dw5_5.bn_dw.running_mean\", \"dw5_5.bn_dw.running_var\", \"dw5_5.conv_sep.weight\", \"dw5_5.bn_sep.weight\", \"dw5_5.bn_sep.bias\", \"dw5_5.bn_sep.running_mean\", \"dw5_5.bn_sep.running_var\", \"dw5_6.conv_dw.weight\", \"dw5_6.bn_dw.weight\", \"dw5_6.bn_dw.bias\", \"dw5_6.bn_dw.running_mean\", \"dw5_6.bn_dw.running_var\", \"dw5_6.conv_sep.weight\", \"dw5_6.bn_sep.weight\", \"dw5_6.bn_sep.bias\", \"dw5_6.bn_sep.running_mean\", \"dw5_6.bn_sep.running_var\", \"dw6.conv_dw.weight\", \"dw6.bn_dw.weight\", \"dw6.bn_dw.bias\", \"dw6.bn_dw.running_mean\", \"dw6.bn_dw.running_var\", \"dw6.conv_sep.weight\", \"dw6.bn_sep.weight\", \"dw6.bn_sep.bias\", \"dw6.bn_sep.running_mean\", \"dw6.bn_sep.running_var\", \"fc.weight\", \"fc.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"features.0.0.weight\", \"features.0.1.weight\", \"features.0.1.bias\", \"features.0.1.running_mean\", \"features.0.1.running_var\", \"features.0.1.num_batches_tracked\", \"features.1.conv.0.weight\", \"features.1.conv.1.weight\", \"features.1.conv.1.bias\", \"features.1.conv.1.running_mean\", \"features.1.conv.1.running_var\", \"features.1.conv.1.num_batches_tracked\", \"features.1.conv.3.weight\", \"features.1.conv.4.weight\", \"features.1.conv.4.bias\", \"features.1.conv.4.running_mean\", \"features.1.conv.4.running_var\", \"features.1.conv.4.num_batches_tracked\", \"features.2.conv.0.weight\", \"features.2.conv.1.weight\", \"features.2.conv.1.bias\", \"features.2.conv.1.running_mean\", \"features.2.conv.1.running_var\", \"features.2.conv.1.num_batches_tracked\", \"features.2.conv.3.weight\", \"features.2.conv.4.weight\", \"features.2.conv.4.bias\", \"features.2.conv.4.running_mean\", \"features.2.conv.4.running_var\", \"features.2.conv.4.num_batches_tracked\", \"features.2.conv.6.weight\", \"features.2.conv.7.weight\", \"features.2.conv.7.bias\", \"features.2.conv.7.running_mean\", \"features.2.conv.7.running_var\", \"features.2.conv.7.num_batches_tracked\", \"features.3.conv.0.weight\", \"features.3.conv.1.weight\", \"features.3.conv.1.bias\", \"features.3.conv.1.running_mean\", \"features.3.conv.1.running_var\", \"features.3.conv.1.num_batches_tracked\", \"features.3.conv.3.weight\", \"features.3.conv.4.weight\", \"features.3.conv.4.bias\", \"features.3.conv.4.running_mean\", \"features.3.conv.4.running_var\", \"features.3.conv.4.num_batches_tracked\", \"features.3.conv.6.weight\", \"features.3.conv.7.weight\", \"features.3.conv.7.bias\", \"features.3.conv.7.running_mean\", \"features.3.conv.7.running_var\", \"features.3.conv.7.num_batches_tracked\", \"features.4.conv.0.weight\", \"features.4.conv.1.weight\", \"features.4.conv.1.bias\", \"features.4.conv.1.running_mean\", \"features.4.conv.1.running_var\", \"features.4.conv.1.num_batches_tracked\", \"features.4.conv.3.weight\", \"features.4.conv.4.weight\", \"features.4.conv.4.bias\", \"features.4.conv.4.running_mean\", \"features.4.conv.4.running_var\", \"features.4.conv.4.num_batches_tracked\", \"features.4.conv.6.weight\", \"features.4.conv.7.weight\", \"features.4.conv.7.bias\", \"features.4.conv.7.running_mean\", \"features.4.conv.7.running_var\", \"features.4.conv.7.num_batches_tracked\", \"features.5.conv.0.weight\", \"features.5.conv.1.weight\", \"features.5.conv.1.bias\", \"features.5.conv.1.running_mean\", \"features.5.conv.1.running_var\", \"features.5.conv.1.num_batches_tracked\", \"features.5.conv.3.weight\", \"features.5.conv.4.weight\", \"features.5.conv.4.bias\", \"features.5.conv.4.running_mean\", \"features.5.conv.4.running_var\", \"features.5.conv.4.num_batches_tracked\", \"features.5.conv.6.weight\", \"features.5.conv.7.weight\", \"features.5.conv.7.bias\", \"features.5.conv.7.running_mean\", \"features.5.conv.7.running_var\", \"features.5.conv.7.num_batches_tracked\", \"features.6.conv.0.weight\", \"features.6.conv.1.weight\", \"features.6.conv.1.bias\", \"features.6.conv.1.running_mean\", \"features.6.conv.1.running_var\", \"features.6.conv.1.num_batches_tracked\", \"features.6.conv.3.weight\", \"features.6.conv.4.weight\", \"features.6.conv.4.bias\", \"features.6.conv.4.running_mean\", \"features.6.conv.4.running_var\", \"features.6.conv.4.num_batches_tracked\", \"features.6.conv.6.weight\", \"features.6.conv.7.weight\", \"features.6.conv.7.bias\", \"features.6.conv.7.running_mean\", \"features.6.conv.7.running_var\", \"features.6.conv.7.num_batches_tracked\", \"features.7.conv.0.weight\", \"features.7.conv.1.weight\", \"features.7.conv.1.bias\", \"features.7.conv.1.running_mean\", \"features.7.conv.1.running_var\", \"features.7.conv.1.num_batches_tracked\", \"features.7.conv.3.weight\", \"features.7.conv.4.weight\", \"features.7.conv.4.bias\", \"features.7.conv.4.running_mean\", \"features.7.conv.4.running_var\", \"features.7.conv.4.num_batches_tracked\", \"features.7.conv.6.weight\", \"features.7.conv.7.weight\", \"features.7.conv.7.bias\", \"features.7.conv.7.running_mean\", \"features.7.conv.7.running_var\", \"features.7.conv.7.num_batches_tracked\", \"features.8.conv.0.weight\", \"features.8.conv.1.weight\", \"features.8.conv.1.bias\", \"features.8.conv.1.running_mean\", \"features.8.conv.1.running_var\", \"features.8.conv.1.num_batches_tracked\", \"features.8.conv.3.weight\", \"features.8.conv.4.weight\", \"features.8.conv.4.bias\", \"features.8.conv.4.running_mean\", \"features.8.conv.4.running_var\", \"features.8.conv.4.num_batches_tracked\", \"features.8.conv.6.weight\", \"features.8.conv.7.weight\", \"features.8.conv.7.bias\", \"features.8.conv.7.running_mean\", \"features.8.conv.7.running_var\", \"features.8.conv.7.num_batches_tracked\", \"features.9.conv.0.weight\", \"features.9.conv.1.weight\", \"features.9.conv.1.bias\", \"features.9.conv.1.running_mean\", \"features.9.conv.1.running_var\", \"features.9.conv.1.num_batches_tracked\", \"features.9.conv.3.weight\", \"features.9.conv.4.weight\", \"features.9.conv.4.bias\", \"features.9.conv.4.running_mean\", \"features.9.conv.4.running_var\", \"features.9.conv.4.num_batches_tracked\", \"features.9.conv.6.weight\", \"features.9.conv.7.weight\", \"features.9.conv.7.bias\", \"features.9.conv.7.running_mean\", \"features.9.conv.7.running_var\", \"features.9.conv.7.num_batches_tracked\", \"features.10.conv.0.weight\", \"features.10.conv.1.weight\", \"features.10.conv.1.bias\", \"features.10.conv.1.running_mean\", \"features.10.conv.1.running_var\", \"features.10.conv.1.num_batches_tracked\", \"features.10.conv.3.weight\", \"features.10.conv.4.weight\", \"features.10.conv.4.bias\", \"features.10.conv.4.running_mean\", \"features.10.conv.4.running_var\", \"features.10.conv.4.num_batches_tracked\", \"features.10.conv.6.weight\", \"features.10.conv.7.weight\", \"features.10.conv.7.bias\", \"features.10.conv.7.running_mean\", \"features.10.conv.7.running_var\", \"features.10.conv.7.num_batches_tracked\", \"features.11.conv.0.weight\", \"features.11.conv.1.weight\", \"features.11.conv.1.bias\", \"features.11.conv.1.running_mean\", \"features.11.conv.1.running_var\", \"features.11.conv.1.num_batches_tracked\", \"features.11.conv.3.weight\", \"features.11.conv.4.weight\", \"features.11.conv.4.bias\", \"features.11.conv.4.running_mean\", \"features.11.conv.4.running_var\", \"features.11.conv.4.num_batches_tracked\", \"features.11.conv.6.weight\", \"features.11.conv.7.weight\", \"features.11.conv.7.bias\", \"features.11.conv.7.running_mean\", \"features.11.conv.7.running_var\", \"features.11.conv.7.num_batches_tracked\", \"features.12.conv.0.weight\", \"features.12.conv.1.weight\", \"features.12.conv.1.bias\", \"features.12.conv.1.running_mean\", \"features.12.conv.1.running_var\", \"features.12.conv.1.num_batches_tracked\", \"features.12.conv.3.weight\", \"features.12.conv.4.weight\", \"features.12.conv.4.bias\", \"features.12.conv.4.running_mean\", \"features.12.conv.4.running_var\", \"features.12.conv.4.num_batches_tracked\", \"features.12.conv.6.weight\", \"features.12.conv.7.weight\", \"features.12.conv.7.bias\", \"features.12.conv.7.running_mean\", \"features.12.conv.7.running_var\", \"features.12.conv.7.num_batches_tracked\", \"features.13.conv.0.weight\", \"features.13.conv.1.weight\", \"features.13.conv.1.bias\", \"features.13.conv.1.running_mean\", \"features.13.conv.1.running_var\", \"features.13.conv.1.num_batches_tracked\", \"features.13.conv.3.weight\", \"features.13.conv.4.weight\", \"features.13.conv.4.bias\", \"features.13.conv.4.running_mean\", \"features.13.conv.4.running_var\", \"features.13.conv.4.num_batches_tracked\", \"features.13.conv.6.weight\", \"features.13.conv.7.weight\", \"features.13.conv.7.bias\", \"features.13.conv.7.running_mean\", \"features.13.conv.7.running_var\", \"features.13.conv.7.num_batches_tracked\", \"features.14.conv.0.weight\", \"features.14.conv.1.weight\", \"features.14.conv.1.bias\", \"features.14.conv.1.running_mean\", \"features.14.conv.1.running_var\", \"features.14.conv.1.num_batches_tracked\", \"features.14.conv.3.weight\", \"features.14.conv.4.weight\", \"features.14.conv.4.bias\", \"features.14.conv.4.running_mean\", \"features.14.conv.4.running_var\", \"features.14.conv.4.num_batches_tracked\", \"features.14.conv.6.weight\", \"features.14.conv.7.weight\", \"features.14.conv.7.bias\", \"features.14.conv.7.running_mean\", \"features.14.conv.7.running_var\", \"features.14.conv.7.num_batches_tracked\", \"features.15.conv.0.weight\", \"features.15.conv.1.weight\", \"features.15.conv.1.bias\", \"features.15.conv.1.running_mean\", \"features.15.conv.1.running_var\", \"features.15.conv.1.num_batches_tracked\", \"features.15.conv.3.weight\", \"features.15.conv.4.weight\", \"features.15.conv.4.bias\", \"features.15.conv.4.running_mean\", \"features.15.conv.4.running_var\", \"features.15.conv.4.num_batches_tracked\", \"features.15.conv.6.weight\", \"features.15.conv.7.weight\", \"features.15.conv.7.bias\", \"features.15.conv.7.running_mean\", \"features.15.conv.7.running_var\", \"features.15.conv.7.num_batches_tracked\", \"features.16.conv.0.weight\", \"features.16.conv.1.weight\", \"features.16.conv.1.bias\", \"features.16.conv.1.running_mean\", \"features.16.conv.1.running_var\", \"features.16.conv.1.num_batches_tracked\", \"features.16.conv.3.weight\", \"features.16.conv.4.weight\", \"features.16.conv.4.bias\", \"features.16.conv.4.running_mean\", \"features.16.conv.4.running_var\", \"features.16.conv.4.num_batches_tracked\", \"features.16.conv.6.weight\", \"features.16.conv.7.weight\", \"features.16.conv.7.bias\", \"features.16.conv.7.running_mean\", \"features.16.conv.7.running_var\", \"features.16.conv.7.num_batches_tracked\", \"features.17.conv.0.weight\", \"features.17.conv.1.weight\", \"features.17.conv.1.bias\", \"features.17.conv.1.running_mean\", \"features.17.conv.1.running_var\", \"features.17.conv.1.num_batches_tracked\", \"features.17.conv.3.weight\", \"features.17.conv.4.weight\", \"features.17.conv.4.bias\", \"features.17.conv.4.running_mean\", \"features.17.conv.4.running_var\", \"features.17.conv.4.num_batches_tracked\", \"features.17.conv.6.weight\", \"features.17.conv.7.weight\", \"features.17.conv.7.bias\", \"features.17.conv.7.running_mean\", \"features.17.conv.7.running_var\", \"features.17.conv.7.num_batches_tracked\", \"conv.0.weight\", \"conv.1.weight\", \"conv.1.bias\", \"conv.1.running_mean\", \"conv.1.running_var\", \"conv.1.num_batches_tracked\", \"classifier.weight\", \"classifier.bias\". \n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 3\n",
      "h: 32\n",
      "w: 32\n",
      "tensor([[ 1.1869, -2.7726,  0.3580, -0.4164,  0.6805,  1.1812, -0.3896]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "#验证模型正确性\n",
    "\"\"\"\n",
    "visualize results for test image\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from mobilenet_v1 import mobilenet, MobileNet, mobilenet_05\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "raw_img = io.imread('images/2.jpg')\n",
    "gray = rgb2gray(raw_img)\n",
    "gray = resize(gray, (48,48), mode='symmetric').astype(np.uint8)\n",
    "\n",
    "img = gray[:, :, np.newaxis]\n",
    "\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "net = mobilenetv2(num_classes=7,input_size=32)\n",
    "checkpoint = torch.load(os.path.join('FER2013_mobilenetv2', 'PrivateTest_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "print(\"c:\",c);\n",
    "print(\"h:\",h);\n",
    "print(\"w:\",w);\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "outputs = net(inputs)\n",
    "print(outputs)\n",
    "\n",
    "#outputs_avg = outputs[0].view(ncrops, -1).mean(0)  # avg over crops\n",
    "\n",
    "score = F.softmax(outputs[0])\n",
    "_, predicted = torch.max(outputs[0], 0)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (13.5,5.5)\n",
    "axes=plt.subplot(1, 3, 1)\n",
    "plt.imshow(raw_img)\n",
    "plt.xlabel('Input Image', fontsize=16)\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.9, hspace=0.02, wspace=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "ind = 0.1+0.6*np.arange(len(class_names))    # the x locations for the groups\n",
    "width = 0.4       # the width of the bars: can also be len(x) sequence\n",
    "color_list = ['red','orangered','darkorange','limegreen','darkgreen','royalblue','navy']\n",
    "for i in range(len(class_names)):\n",
    "    plt.bar(ind[i], score.data.cpu().numpy()[i], width, color=color_list[i])\n",
    "plt.title(\"Classification results \",fontsize=20)\n",
    "plt.xlabel(\" Expression Category \",fontsize=16)\n",
    "plt.ylabel(\" Classification Score \",fontsize=16)\n",
    "plt.xticks(ind, class_names, rotation=45, fontsize=14)\n",
    "\n",
    "axes=plt.subplot(1, 3, 3)\n",
    "emojis_img = io.imread('images/emojis/%s.png' % str(class_names[int(predicted.cpu().numpy())]))\n",
    "plt.imshow(emojis_img)\n",
    "plt.xlabel('Emoji Expression', fontsize=16)\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "plt.tight_layout()\n",
    "# show emojis\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join('images/results/2.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[int(predicted.cpu().numpy())]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (1.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.1)\r\n",
      "Requirement already satisfied: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.9.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.12.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.17.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (41.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%data : Float(1, 3, 32, 32),\n",
      "      %1 : Float(32, 3, 3, 3),\n",
      "      %2 : Float(32),\n",
      "      %3 : Float(32),\n",
      "      %4 : Float(32),\n",
      "      %5 : Float(32),\n",
      "      %6 : Long(),\n",
      "      %7 : Float(32, 1, 3, 3),\n",
      "      %8 : Float(32),\n",
      "      %9 : Float(32),\n",
      "      %10 : Float(32),\n",
      "      %11 : Float(32),\n",
      "      %12 : Long(),\n",
      "      %13 : Float(16, 32, 1, 1),\n",
      "      %14 : Float(16),\n",
      "      %15 : Float(16),\n",
      "      %16 : Float(16),\n",
      "      %17 : Float(16),\n",
      "      %18 : Long(),\n",
      "      %19 : Float(96, 16, 1, 1),\n",
      "      %20 : Float(96),\n",
      "      %21 : Float(96),\n",
      "      %22 : Float(96),\n",
      "      %23 : Float(96),\n",
      "      %24 : Long(),\n",
      "      %25 : Float(96, 1, 3, 3),\n",
      "      %26 : Float(96),\n",
      "      %27 : Float(96),\n",
      "      %28 : Float(96),\n",
      "      %29 : Float(96),\n",
      "      %30 : Long(),\n",
      "      %31 : Float(24, 96, 1, 1),\n",
      "      %32 : Float(24),\n",
      "      %33 : Float(24),\n",
      "      %34 : Float(24),\n",
      "      %35 : Float(24),\n",
      "      %36 : Long(),\n",
      "      %37 : Float(144, 24, 1, 1),\n",
      "      %38 : Float(144),\n",
      "      %39 : Float(144),\n",
      "      %40 : Float(144),\n",
      "      %41 : Float(144),\n",
      "      %42 : Long(),\n",
      "      %43 : Float(144, 1, 3, 3),\n",
      "      %44 : Float(144),\n",
      "      %45 : Float(144),\n",
      "      %46 : Float(144),\n",
      "      %47 : Float(144),\n",
      "      %48 : Long(),\n",
      "      %49 : Float(24, 144, 1, 1),\n",
      "      %50 : Float(24),\n",
      "      %51 : Float(24),\n",
      "      %52 : Float(24),\n",
      "      %53 : Float(24),\n",
      "      %54 : Long(),\n",
      "      %55 : Float(144, 24, 1, 1),\n",
      "      %56 : Float(144),\n",
      "      %57 : Float(144),\n",
      "      %58 : Float(144),\n",
      "      %59 : Float(144),\n",
      "      %60 : Long(),\n",
      "      %61 : Float(144, 1, 3, 3),\n",
      "      %62 : Float(144),\n",
      "      %63 : Float(144),\n",
      "      %64 : Float(144),\n",
      "      %65 : Float(144),\n",
      "      %66 : Long(),\n",
      "      %67 : Float(32, 144, 1, 1),\n",
      "      %68 : Float(32),\n",
      "      %69 : Float(32),\n",
      "      %70 : Float(32),\n",
      "      %71 : Float(32),\n",
      "      %72 : Long(),\n",
      "      %73 : Float(192, 32, 1, 1),\n",
      "      %74 : Float(192),\n",
      "      %75 : Float(192),\n",
      "      %76 : Float(192),\n",
      "      %77 : Float(192),\n",
      "      %78 : Long(),\n",
      "      %79 : Float(192, 1, 3, 3),\n",
      "      %80 : Float(192),\n",
      "      %81 : Float(192),\n",
      "      %82 : Float(192),\n",
      "      %83 : Float(192),\n",
      "      %84 : Long(),\n",
      "      %85 : Float(32, 192, 1, 1),\n",
      "      %86 : Float(32),\n",
      "      %87 : Float(32),\n",
      "      %88 : Float(32),\n",
      "      %89 : Float(32),\n",
      "      %90 : Long(),\n",
      "      %91 : Float(192, 32, 1, 1),\n",
      "      %92 : Float(192),\n",
      "      %93 : Float(192),\n",
      "      %94 : Float(192),\n",
      "      %95 : Float(192),\n",
      "      %96 : Long(),\n",
      "      %97 : Float(192, 1, 3, 3),\n",
      "      %98 : Float(192),\n",
      "      %99 : Float(192),\n",
      "      %100 : Float(192),\n",
      "      %101 : Float(192),\n",
      "      %102 : Long(),\n",
      "      %103 : Float(32, 192, 1, 1),\n",
      "      %104 : Float(32),\n",
      "      %105 : Float(32),\n",
      "      %106 : Float(32),\n",
      "      %107 : Float(32),\n",
      "      %108 : Long(),\n",
      "      %109 : Float(192, 32, 1, 1),\n",
      "      %110 : Float(192),\n",
      "      %111 : Float(192),\n",
      "      %112 : Float(192),\n",
      "      %113 : Float(192),\n",
      "      %114 : Long(),\n",
      "      %115 : Float(192, 1, 3, 3),\n",
      "      %116 : Float(192),\n",
      "      %117 : Float(192),\n",
      "      %118 : Float(192),\n",
      "      %119 : Float(192),\n",
      "      %120 : Long(),\n",
      "      %121 : Float(64, 192, 1, 1),\n",
      "      %122 : Float(64),\n",
      "      %123 : Float(64),\n",
      "      %124 : Float(64),\n",
      "      %125 : Float(64),\n",
      "      %126 : Long(),\n",
      "      %127 : Float(384, 64, 1, 1),\n",
      "      %128 : Float(384),\n",
      "      %129 : Float(384),\n",
      "      %130 : Float(384),\n",
      "      %131 : Float(384),\n",
      "      %132 : Long(),\n",
      "      %133 : Float(384, 1, 3, 3),\n",
      "      %134 : Float(384),\n",
      "      %135 : Float(384),\n",
      "      %136 : Float(384),\n",
      "      %137 : Float(384),\n",
      "      %138 : Long(),\n",
      "      %139 : Float(64, 384, 1, 1),\n",
      "      %140 : Float(64),\n",
      "      %141 : Float(64),\n",
      "      %142 : Float(64),\n",
      "      %143 : Float(64),\n",
      "      %144 : Long(),\n",
      "      %145 : Float(384, 64, 1, 1),\n",
      "      %146 : Float(384),\n",
      "      %147 : Float(384),\n",
      "      %148 : Float(384),\n",
      "      %149 : Float(384),\n",
      "      %150 : Long(),\n",
      "      %151 : Float(384, 1, 3, 3),\n",
      "      %152 : Float(384),\n",
      "      %153 : Float(384),\n",
      "      %154 : Float(384),\n",
      "      %155 : Float(384),\n",
      "      %156 : Long(),\n",
      "      %157 : Float(64, 384, 1, 1),\n",
      "      %158 : Float(64),\n",
      "      %159 : Float(64),\n",
      "      %160 : Float(64),\n",
      "      %161 : Float(64),\n",
      "      %162 : Long(),\n",
      "      %163 : Float(384, 64, 1, 1),\n",
      "      %164 : Float(384),\n",
      "      %165 : Float(384),\n",
      "      %166 : Float(384),\n",
      "      %167 : Float(384),\n",
      "      %168 : Long(),\n",
      "      %169 : Float(384, 1, 3, 3),\n",
      "      %170 : Float(384),\n",
      "      %171 : Float(384),\n",
      "      %172 : Float(384),\n",
      "      %173 : Float(384),\n",
      "      %174 : Long(),\n",
      "      %175 : Float(64, 384, 1, 1),\n",
      "      %176 : Float(64),\n",
      "      %177 : Float(64),\n",
      "      %178 : Float(64),\n",
      "      %179 : Float(64),\n",
      "      %180 : Long(),\n",
      "      %181 : Float(384, 64, 1, 1),\n",
      "      %182 : Float(384),\n",
      "      %183 : Float(384),\n",
      "      %184 : Float(384),\n",
      "      %185 : Float(384),\n",
      "      %186 : Long(),\n",
      "      %187 : Float(384, 1, 3, 3),\n",
      "      %188 : Float(384),\n",
      "      %189 : Float(384),\n",
      "      %190 : Float(384),\n",
      "      %191 : Float(384),\n",
      "      %192 : Long(),\n",
      "      %193 : Float(96, 384, 1, 1),\n",
      "      %194 : Float(96),\n",
      "      %195 : Float(96),\n",
      "      %196 : Float(96),\n",
      "      %197 : Float(96),\n",
      "      %198 : Long(),\n",
      "      %199 : Float(576, 96, 1, 1),\n",
      "      %200 : Float(576),\n",
      "      %201 : Float(576),\n",
      "      %202 : Float(576),\n",
      "      %203 : Float(576),\n",
      "      %204 : Long(),\n",
      "      %205 : Float(576, 1, 3, 3),\n",
      "      %206 : Float(576),\n",
      "      %207 : Float(576),\n",
      "      %208 : Float(576),\n",
      "      %209 : Float(576),\n",
      "      %210 : Long(),\n",
      "      %211 : Float(96, 576, 1, 1),\n",
      "      %212 : Float(96),\n",
      "      %213 : Float(96),\n",
      "      %214 : Float(96),\n",
      "      %215 : Float(96),\n",
      "      %216 : Long(),\n",
      "      %217 : Float(576, 96, 1, 1),\n",
      "      %218 : Float(576),\n",
      "      %219 : Float(576),\n",
      "      %220 : Float(576),\n",
      "      %221 : Float(576),\n",
      "      %222 : Long(),\n",
      "      %223 : Float(576, 1, 3, 3),\n",
      "      %224 : Float(576),\n",
      "      %225 : Float(576),\n",
      "      %226 : Float(576),\n",
      "      %227 : Float(576),\n",
      "      %228 : Long(),\n",
      "      %229 : Float(96, 576, 1, 1),\n",
      "      %230 : Float(96),\n",
      "      %231 : Float(96),\n",
      "      %232 : Float(96),\n",
      "      %233 : Float(96),\n",
      "      %234 : Long(),\n",
      "      %235 : Float(576, 96, 1, 1),\n",
      "      %236 : Float(576),\n",
      "      %237 : Float(576),\n",
      "      %238 : Float(576),\n",
      "      %239 : Float(576),\n",
      "      %240 : Long(),\n",
      "      %241 : Float(576, 1, 3, 3),\n",
      "      %242 : Float(576),\n",
      "      %243 : Float(576),\n",
      "      %244 : Float(576),\n",
      "      %245 : Float(576),\n",
      "      %246 : Long(),\n",
      "      %247 : Float(160, 576, 1, 1),\n",
      "      %248 : Float(160),\n",
      "      %249 : Float(160),\n",
      "      %250 : Float(160),\n",
      "      %251 : Float(160),\n",
      "      %252 : Long(),\n",
      "      %253 : Float(960, 160, 1, 1),\n",
      "      %254 : Float(960),\n",
      "      %255 : Float(960),\n",
      "      %256 : Float(960),\n",
      "      %257 : Float(960),\n",
      "      %258 : Long(),\n",
      "      %259 : Float(960, 1, 3, 3),\n",
      "      %260 : Float(960),\n",
      "      %261 : Float(960),\n",
      "      %262 : Float(960),\n",
      "      %263 : Float(960),\n",
      "      %264 : Long(),\n",
      "      %265 : Float(160, 960, 1, 1),\n",
      "      %266 : Float(160),\n",
      "      %267 : Float(160),\n",
      "      %268 : Float(160),\n",
      "      %269 : Float(160),\n",
      "      %270 : Long(),\n",
      "      %271 : Float(960, 160, 1, 1),\n",
      "      %272 : Float(960),\n",
      "      %273 : Float(960),\n",
      "      %274 : Float(960),\n",
      "      %275 : Float(960),\n",
      "      %276 : Long(),\n",
      "      %277 : Float(960, 1, 3, 3),\n",
      "      %278 : Float(960),\n",
      "      %279 : Float(960),\n",
      "      %280 : Float(960),\n",
      "      %281 : Float(960),\n",
      "      %282 : Long(),\n",
      "      %283 : Float(160, 960, 1, 1),\n",
      "      %284 : Float(160),\n",
      "      %285 : Float(160),\n",
      "      %286 : Float(160),\n",
      "      %287 : Float(160),\n",
      "      %288 : Long(),\n",
      "      %289 : Float(960, 160, 1, 1),\n",
      "      %290 : Float(960),\n",
      "      %291 : Float(960),\n",
      "      %292 : Float(960),\n",
      "      %293 : Float(960),\n",
      "      %294 : Long(),\n",
      "      %295 : Float(960, 1, 3, 3),\n",
      "      %296 : Float(960),\n",
      "      %297 : Float(960),\n",
      "      %298 : Float(960),\n",
      "      %299 : Float(960),\n",
      "      %300 : Long(),\n",
      "      %301 : Float(320, 960, 1, 1),\n",
      "      %302 : Float(320),\n",
      "      %303 : Float(320),\n",
      "      %304 : Float(320),\n",
      "      %305 : Float(320),\n",
      "      %306 : Long(),\n",
      "      %307 : Float(1280, 320, 1, 1),\n",
      "      %308 : Float(1280),\n",
      "      %309 : Float(1280),\n",
      "      %310 : Float(1280),\n",
      "      %311 : Float(1280),\n",
      "      %312 : Long(),\n",
      "      %313 : Float(7, 1280),\n",
      "      %314 : Float(7)):\n",
      "  %315 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%data, %1), scope: MobileNetV2/Sequential[features]/Sequential[0]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %316 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%315, %2, %3, %4, %5), scope: MobileNetV2/Sequential[features]/Sequential[0]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %317 : Float(1, 32, 16, 16) = onnx::Clip[max=6, min=0](%316), scope: MobileNetV2/Sequential[features]/Sequential[0]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %318 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%317, %7), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %319 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%318, %8, %9, %10, %11), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %320 : Float(1, 32, 16, 16) = onnx::Clip[max=6, min=0](%319), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %321 : Float(1, 16, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%320, %13), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %322 : Float(1, 16, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%321, %14, %15, %16, %17), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %323 : Float(1, 96, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%322, %19), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %324 : Float(1, 96, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%323, %20, %21, %22, %23), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %325 : Float(1, 96, 16, 16) = onnx::Clip[max=6, min=0](%324), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %326 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%325, %25), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %327 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%326, %26, %27, %28, %29), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %328 : Float(1, 96, 8, 8) = onnx::Clip[max=6, min=0](%327), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %329 : Float(1, 24, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%328, %31), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %330 : Float(1, 24, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%329, %32, %33, %34, %35), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %331 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%330, %37), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %332 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%331, %38, %39, %40, %41), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %333 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%332), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %334 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%333, %43), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %335 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%334, %44, %45, %46, %47), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %336 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%335), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %337 : Float(1, 24, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%336, %49), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %338 : Float(1, 24, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%337, %50, %51, %52, %53), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %339 : Float(1, 24, 8, 8) = onnx::Add(%330, %338), scope: MobileNetV2/Sequential[features]/InvertedResidual[3] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %340 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%339, %55), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %341 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%340, %56, %57, %58, %59), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %342 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%341), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %343 : Float(1, 144, 4, 4) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%342, %61), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %344 : Float(1, 144, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%343, %62, %63, %64, %65), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %345 : Float(1, 144, 4, 4) = onnx::Clip[max=6, min=0](%344), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %346 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%345, %67), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %347 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%346, %68, %69, %70, %71), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %348 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%347, %73), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %349 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%348, %74, %75, %76, %77), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %350 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%349), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %351 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%350, %79), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %352 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%351, %80, %81, %82, %83), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %353 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%352), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %354 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%353, %85), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %355 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%354, %86, %87, %88, %89), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %356 : Float(1, 32, 4, 4) = onnx::Add(%347, %355), scope: MobileNetV2/Sequential[features]/InvertedResidual[5] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %357 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%356, %91), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %358 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%357, %92, %93, %94, %95), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %359 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%358), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %360 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%359, %97), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %361 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%360, %98, %99, %100, %101), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %362 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%361), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %363 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%362, %103), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %364 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%363, %104, %105, %106, %107), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %365 : Float(1, 32, 4, 4) = onnx::Add(%356, %364), scope: MobileNetV2/Sequential[features]/InvertedResidual[6] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %366 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %109), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %367 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%366, %110, %111, %112, %113), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %368 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%367), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %369 : Float(1, 192, 2, 2) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%368, %115), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %370 : Float(1, 192, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%369, %116, %117, %118, %119), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %371 : Float(1, 192, 2, 2) = onnx::Clip[max=6, min=0](%370), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %372 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%371, %121), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %373 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%372, %122, %123, %124, %125), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %374 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%373, %127), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %375 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%374, %128, %129, %130, %131), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %376 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%375), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %377 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%376, %133), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %378 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%377, %134, %135, %136, %137), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %379 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%378), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %380 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%379, %139), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %381 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%380, %140, %141, %142, %143), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %382 : Float(1, 64, 2, 2) = onnx::Add(%373, %381), scope: MobileNetV2/Sequential[features]/InvertedResidual[8] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %383 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%382, %145), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %384 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%383, %146, %147, %148, %149), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %385 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%384), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %386 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%385, %151), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %387 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%386, %152, %153, %154, %155), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %388 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%387), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %389 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%388, %157), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %390 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%389, %158, %159, %160, %161), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %391 : Float(1, 64, 2, 2) = onnx::Add(%382, %390), scope: MobileNetV2/Sequential[features]/InvertedResidual[9] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %392 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%391, %163), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %393 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%392, %164, %165, %166, %167), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %394 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%393), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %395 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%394, %169), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %396 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%395, %170, %171, %172, %173), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %397 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%396), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %398 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%397, %175), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %399 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%398, %176, %177, %178, %179), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %400 : Float(1, 64, 2, 2) = onnx::Add(%391, %399), scope: MobileNetV2/Sequential[features]/InvertedResidual[10] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %401 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%400, %181), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %402 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%401, %182, %183, %184, %185), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %403 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%402), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %404 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%403, %187), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %405 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%404, %188, %189, %190, %191), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %406 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%405), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %407 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%406, %193), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %408 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%407, %194, %195, %196, %197), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %409 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%408, %199), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %410 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%409, %200, %201, %202, %203), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %411 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%410), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %412 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%411, %205), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %413 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%412, %206, %207, %208, %209), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %414 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%413), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %415 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%414, %211), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %416 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%415, %212, %213, %214, %215), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %417 : Float(1, 96, 2, 2) = onnx::Add(%408, %416), scope: MobileNetV2/Sequential[features]/InvertedResidual[12] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %418 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%417, %217), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %419 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%418, %218, %219, %220, %221), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %420 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%419), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %421 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%420, %223), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %422 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%421, %224, %225, %226, %227), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %423 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%422), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %424 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%423, %229), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %425 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%424, %230, %231, %232, %233), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %426 : Float(1, 96, 2, 2) = onnx::Add(%417, %425), scope: MobileNetV2/Sequential[features]/InvertedResidual[13] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %427 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%426, %235), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %428 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%427, %236, %237, %238, %239), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %429 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%428), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %430 : Float(1, 576, 1, 1) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%429, %241), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %431 : Float(1, 576, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%430, %242, %243, %244, %245), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %432 : Float(1, 576, 1, 1) = onnx::Clip[max=6, min=0](%431), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %433 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%432, %247), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %434 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%433, %248, %249, %250, %251), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %435 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%434, %253), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %436 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%435, %254, %255, %256, %257), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %437 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%436), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %438 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%437, %259), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %439 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%438, %260, %261, %262, %263), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %440 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%439), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %441 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %265), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %442 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%441, %266, %267, %268, %269), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %443 : Float(1, 160, 1, 1) = onnx::Add(%434, %442), scope: MobileNetV2/Sequential[features]/InvertedResidual[15] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %444 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%443, %271), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %445 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%444, %272, %273, %274, %275), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %446 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%445), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %447 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%446, %277), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %448 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%447, %278, %279, %280, %281), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %449 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%448), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %450 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %283), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %451 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%450, %284, %285, %286, %287), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %452 : Float(1, 160, 1, 1) = onnx::Add(%443, %451), scope: MobileNetV2/Sequential[features]/InvertedResidual[16] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %453 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%452, %289), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %454 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%453, %290, %291, %292, %293), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %455 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%454), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %456 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%455, %295), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %457 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%456, %296, %297, %298, %299), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %458 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%457), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %459 : Float(1, 320, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%458, %301), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %460 : Float(1, 320, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%459, %302, %303, %304, %305), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %461 : Float(1, 1280, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%460, %307), scope: MobileNetV2/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %462 : Float(1, 1280, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%461, %308, %309, %310, %311), scope: MobileNetV2/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %463 : Float(1, 1280, 1, 1) = onnx::Clip[max=6, min=0](%462), scope: MobileNetV2/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %464 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%463), scope: MobileNetV2/AvgPool2d[avgpool]\n",
      "  %465 : Float(1, 1280, 1, 1) = onnx::AveragePool[kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%464), scope: MobileNetV2/AvgPool2d[avgpool] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py:551:0\n",
      "  %466 : Long() = onnx::Constant[value={0}](), scope: MobileNetV2\n",
      "  %467 : Tensor = onnx::Shape(%465), scope: MobileNetV2\n",
      "  %468 : Long() = onnx::Gather[axis=0](%467, %466), scope: MobileNetV2 # /storage/expression_recognition/mobilenetv2.py:132:0\n",
      "  %469 : Long() = onnx::Constant[value={-1}](), scope: MobileNetV2\n",
      "  %470 : Tensor = onnx::Unsqueeze[axes=[0]](%468)\n",
      "  %471 : Tensor = onnx::Unsqueeze[axes=[0]](%469)\n",
      "  %472 : Tensor = onnx::Concat[axis=0](%470, %471)\n",
      "  %473 : Float(1, 1280) = onnx::Reshape(%465, %472), scope: MobileNetV2 # /storage/expression_recognition/mobilenetv2.py:132:0\n",
      "  %outTensor : Float(1, 7) = onnx::Gemm[alpha=1, beta=1, transB=1](%473, %313, %314), scope: MobileNetV2/Linear[classifier] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1369:0\n",
      "  return (%outTensor)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导出onnx模型\n",
    "\"\"\"\n",
    "visualize results for test image\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "raw_img = io.imread('images/1.jpg')\n",
    "gray = rgb2gray(raw_img)\n",
    "gray = resize(gray, (48,48), mode='symmetric').astype(np.uint8)\n",
    "\n",
    "img = gray[:, :, np.newaxis]\n",
    "\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "#导入模型，用训练模式\n",
    "net = mobilenetv2(num_classes=7,input_size=32)\n",
    "checkpoint = torch.load(os.path.join('FER2013_mobilenetv2', 'PublicTest_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.train(False)\n",
    "\n",
    "#模拟input\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "#导出模型\n",
    "torch_out = torch.onnx._export(net,  # model being run\n",
    "                               inputs,  # model input (or a tuple for multiple inputs)\n",
    "                               \"FER2013_mobilenetv2_privateTest.onnx\",  # where to save the model\n",
    "                               verbose=True,\n",
    "                               input_names=['data'], \n",
    "                               output_names=['outTensor'], \n",
    "                               export_params=True, \n",
    "                               training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.8019464  -2.4890704   0.40947443  0.11994011 -0.45177376  1.0437407\n",
      "  -0.58876187]]\n",
      "     0.473\n",
      "     0.006\n",
      "     0.118\n",
      "     0.088\n",
      "     0.050\n",
      "     0.222\n",
      "     0.043\n",
      "tensor(0.4732)\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "# 验证onnx模型\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"FER2013_mobilenetv2_privateTest.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "raw_img = io.imread('images/2.jpg')\n",
    "gray = rgb2gray(raw_img)\n",
    "gray = resize(gray, (48,48), mode='symmetric').astype(np.uint8)\n",
    "img = gray[:, :, np.newaxis]\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "predicted = torch.max(score.data)\n",
    "print(predicted)\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[int(predicted.cpu().numpy())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx-simplifier in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (3.9.1)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.6.0)\n",
      "Requirement already satisfied: onnxruntime>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.0.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.7.0->onnx-simplifier) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.7.0->onnx-simplifier) (41.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (3.7.4.1)\n",
      "Simplifying...\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx-simplifier\n",
    "# onnx模型简单化\n",
    "!python -m onnxsim \"FER2013_mobilenetv2_privateTest.onnx\" \"FER2013_mobilenetv2_privateTest_sim.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n",
      "[[ 1.9776292  -3.1441724  -0.05590643  1.0978684   0.11394835 -0.25046366\n",
      "   0.13199115]]\n",
      "     0.507\n",
      "     0.003\n",
      "     0.066\n",
      "     0.210\n",
      "     0.079\n",
      "     0.055\n",
      "     0.080\n",
      "tensor(0.5070)\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "# 验证 sim onnx模型\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"FER2013_mobilenetv2_privateTest_sim.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "raw_img = io.imread('images/1_gray.jpg')\n",
    "img = Image.fromarray(raw_img)\n",
    "inputs = transform_test(img)\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "print(inputs.numpy().shape)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "predicted = torch.max(score.data)\n",
    "print(predicted)\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[int(predicted.cpu().numpy())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: onnx-coreml in /usr/local/lib/python3.6/dist-packages (1.0)\n",
      "Requirement already satisfied, skipping upgrade: onnx==1.5.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: coremltools==3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.0)\n",
      "Requirement already satisfied, skipping upgrade: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx==1.5.0->onnx-coreml) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from onnx==1.5.0->onnx-coreml) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx==1.5.0->onnx-coreml) (41.0.1)\n",
      "1/100: Converting Node Type Conv\n",
      "2/100: Converting Node Type Clip\n",
      "3/100: Converting Node Type Conv\n",
      "4/100: Converting Node Type Clip\n",
      "5/100: Converting Node Type Conv\n",
      "6/100: Converting Node Type Conv\n",
      "7/100: Converting Node Type Clip\n",
      "8/100: Converting Node Type Conv\n",
      "9/100: Converting Node Type Clip\n",
      "10/100: Converting Node Type Conv\n",
      "11/100: Converting Node Type Conv\n",
      "12/100: Converting Node Type Clip\n",
      "13/100: Converting Node Type Conv\n",
      "14/100: Converting Node Type Clip\n",
      "15/100: Converting Node Type Conv\n",
      "16/100: Converting Node Type Add\n",
      "17/100: Converting Node Type Conv\n",
      "18/100: Converting Node Type Clip\n",
      "19/100: Converting Node Type Conv\n",
      "20/100: Converting Node Type Clip\n",
      "21/100: Converting Node Type Conv\n",
      "22/100: Converting Node Type Conv\n",
      "23/100: Converting Node Type Clip\n",
      "24/100: Converting Node Type Conv\n",
      "25/100: Converting Node Type Clip\n",
      "26/100: Converting Node Type Conv\n",
      "27/100: Converting Node Type Add\n",
      "28/100: Converting Node Type Conv\n",
      "29/100: Converting Node Type Clip\n",
      "30/100: Converting Node Type Conv\n",
      "31/100: Converting Node Type Clip\n",
      "32/100: Converting Node Type Conv\n",
      "33/100: Converting Node Type Add\n",
      "34/100: Converting Node Type Conv\n",
      "35/100: Converting Node Type Clip\n",
      "36/100: Converting Node Type Conv\n",
      "37/100: Converting Node Type Clip\n",
      "38/100: Converting Node Type Conv\n",
      "39/100: Converting Node Type Conv\n",
      "40/100: Converting Node Type Clip\n",
      "41/100: Converting Node Type Conv\n",
      "42/100: Converting Node Type Clip\n",
      "43/100: Converting Node Type Conv\n",
      "44/100: Converting Node Type Add\n",
      "45/100: Converting Node Type Conv\n",
      "46/100: Converting Node Type Clip\n",
      "47/100: Converting Node Type Conv\n",
      "48/100: Converting Node Type Clip\n",
      "49/100: Converting Node Type Conv\n",
      "50/100: Converting Node Type Add\n",
      "51/100: Converting Node Type Conv\n",
      "52/100: Converting Node Type Clip\n",
      "53/100: Converting Node Type Conv\n",
      "54/100: Converting Node Type Clip\n",
      "55/100: Converting Node Type Conv\n",
      "56/100: Converting Node Type Add\n",
      "57/100: Converting Node Type Conv\n",
      "58/100: Converting Node Type Clip\n",
      "59/100: Converting Node Type Conv\n",
      "60/100: Converting Node Type Clip\n",
      "61/100: Converting Node Type Conv\n",
      "62/100: Converting Node Type Conv\n",
      "63/100: Converting Node Type Clip\n",
      "64/100: Converting Node Type Conv\n",
      "65/100: Converting Node Type Clip\n",
      "66/100: Converting Node Type Conv\n",
      "67/100: Converting Node Type Add\n",
      "68/100: Converting Node Type Conv\n",
      "69/100: Converting Node Type Clip\n",
      "70/100: Converting Node Type Conv\n",
      "71/100: Converting Node Type Clip\n",
      "72/100: Converting Node Type Conv\n",
      "73/100: Converting Node Type Add\n",
      "74/100: Converting Node Type Conv\n",
      "75/100: Converting Node Type Clip\n",
      "76/100: Converting Node Type Conv\n",
      "77/100: Converting Node Type Clip\n",
      "78/100: Converting Node Type Conv\n",
      "79/100: Converting Node Type Conv\n",
      "80/100: Converting Node Type Clip\n",
      "81/100: Converting Node Type Conv\n",
      "82/100: Converting Node Type Clip\n",
      "83/100: Converting Node Type Conv\n",
      "84/100: Converting Node Type Add\n",
      "85/100: Converting Node Type Conv\n",
      "86/100: Converting Node Type Clip\n",
      "87/100: Converting Node Type Conv\n",
      "88/100: Converting Node Type Clip\n",
      "89/100: Converting Node Type Conv\n",
      "90/100: Converting Node Type Add\n",
      "91/100: Converting Node Type Conv\n",
      "92/100: Converting Node Type Clip\n",
      "93/100: Converting Node Type Conv\n",
      "94/100: Converting Node Type Clip\n",
      "95/100: Converting Node Type Conv\n",
      "96/100: Converting Node Type Conv\n",
      "97/100: Converting Node Type Clip\n",
      "98/100: Converting Node Type AveragePool\n",
      "99/100: Converting Node Type Reshape\n",
      "100/100: Converting Node Type Gemm\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U onnx-coreml\n",
    "#转成corml模型\n",
    "import onnx;\n",
    "from onnx_coreml import convert\n",
    "\n",
    "onnx_model = onnx.load(\"FER2013_mobilenetv2_privateTest_sim.onnx\")\n",
    "cml_model= convert(onnx_model,image_input_names='data',target_ios='13')\n",
    "cml_model.save(\"FER2013_mobilenetv2_privateTest_sim_13.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
