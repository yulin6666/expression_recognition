{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/expression_recognition\n"
     ]
    }
   ],
   "source": [
    "cd /storage/expression_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"mainpro_FER_moblienetv2-tencopy.py\", line 115, in <module>\n",
      "    net = net.cuda()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 311, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 208, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 208, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 208, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 230, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 311, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python mainpro_FER_moblienetv2-tencopy.py --model mobilenetv2 --bs 128 --lr 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [=============================>] | Loss: 1.879 | Acc: 22.174% (6366/28709)     225/225 \n",
      "mainpro_FER_moblienetv2.py:232: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
      " [============================>.] | Loss: 1.951 | Acc: 24.000% (877/3589)       29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 24.000\n",
      "mainpro_FER_moblienetv2.py:272: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
      " [============================>.] | Loss: 1.955 | Acc: 23.000% (838/3589)       29/29 \n",
      "\n",
      "Epoch: 1\n",
      " [=============================>] | Loss: 1.838 | Acc: 25.264% (7253/28709)     225/225 \n",
      " [============================>.] | Loss: 1.809 | Acc: 29.000% (1060/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 29.000\n",
      " [============================>.] | Loss: 1.810 | Acc: 27.000% (1000/3589)      29/29 \n",
      "\n",
      "Epoch: 2\n",
      " [=============================>] | Loss: 1.752 | Acc: 30.214% (8674/28709)     225/225 \n",
      " [============================>.] | Loss: 1.713 | Acc: 32.000% (1172/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 32.000\n",
      " [============================>.] | Loss: 1.713 | Acc: 30.000% (1110/3589)      29/29 \n",
      "\n",
      "Epoch: 3\n",
      " [=============================>] | Loss: 1.685 | Acc: 33.683% (9670/28709)     225/225 \n",
      " [============================>.] | Loss: 1.653 | Acc: 35.000% (1285/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 35.000\n",
      " [============================>.] | Loss: 1.657 | Acc: 35.000% (1257/3589)      29/29 \n",
      "\n",
      "Epoch: 4\n",
      " [=============================>] | Loss: 1.612 | Acc: 36.497% (10478/28709)    225/225 \n",
      " [============================>.] | Loss: 1.579 | Acc: 37.000% (1336/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 37.000\n",
      " [============================>.] | Loss: 1.590 | Acc: 37.000% (1329/3589)      29/29 \n",
      "\n",
      "Epoch: 5\n",
      " [=============================>] | Loss: 1.556 | Acc: 39.263% (11272/28709)    225/225 \n",
      " [============================>.] | Loss: 1.533 | Acc: 39.000% (1431/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 39.000\n",
      " [============================>.] | Loss: 1.556 | Acc: 39.000% (1424/3589)      29/29 \n",
      "\n",
      "Epoch: 6\n",
      " [=============================>] | Loss: 1.511 | Acc: 41.555% (11930/28709)    225/225 \n",
      " [============================>.] | Loss: 1.480 | Acc: 42.000% (1534/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 42.000\n",
      " [============================>.] | Loss: 1.498 | Acc: 41.000% (1501/3589)      29/29 \n",
      "\n",
      "Epoch: 7\n",
      " [=============================>] | Loss: 1.486 | Acc: 42.502% (12202/28709)    225/225 \n",
      " [============================>.] | Loss: 1.489 | Acc: 41.000% (1488/3589)      29/29 \n",
      " [============================>.] | Loss: 1.510 | Acc: 41.000% (1488/3589)      29/29 \n",
      "\n",
      "Epoch: 8\n",
      " [=============================>] | Loss: 1.452 | Acc: 44.143% (12673/28709)    225/225 \n",
      " [============================>.] | Loss: 1.480 | Acc: 42.000% (1527/3589)      29/29 \n",
      " [============================>.] | Loss: 1.496 | Acc: 42.000% (1514/3589)      29/29 \n",
      "\n",
      "Epoch: 9\n",
      " [=============================>] | Loss: 1.429 | Acc: 45.341% (13017/28709)    225/225 \n",
      " [============================>.] | Loss: 1.440 | Acc: 44.000% (1580/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 44.000\n",
      " [============================>.] | Loss: 1.457 | Acc: 43.000% (1573/3589)      29/29 \n",
      "\n",
      "Epoch: 10\n",
      " [=============================>] | Loss: 1.399 | Acc: 46.167% (13254/28709)    225/225 \n",
      " [============================>.] | Loss: 1.443 | Acc: 44.000% (1598/3589)      29/29 \n",
      " [============================>.] | Loss: 1.460 | Acc: 42.000% (1543/3589)      29/29 \n",
      "\n",
      "Epoch: 11\n",
      " [=============================>] | Loss: 1.383 | Acc: 46.957% (13481/28709)    225/225 \n",
      " [============================>.] | Loss: 1.435 | Acc: 44.000% (1609/3589)      29/29 \n",
      " [============================>.] | Loss: 1.439 | Acc: 44.000% (1593/3589)      29/29 \n",
      "\n",
      "Epoch: 12\n",
      " [=============================>] | Loss: 1.368 | Acc: 47.469% (13628/28709)    225/225 \n",
      " [============================>.] | Loss: 1.414 | Acc: 46.000% (1670/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 46.000\n",
      " [============================>.] | Loss: 1.433 | Acc: 44.000% (1609/3589)      29/29 \n",
      "\n",
      "Epoch: 13\n",
      " [=============================>] | Loss: 1.342 | Acc: 48.605% (13954/28709)    225/225 \n",
      " [============================>.] | Loss: 1.416 | Acc: 45.000% (1634/3589)      29/29 \n",
      " [============================>.] | Loss: 1.409 | Acc: 45.000% (1625/3589)      29/29 \n",
      "\n",
      "Epoch: 14\n",
      " [=============================>] | Loss: 1.316 | Acc: 49.511% (14214/28709)    225/225 \n",
      " [============================>.] | Loss: 1.413 | Acc: 44.000% (1601/3589)      29/29 \n",
      " [============================>.] | Loss: 1.444 | Acc: 43.000% (1570/3589)      29/29 \n",
      "\n",
      "Epoch: 15\n",
      " [=============================>] | Loss: 1.305 | Acc: 50.071% (14375/28709)    225/225 \n",
      " [============================>.] | Loss: 1.402 | Acc: 45.000% (1625/3589)      29/29 \n",
      " [============================>.] | Loss: 1.375 | Acc: 46.000% (1686/3589)      29/29 \n",
      "\n",
      "Epoch: 16\n",
      " [=============================>] | Loss: 1.285 | Acc: 50.601% (14527/28709)    225/225 \n",
      " [============================>.] | Loss: 1.375 | Acc: 46.000% (1675/3589)      29/29 \n",
      " [============================>.] | Loss: 1.384 | Acc: 46.000% (1670/3589)      29/29 \n",
      "\n",
      "Epoch: 17\n",
      " [=============================>] | Loss: 1.273 | Acc: 51.155% (14686/28709)    225/225 \n",
      " [============================>.] | Loss: 1.391 | Acc: 46.000% (1679/3589)      29/29 \n",
      " [============================>.] | Loss: 1.374 | Acc: 47.000% (1706/3589)      29/29 \n",
      "\n",
      "Epoch: 18\n",
      " [=============================>] | Loss: 1.257 | Acc: 51.949% (14914/28709)    225/225 \n",
      " [============================>.] | Loss: 1.374 | Acc: 46.000% (1670/3589)      29/29 \n",
      " [============================>.] | Loss: 1.365 | Acc: 46.000% (1677/3589)      29/29 \n",
      "\n",
      "Epoch: 19\n",
      " [=============================>] | Loss: 1.242 | Acc: 52.649% (15115/28709)    225/225 \n",
      " [============================>.] | Loss: 1.386 | Acc: 46.000% (1658/3589)      29/29 \n",
      " [============================>.] | Loss: 1.363 | Acc: 47.000% (1703/3589)      29/29 \n",
      "\n",
      "Epoch: 20\n",
      " [=============================>] | Loss: 1.225 | Acc: 53.506% (15361/28709)    225/225 \n",
      " [============================>.] | Loss: 1.346 | Acc: 48.000% (1741/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 48.000\n",
      " [============================>.] | Loss: 1.353 | Acc: 48.000% (1755/3589)      29/29 \n",
      "\n",
      "Epoch: 21\n",
      " [=============================>] | Loss: 1.219 | Acc: 53.401% (15331/28709)    225/225 \n",
      " [============================>.] | Loss: 1.365 | Acc: 47.000% (1707/3589)      29/29 \n",
      " [============================>.] | Loss: 1.354 | Acc: 48.000% (1733/3589)      29/29 \n",
      "\n",
      "Epoch: 22\n",
      " [=============================>] | Loss: 1.199 | Acc: 54.492% (15644/28709)    225/225 \n",
      " [============================>.] | Loss: 1.365 | Acc: 47.000% (1688/3589)      29/29 \n",
      " [============================>.] | Loss: 1.361 | Acc: 47.000% (1710/3589)      29/29 \n",
      "\n",
      "Epoch: 23\n",
      " [=============================>] | Loss: 1.198 | Acc: 54.478% (15640/28709)    225/225 \n",
      " [============================>.] | Loss: 1.339 | Acc: 47.000% (1700/3589)      29/29 \n",
      " [============================>.] | Loss: 1.363 | Acc: 46.000% (1659/3589)      29/29 \n",
      "\n",
      "Epoch: 24\n",
      " [=============================>] | Loss: 1.173 | Acc: 55.697% (15990/28709)    225/225 \n",
      " [============================>.] | Loss: 1.350 | Acc: 48.000% (1752/3589)      29/29 \n",
      " [============================>.] | Loss: 1.334 | Acc: 49.000% (1772/3589)      29/29 \n",
      "\n",
      "Epoch: 25\n",
      " [=============================>] | Loss: 1.169 | Acc: 55.725% (15998/28709)    225/225 \n",
      " [============================>.] | Loss: 1.376 | Acc: 46.000% (1660/3589)      29/29 \n",
      " [============================>.] | Loss: 1.379 | Acc: 46.000% (1655/3589)      29/29 \n",
      "\n",
      "Epoch: 26\n",
      " [=============================>] | Loss: 1.151 | Acc: 56.494% (16219/28709)    225/225 \n",
      " [============================>.] | Loss: 1.369 | Acc: 47.000% (1691/3589)      29/29 \n",
      " [============================>.] | Loss: 1.371 | Acc: 47.000% (1695/3589)      29/29 \n",
      "\n",
      "Epoch: 27\n",
      " [=============================>] | Loss: 1.138 | Acc: 56.867% (16326/28709)    225/225 \n",
      " [============================>.] | Loss: 1.351 | Acc: 48.000% (1738/3589)      29/29 \n",
      " [============================>.] | Loss: 1.339 | Acc: 48.000% (1749/3589)      29/29 \n",
      "\n",
      "Epoch: 28\n",
      " [=============================>] | Loss: 1.130 | Acc: 57.411% (16482/28709)    225/225 \n",
      " [============================>.] | Loss: 1.369 | Acc: 48.000% (1733/3589)      29/29 \n",
      " [============================>.] | Loss: 1.330 | Acc: 48.000% (1742/3589)      29/29 \n",
      "\n",
      "Epoch: 29\n",
      " [=============================>] | Loss: 1.121 | Acc: 57.505% (16509/28709)    225/225 \n",
      " [============================>.] | Loss: 1.376 | Acc: 47.000% (1706/3589)      29/29 \n",
      " [============================>.] | Loss: 1.352 | Acc: 48.000% (1724/3589)      29/29 \n",
      "\n",
      "Epoch: 30\n",
      " [=============================>] | Loss: 1.107 | Acc: 58.017% (16656/28709)    225/225 \n",
      " [============================>.] | Loss: 1.352 | Acc: 49.000% (1759/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 49.000\n",
      " [============================>.] | Loss: 1.318 | Acc: 50.000% (1807/3589)      29/29 \n",
      "\n",
      "Epoch: 31\n",
      " [=============================>] | Loss: 1.098 | Acc: 58.856% (16897/28709)    225/225 \n",
      " [============================>.] | Loss: 1.357 | Acc: 47.000% (1703/3589)      29/29 \n",
      " [============================>.] | Loss: 1.341 | Acc: 49.000% (1768/3589)      29/29 \n",
      "\n",
      "Epoch: 32\n",
      " [=============================>] | Loss: 1.091 | Acc: 58.908% (16912/28709)    225/225 \n",
      " [============================>.] | Loss: 1.351 | Acc: 48.000% (1753/3589)      29/29 \n",
      " [============================>.] | Loss: 1.322 | Acc: 50.000% (1812/3589)      29/29 \n",
      "\n",
      "Epoch: 33\n",
      " [=============================>] | Loss: 1.079 | Acc: 59.636% (17121/28709)    225/225 \n",
      " [============================>.] | Loss: 1.345 | Acc: 49.000% (1777/3589)      29/29 \n",
      " [============================>.] | Loss: 1.333 | Acc: 50.000% (1818/3589)      29/29 \n",
      "\n",
      "Epoch: 34\n",
      " [=============================>] | Loss: 1.060 | Acc: 60.190% (17280/28709)    225/225 \n",
      " [============================>.] | Loss: 1.386 | Acc: 48.000% (1750/3589)      29/29 \n",
      " [============================>.] | Loss: 1.366 | Acc: 49.000% (1783/3589)      29/29 \n",
      "\n",
      "Epoch: 35\n",
      " [=============================>] | Loss: 1.049 | Acc: 60.598% (17397/28709)    225/225 \n",
      " [============================>.] | Loss: 1.360 | Acc: 49.000% (1760/3589)      29/29 \n",
      " [============================>.] | Loss: 1.355 | Acc: 49.000% (1773/3589)      29/29 \n",
      "\n",
      "Epoch: 36\n",
      " [=============================>] | Loss: 1.045 | Acc: 60.692% (17424/28709)    225/225 \n",
      " [============================>.] | Loss: 1.341 | Acc: 49.000% (1793/3589)      29/29 \n",
      " [============================>.] | Loss: 1.319 | Acc: 51.000% (1835/3589)      29/29 \n",
      "\n",
      "Epoch: 37\n",
      " [=============================>] | Loss: 1.028 | Acc: 61.096% (17540/28709)    225/225 \n",
      " [============================>.] | Loss: 1.355 | Acc: 48.000% (1745/3589)      29/29 \n",
      " [============================>.] | Loss: 1.339 | Acc: 50.000% (1816/3589)      29/29 \n",
      "\n",
      "Epoch: 38\n",
      " [=============================>] | Loss: 1.021 | Acc: 61.792% (17740/28709)    225/225 \n",
      " [============================>.] | Loss: 1.375 | Acc: 48.000% (1754/3589)      29/29 \n",
      " [============================>.] | Loss: 1.341 | Acc: 49.000% (1782/3589)      29/29 \n",
      "\n",
      "Epoch: 39\n",
      " [=============================>] | Loss: 1.012 | Acc: 62.179% (17851/28709)    225/225 \n",
      " [============================>.] | Loss: 1.392 | Acc: 49.000% (1765/3589)      29/29 \n",
      " [============================>.] | Loss: 1.357 | Acc: 49.000% (1787/3589)      29/29 \n",
      "\n",
      "Epoch: 40\n",
      " [=============================>] | Loss: 1.010 | Acc: 62.527% (17951/28709)    225/225 \n",
      " [============================>.] | Loss: 1.382 | Acc: 49.000% (1762/3589)      29/29 \n",
      " [============================>.] | Loss: 1.361 | Acc: 49.000% (1782/3589)      29/29 \n",
      "\n",
      "Epoch: 41\n",
      " [=============================>] | Loss: 0.992 | Acc: 63.137% (18126/28709)    225/225 \n",
      " [============================>.] | Loss: 1.367 | Acc: 50.000% (1814/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 50.000\n",
      " [============================>.] | Loss: 1.351 | Acc: 51.000% (1855/3589)      29/29 \n",
      "\n",
      "Epoch: 42\n",
      " [=============================>] | Loss: 0.998 | Acc: 62.604% (17973/28709)    225/225 \n",
      " [============================>.] | Loss: 1.359 | Acc: 49.000% (1762/3589)      29/29 \n",
      " [============================>.] | Loss: 1.324 | Acc: 51.000% (1859/3589)      29/29 \n",
      "\n",
      "Epoch: 43\n",
      " [=============================>] | Loss: 0.977 | Acc: 63.315% (18177/28709)    225/225 \n",
      " [============================>.] | Loss: 1.361 | Acc: 50.000% (1801/3589)      29/29 \n",
      " [============================>.] | Loss: 1.342 | Acc: 49.000% (1781/3589)      29/29 \n",
      "\n",
      "Epoch: 44\n",
      " [=============================>] | Loss: 0.961 | Acc: 63.942% (18357/28709)    225/225 \n",
      " [============================>.] | Loss: 1.371 | Acc: 49.000% (1784/3589)      29/29 \n",
      " [============================>.] | Loss: 1.354 | Acc: 51.000% (1844/3589)      29/29 \n",
      "\n",
      "Epoch: 45\n",
      " [=============================>] | Loss: 0.956 | Acc: 64.290% (18457/28709)    225/225 \n",
      " [============================>.] | Loss: 1.362 | Acc: 50.000% (1824/3589)      29/29 \n",
      " [============================>.] | Loss: 1.366 | Acc: 51.000% (1841/3589)      29/29 \n",
      "\n",
      "Epoch: 46\n",
      " [=============================>] | Loss: 0.959 | Acc: 64.025% (18381/28709)    225/225 \n",
      " [============================>.] | Loss: 1.374 | Acc: 49.000% (1793/3589)      29/29 \n",
      " [============================>.] | Loss: 1.312 | Acc: 50.000% (1829/3589)      29/29 \n",
      "\n",
      "Epoch: 47\n",
      " [=============================>] | Loss: 0.937 | Acc: 65.070% (18681/28709)    225/225 \n",
      " [============================>.] | Loss: 1.393 | Acc: 49.000% (1793/3589)      29/29 \n",
      " [============================>.] | Loss: 1.338 | Acc: 51.000% (1848/3589)      29/29 \n",
      "\n",
      "Epoch: 48\n",
      " [=============================>] | Loss: 0.933 | Acc: 65.401% (18776/28709)    225/225 \n",
      " [============================>.] | Loss: 1.391 | Acc: 50.000% (1828/3589)      29/29 \n",
      " [============================>.] | Loss: 1.358 | Acc: 50.000% (1818/3589)      29/29 \n",
      "\n",
      "Epoch: 49\n",
      " [=============================>] | Loss: 0.932 | Acc: 65.199% (18718/28709)    225/225 \n",
      " [============================>.] | Loss: 1.373 | Acc: 50.000% (1818/3589)      29/29 \n",
      " [============================>.] | Loss: 1.319 | Acc: 52.000% (1872/3589)      29/29 \n",
      "\n",
      "Epoch: 50\n",
      " [=============================>] | Loss: 0.918 | Acc: 65.697% (18861/28709)    225/225 \n",
      " [============================>.] | Loss: 1.455 | Acc: 49.000% (1775/3589)      29/29 \n",
      " [============================>.] | Loss: 1.374 | Acc: 51.000% (1832/3589)      29/29 \n",
      "\n",
      "Epoch: 51\n",
      " [=============================>] | Loss: 0.907 | Acc: 66.035% (18958/28709)    225/225 \n",
      " [============================>.] | Loss: 1.400 | Acc: 49.000% (1788/3589)      29/29 \n",
      " [============================>.] | Loss: 1.365 | Acc: 52.000% (1868/3589)      29/29 \n",
      "\n",
      "Epoch: 52\n",
      " [=============================>] | Loss: 0.899 | Acc: 66.540% (19103/28709)    225/225 \n",
      " [============================>.] | Loss: 1.398 | Acc: 51.000% (1863/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 51.000\n",
      " [============================>.] | Loss: 1.344 | Acc: 51.000% (1858/3589)      29/29 \n",
      "\n",
      "Epoch: 53\n",
      " [=============================>] | Loss: 0.897 | Acc: 66.432% (19072/28709)    225/225 \n",
      " [============================>.] | Loss: 1.375 | Acc: 50.000% (1795/3589)      29/29 \n",
      " [============================>.] | Loss: 1.371 | Acc: 50.000% (1814/3589)      29/29 \n",
      "\n",
      "Epoch: 54\n",
      " [=============================>] | Loss: 0.890 | Acc: 66.345% (19047/28709)    225/225 \n",
      " [============================>.] | Loss: 1.437 | Acc: 48.000% (1758/3589)      29/29 \n",
      " [============================>.] | Loss: 1.363 | Acc: 51.000% (1841/3589)      29/29 \n",
      "\n",
      "Epoch: 55\n",
      " [=============================>] | Loss: 0.858 | Acc: 68.156% (19567/28709)    225/225 \n",
      " [============================>.] | Loss: 1.378 | Acc: 51.000% (1842/3589)      29/29 \n",
      " [============================>.] | Loss: 1.353 | Acc: 52.000% (1887/3589)      29/29 \n",
      "\n",
      "Epoch: 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.839 | Acc: 68.634% (19704/28709)    225/225 \n",
      " [============================>.] | Loss: 1.393 | Acc: 50.000% (1828/3589)      29/29 \n",
      " [============================>.] | Loss: 1.340 | Acc: 53.000% (1914/3589)      29/29 \n",
      "\n",
      "Epoch: 57\n",
      " [=============================>] | Loss: 0.840 | Acc: 68.975% (19802/28709)    225/225 \n",
      " [============================>.] | Loss: 1.405 | Acc: 50.000% (1822/3589)      29/29 \n",
      " [============================>.] | Loss: 1.349 | Acc: 52.000% (1888/3589)      29/29 \n",
      "\n",
      "Epoch: 58\n",
      " [=============================>] | Loss: 0.824 | Acc: 69.156% (19854/28709)    225/225 \n",
      " [============================>.] | Loss: 1.423 | Acc: 51.000% (1849/3589)      29/29 \n",
      " [============================>.] | Loss: 1.408 | Acc: 50.000% (1822/3589)      29/29 \n",
      "\n",
      "Epoch: 59\n",
      " [=============================>] | Loss: 0.820 | Acc: 69.146% (19851/28709)    225/225 \n",
      " [============================>.] | Loss: 1.410 | Acc: 50.000% (1819/3589)      29/29 \n",
      " [============================>.] | Loss: 1.402 | Acc: 52.000% (1870/3589)      29/29 \n",
      "\n",
      "Epoch: 60\n",
      " [=============================>] | Loss: 0.786 | Acc: 70.668% (20288/28709)    225/225 \n",
      " [============================>.] | Loss: 1.456 | Acc: 49.000% (1785/3589)      29/29 \n",
      " [============================>.] | Loss: 1.408 | Acc: 51.000% (1847/3589)      29/29 \n",
      "\n",
      "Epoch: 61\n",
      " [=============================>] | Loss: 0.768 | Acc: 71.361% (20487/28709)    225/225 \n",
      " [============================>.] | Loss: 1.430 | Acc: 51.000% (1833/3589)      29/29 \n",
      " [============================>.] | Loss: 1.372 | Acc: 52.000% (1885/3589)      29/29 \n",
      "\n",
      "Epoch: 62\n",
      " [=============================>] | Loss: 0.758 | Acc: 72.047% (20684/28709)    225/225 \n",
      " [============================>.] | Loss: 1.449 | Acc: 50.000% (1822/3589)      29/29 \n",
      " [============================>.] | Loss: 1.417 | Acc: 52.000% (1868/3589)      29/29 \n",
      "\n",
      "Epoch: 63\n",
      " [=============================>] | Loss: 0.760 | Acc: 71.608% (20558/28709)    225/225 \n",
      " [============================>.] | Loss: 1.430 | Acc: 51.000% (1833/3589)      29/29 \n",
      " [============================>.] | Loss: 1.388 | Acc: 52.000% (1894/3589)      29/29 \n",
      "\n",
      "Epoch: 64\n",
      " [=============================>] | Loss: 0.752 | Acc: 72.131% (20708/28709)    225/225 \n",
      " [============================>.] | Loss: 1.447 | Acc: 51.000% (1851/3589)      29/29 \n",
      " [============================>.] | Loss: 1.382 | Acc: 52.000% (1897/3589)      29/29 \n",
      "\n",
      "Epoch: 65\n",
      " [=============================>] | Loss: 0.714 | Acc: 73.569% (21121/28709)    225/225 \n",
      " [============================>.] | Loss: 1.472 | Acc: 52.000% (1872/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 52.000\n",
      " [============================>.] | Loss: 1.447 | Acc: 53.000% (1904/3589)      29/29 \n",
      "\n",
      "Epoch: 66\n",
      " [=============================>] | Loss: 0.697 | Acc: 74.722% (21452/28709)    225/225 \n",
      " [============================>.] | Loss: 1.520 | Acc: 51.000% (1831/3589)      29/29 \n",
      " [============================>.] | Loss: 1.476 | Acc: 52.000% (1875/3589)      29/29 \n",
      "\n",
      "Epoch: 67\n",
      " [=============================>] | Loss: 0.692 | Acc: 74.597% (21416/28709)    225/225 \n",
      " [============================>.] | Loss: 1.480 | Acc: 51.000% (1840/3589)      29/29 \n",
      " [============================>.] | Loss: 1.455 | Acc: 53.000% (1912/3589)      29/29 \n",
      "\n",
      "Epoch: 68\n",
      " [=============================>] | Loss: 0.688 | Acc: 74.945% (21516/28709)    225/225 \n",
      " [============================>.] | Loss: 1.566 | Acc: 48.000% (1730/3589)      29/29 \n",
      " [============================>.] | Loss: 1.476 | Acc: 50.000% (1822/3589)      29/29 \n",
      "\n",
      "Epoch: 69\n",
      " [=============================>] | Loss: 0.690 | Acc: 74.764% (21464/28709)    225/225 \n",
      " [============================>.] | Loss: 1.530 | Acc: 50.000% (1799/3589)      29/29 \n",
      " [============================>.] | Loss: 1.475 | Acc: 51.000% (1865/3589)      29/29 \n",
      "\n",
      "Epoch: 70\n",
      " [=============================>] | Loss: 0.650 | Acc: 76.499% (21962/28709)    225/225 \n",
      " [============================>.] | Loss: 1.537 | Acc: 51.000% (1846/3589)      29/29 \n",
      " [============================>.] | Loss: 1.475 | Acc: 53.000% (1927/3589)      29/29 \n",
      "\n",
      "Epoch: 71\n",
      " [=============================>] | Loss: 0.643 | Acc: 76.217% (21881/28709)    225/225 \n",
      " [============================>.] | Loss: 1.547 | Acc: 51.000% (1835/3589)      29/29 \n",
      " [============================>.] | Loss: 1.501 | Acc: 53.000% (1913/3589)      29/29 \n",
      "\n",
      "Epoch: 72\n",
      " [=============================>] | Loss: 0.629 | Acc: 76.941% (22089/28709)    225/225 \n",
      " [============================>.] | Loss: 1.607 | Acc: 51.000% (1842/3589)      29/29 \n",
      " [============================>.] | Loss: 1.554 | Acc: 53.000% (1911/3589)      29/29 \n",
      "\n",
      "Epoch: 73\n",
      " [=============================>] | Loss: 0.622 | Acc: 77.042% (22118/28709)    225/225 \n",
      " [============================>.] | Loss: 1.592 | Acc: 51.000% (1851/3589)      29/29 \n",
      " [============================>.] | Loss: 1.542 | Acc: 53.000% (1925/3589)      29/29 \n",
      "\n",
      "Epoch: 74\n",
      " [=============================>] | Loss: 0.618 | Acc: 77.474% (22242/28709)    225/225 \n",
      " [============================>.] | Loss: 1.532 | Acc: 51.000% (1864/3589)      29/29 \n",
      " [============================>.] | Loss: 1.499 | Acc: 54.000% (1958/3589)      29/29 \n",
      "\n",
      "Epoch: 75\n",
      " [=============================>] | Loss: 0.581 | Acc: 78.878% (22645/28709)    225/225 \n",
      " [============================>.] | Loss: 1.563 | Acc: 51.000% (1863/3589)      29/29 \n",
      " [============================>.] | Loss: 1.524 | Acc: 53.000% (1915/3589)      29/29 \n",
      "\n",
      "Epoch: 76\n",
      " [=============================>] | Loss: 0.560 | Acc: 79.616% (22857/28709)    225/225 \n",
      " [============================>.] | Loss: 1.659 | Acc: 52.000% (1896/3589)      29/29 \n",
      " [============================>.] | Loss: 1.582 | Acc: 54.000% (1944/3589)      29/29 \n",
      "\n",
      "Epoch: 77\n",
      " [=============================>] | Loss: 0.564 | Acc: 79.320% (22772/28709)    225/225 \n",
      " [============================>.] | Loss: 1.599 | Acc: 51.000% (1863/3589)      29/29 \n",
      " [============================>.] | Loss: 1.562 | Acc: 53.000% (1904/3589)      29/29 \n",
      "\n",
      "Epoch: 78\n",
      " [=============================>] | Loss: 0.552 | Acc: 79.661% (22870/28709)    225/225 \n",
      " [============================>.] | Loss: 1.641 | Acc: 50.000% (1823/3589)      29/29 \n",
      " [============================>.] | Loss: 1.622 | Acc: 53.000% (1908/3589)      29/29 \n",
      "\n",
      "Epoch: 79\n",
      " [=============================>] | Loss: 0.553 | Acc: 79.951% (22953/28709)    225/225 \n",
      " [============================>.] | Loss: 1.630 | Acc: 51.000% (1831/3589)      29/29 \n",
      " [============================>.] | Loss: 1.612 | Acc: 52.000% (1889/3589)      29/29 \n",
      "\n",
      "Epoch: 80\n",
      " [=============================>] | Loss: 0.494 | Acc: 81.939% (23524/28709)    225/225 \n",
      " [============================>.] | Loss: 1.658 | Acc: 51.000% (1856/3589)      29/29 \n",
      " [============================>.] | Loss: 1.683 | Acc: 53.000% (1909/3589)      29/29 \n",
      "\n",
      "Epoch: 81\n",
      " [=============================>] | Loss: 0.503 | Acc: 81.699% (23455/28709)    225/225 \n",
      " [============================>.] | Loss: 1.668 | Acc: 50.000% (1829/3589)      29/29 \n",
      " [============================>.] | Loss: 1.675 | Acc: 52.000% (1894/3589)      29/29 \n",
      "\n",
      "Epoch: 82\n",
      " [=============================>] | Loss: 0.506 | Acc: 81.654% (23442/28709)    225/225 \n",
      " [============================>.] | Loss: 1.622 | Acc: 51.000% (1855/3589)      29/29 \n",
      " [============================>.] | Loss: 1.644 | Acc: 53.000% (1913/3589)      29/29 \n",
      "\n",
      "Epoch: 83\n",
      " [=============================>] | Loss: 0.496 | Acc: 81.835% (23494/28709)    225/225 \n",
      " [============================>.] | Loss: 1.677 | Acc: 51.000% (1831/3589)      29/29 \n",
      " [============================>.] | Loss: 1.662 | Acc: 52.000% (1898/3589)      29/29 \n",
      "\n",
      "Epoch: 84\n",
      " [=============================>] | Loss: 0.488 | Acc: 82.406% (23658/28709)    225/225 \n",
      " [============================>.] | Loss: 1.679 | Acc: 50.000% (1803/3589)      29/29 \n",
      " [============================>.] | Loss: 1.656 | Acc: 52.000% (1877/3589)      29/29 \n",
      "\n",
      "Epoch: 85\n",
      " [=============================>] | Loss: 0.469 | Acc: 82.904% (23801/28709)    225/225 \n",
      " [============================>.] | Loss: 1.732 | Acc: 51.000% (1850/3589)      29/29 \n",
      " [============================>.] | Loss: 1.658 | Acc: 53.000% (1936/3589)      29/29 \n",
      "\n",
      "Epoch: 86\n",
      " [=============================>] | Loss: 0.450 | Acc: 83.406% (23945/28709)    225/225 \n",
      " [============================>.] | Loss: 1.722 | Acc: 51.000% (1848/3589)      29/29 \n",
      " [============================>.] | Loss: 1.680 | Acc: 52.000% (1896/3589)      29/29 \n",
      "\n",
      "Epoch: 87\n",
      " [=============================>] | Loss: 0.431 | Acc: 84.322% (24208/28709)    225/225 \n",
      " [============================>.] | Loss: 1.807 | Acc: 52.000% (1880/3589)      29/29 \n",
      " [============================>.] | Loss: 1.738 | Acc: 52.000% (1885/3589)      29/29 \n",
      "\n",
      "Epoch: 88\n",
      " [=============================>] | Loss: 0.436 | Acc: 84.047% (24129/28709)    225/225 \n",
      " [============================>.] | Loss: 1.758 | Acc: 51.000% (1862/3589)      29/29 \n",
      " [============================>.] | Loss: 1.705 | Acc: 54.000% (1947/3589)      29/29 \n",
      "\n",
      "Epoch: 89\n",
      " [=============================>] | Loss: 0.429 | Acc: 84.555% (24275/28709)    225/225 \n",
      " [============================>.] | Loss: 1.778 | Acc: 50.000% (1822/3589)      29/29 \n",
      " [============================>.] | Loss: 1.724 | Acc: 54.000% (1963/3589)      29/29 \n",
      "\n",
      "Epoch: 90\n",
      " [=============================>] | Loss: 0.392 | Acc: 85.684% (24599/28709)    225/225 \n",
      " [============================>.] | Loss: 1.878 | Acc: 52.000% (1872/3589)      29/29 \n",
      " [============================>.] | Loss: 1.844 | Acc: 53.000% (1926/3589)      29/29 \n",
      "\n",
      "Epoch: 91\n",
      " [=============================>] | Loss: 0.381 | Acc: 86.297% (24775/28709)    225/225 \n",
      " [============================>.] | Loss: 1.880 | Acc: 51.000% (1852/3589)      29/29 \n",
      " [============================>.] | Loss: 1.809 | Acc: 53.000% (1932/3589)      29/29 \n",
      "\n",
      "Epoch: 92\n",
      " [=============================>] | Loss: 0.375 | Acc: 86.617% (24867/28709)    225/225 \n",
      " [============================>.] | Loss: 1.871 | Acc: 51.000% (1851/3589)      29/29 \n",
      " [============================>.] | Loss: 1.836 | Acc: 53.000% (1904/3589)      29/29 \n",
      "\n",
      "Epoch: 93\n",
      " [=============================>] | Loss: 0.378 | Acc: 86.395% (24803/28709)    225/225 \n",
      " [============================>.] | Loss: 1.851 | Acc: 51.000% (1861/3589)      29/29 \n",
      " [============================>.] | Loss: 1.799 | Acc: 53.000% (1934/3589)      29/29 \n",
      "\n",
      "Epoch: 94\n",
      " [=============================>] | Loss: 0.373 | Acc: 86.496% (24832/28709)    225/225 \n",
      " [============================>.] | Loss: 1.812 | Acc: 52.000% (1877/3589)      29/29 \n",
      " [============================>.] | Loss: 1.856 | Acc: 54.000% (1943/3589)      29/29 \n",
      "\n",
      "Epoch: 95\n",
      " [=============================>] | Loss: 0.332 | Acc: 87.924% (25242/28709)    225/225 \n",
      " [============================>.] | Loss: 2.034 | Acc: 50.000% (1795/3589)      29/29 \n",
      " [============================>.] | Loss: 1.910 | Acc: 52.000% (1883/3589)      29/29 \n",
      "\n",
      "Epoch: 96\n",
      " [=============================>] | Loss: 0.342 | Acc: 87.460% (25109/28709)    225/225 \n",
      " [============================>.] | Loss: 1.940 | Acc: 51.000% (1852/3589)      29/29 \n",
      " [============================>.] | Loss: 1.865 | Acc: 54.000% (1958/3589)      29/29 \n",
      "\n",
      "Epoch: 97\n",
      " [=============================>] | Loss: 0.321 | Acc: 88.443% (25391/28709)    225/225 \n",
      " [============================>.] | Loss: 1.912 | Acc: 52.000% (1878/3589)      29/29 \n",
      " [============================>.] | Loss: 1.850 | Acc: 54.000% (1948/3589)      29/29 \n",
      "\n",
      "Epoch: 98\n",
      " [=============================>] | Loss: 0.320 | Acc: 88.422% (25385/28709)    225/225 \n",
      " [============================>.] | Loss: 2.003 | Acc: 52.000% (1880/3589)      29/29 \n",
      " [============================>.] | Loss: 1.947 | Acc: 53.000% (1912/3589)      29/29 \n",
      "\n",
      "Epoch: 99\n",
      " [=============================>] | Loss: 0.329 | Acc: 88.126% (25300/28709)    225/225 \n",
      " [============================>.] | Loss: 1.876 | Acc: 53.000% (1909/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 53.000\n",
      " [============================>.] | Loss: 1.907 | Acc: 54.000% (1964/3589)      29/29 \n",
      "\n",
      "Epoch: 100\n",
      " [=============================>] | Loss: 0.294 | Acc: 89.533% (25704/28709)    225/225 \n",
      " [============================>.] | Loss: 1.960 | Acc: 52.000% (1880/3589)      29/29 \n",
      " [============================>.] | Loss: 1.929 | Acc: 54.000% (1957/3589)      29/29 \n",
      "\n",
      "Epoch: 101\n",
      " [=============================>] | Loss: 0.281 | Acc: 89.850% (25795/28709)    225/225 \n",
      " [============================>.] | Loss: 1.993 | Acc: 53.000% (1907/3589)      29/29 \n",
      " [============================>.] | Loss: 1.973 | Acc: 55.000% (1992/3589)      29/29 \n",
      "\n",
      "Epoch: 102\n",
      " [=============================>] | Loss: 0.280 | Acc: 89.881% (25804/28709)    225/225 \n",
      " [============================>.] | Loss: 2.038 | Acc: 52.000% (1867/3589)      29/29 \n",
      " [============================>.] | Loss: 1.967 | Acc: 53.000% (1928/3589)      29/29 \n",
      "\n",
      "Epoch: 103\n",
      " [=============================>] | Loss: 0.275 | Acc: 90.104% (25868/28709)    225/225 \n",
      " [============================>.] | Loss: 2.064 | Acc: 53.000% (1907/3589)      29/29 \n",
      " [============================>.] | Loss: 2.017 | Acc: 54.000% (1967/3589)      29/29 \n",
      "\n",
      "Epoch: 104\n",
      " [=============================>] | Loss: 0.272 | Acc: 90.376% (25946/28709)    225/225 \n",
      " [============================>.] | Loss: 2.115 | Acc: 51.000% (1852/3589)      29/29 \n",
      " [============================>.] | Loss: 2.023 | Acc: 54.000% (1970/3589)      29/29 \n",
      "\n",
      "Epoch: 105\n",
      " [=============================>] | Loss: 0.247 | Acc: 91.330% (26220/28709)    225/225 \n",
      " [============================>.] | Loss: 2.090 | Acc: 53.000% (1924/3589)      29/29 \n",
      " [============================>.] | Loss: 2.035 | Acc: 54.000% (1943/3589)      29/29 \n",
      "\n",
      "Epoch: 106\n",
      " [=============================>] | Loss: 0.227 | Acc: 92.072% (26433/28709)    225/225 \n",
      " [============================>.] | Loss: 2.154 | Acc: 51.000% (1845/3589)      29/29 \n",
      " [============================>.] | Loss: 2.112 | Acc: 53.000% (1926/3589)      29/29 \n",
      "\n",
      "Epoch: 107\n",
      " [=============================>] | Loss: 0.234 | Acc: 91.560% (26286/28709)    225/225 \n",
      " [============================>.] | Loss: 2.172 | Acc: 52.000% (1888/3589)      29/29 \n",
      " [============================>.] | Loss: 2.017 | Acc: 55.000% (1976/3589)      29/29 \n",
      "\n",
      "Epoch: 108\n",
      " [=============================>] | Loss: 0.230 | Acc: 91.776% (26348/28709)    225/225 \n",
      " [============================>.] | Loss: 2.143 | Acc: 53.000% (1915/3589)      29/29 \n",
      " [============================>.] | Loss: 2.045 | Acc: 54.000% (1942/3589)      29/29 \n",
      "\n",
      "Epoch: 109\n",
      " [=============================>] | Loss: 0.228 | Acc: 91.950% (26398/28709)    225/225 \n",
      " [============================>.] | Loss: 2.175 | Acc: 52.000% (1895/3589)      29/29 \n",
      " [============================>.] | Loss: 2.035 | Acc: 55.000% (1984/3589)      29/29 \n",
      "\n",
      "Epoch: 110\n",
      " [=============================>] | Loss: 0.206 | Acc: 92.737% (26624/28709)    225/225 \n",
      " [============================>.] | Loss: 2.163 | Acc: 53.000% (1907/3589)      29/29 \n",
      " [============================>.] | Loss: 2.105 | Acc: 55.000% (2008/3589)      29/29 \n",
      "\n",
      "Epoch: 111\n",
      " [=============================>] | Loss: 0.202 | Acc: 92.943% (26683/28709)    225/225 \n",
      " [============================>.] | Loss: 2.178 | Acc: 54.000% (1943/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 54.000\n",
      " [============================>.] | Loss: 2.150 | Acc: 54.000% (1969/3589)      29/29 \n",
      "\n",
      "Epoch: 112\n",
      " [=============================>] | Loss: 0.189 | Acc: 93.497% (26842/28709)    225/225 \n",
      " [============================>.] | Loss: 2.249 | Acc: 52.000% (1867/3589)      29/29 \n",
      " [============================>.] | Loss: 2.107 | Acc: 53.000% (1909/3589)      29/29 \n",
      "\n",
      "Epoch: 113\n",
      " [=============================>] | Loss: 0.195 | Acc: 93.013% (26703/28709)    225/225 \n",
      " [============================>.] | Loss: 2.266 | Acc: 53.000% (1915/3589)      29/29 \n",
      " [============================>.] | Loss: 2.169 | Acc: 55.000% (1985/3589)      29/29 \n",
      "\n",
      "Epoch: 114\n",
      " [=============================>] | Loss: 0.191 | Acc: 93.260% (26774/28709)    225/225 \n",
      " [============================>.] | Loss: 2.261 | Acc: 53.000% (1903/3589)      29/29 \n",
      " [============================>.] | Loss: 2.181 | Acc: 55.000% (1977/3589)      29/29 \n",
      "\n",
      "Epoch: 115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.171 | Acc: 93.995% (26985/28709)    225/225 \n",
      " [============================>.] | Loss: 2.291 | Acc: 53.000% (1937/3589)      29/29 \n",
      " [============================>.] | Loss: 2.193 | Acc: 55.000% (2000/3589)      29/29 \n",
      "\n",
      "Epoch: 116\n",
      " [=============================>] | Loss: 0.163 | Acc: 94.263% (27062/28709)    225/225 \n",
      " [============================>.] | Loss: 2.291 | Acc: 53.000% (1925/3589)      29/29 \n",
      " [============================>.] | Loss: 2.252 | Acc: 55.000% (1980/3589)      29/29 \n",
      "\n",
      "Epoch: 117\n",
      " [=============================>] | Loss: 0.168 | Acc: 94.186% (27040/28709)    225/225 \n",
      " [============================>.] | Loss: 2.262 | Acc: 52.000% (1887/3589)      29/29 \n",
      " [============================>.] | Loss: 2.193 | Acc: 53.000% (1936/3589)      29/29 \n",
      "\n",
      "Epoch: 118\n",
      " [=============================>] | Loss: 0.153 | Acc: 94.611% (27162/28709)    225/225 \n",
      " [============================>.] | Loss: 2.332 | Acc: 53.000% (1908/3589)      29/29 \n",
      " [============================>.] | Loss: 2.248 | Acc: 54.000% (1965/3589)      29/29 \n",
      "\n",
      "Epoch: 119\n",
      " [=============================>] | Loss: 0.163 | Acc: 94.291% (27070/28709)    225/225 \n",
      " [============================>.] | Loss: 2.294 | Acc: 52.000% (1892/3589)      29/29 \n",
      " [============================>.] | Loss: 2.221 | Acc: 55.000% (1990/3589)      29/29 \n",
      "\n",
      "Epoch: 120\n",
      " [=============================>] | Loss: 0.142 | Acc: 95.012% (27277/28709)    225/225 \n",
      " [============================>.] | Loss: 2.303 | Acc: 53.000% (1938/3589)      29/29 \n",
      " [============================>.] | Loss: 2.228 | Acc: 55.000% (1991/3589)      29/29 \n",
      "\n",
      "Epoch: 121\n",
      " [=============================>] | Loss: 0.133 | Acc: 95.489% (27414/28709)    225/225 \n",
      " [============================>.] | Loss: 2.290 | Acc: 54.000% (1949/3589)      29/29 \n",
      " [============================>.] | Loss: 2.259 | Acc: 54.000% (1964/3589)      29/29 \n",
      "\n",
      "Epoch: 122\n",
      " [=============================>] | Loss: 0.125 | Acc: 95.667% (27465/28709)    225/225 \n",
      " [============================>.] | Loss: 2.403 | Acc: 53.000% (1906/3589)      29/29 \n",
      " [============================>.] | Loss: 2.324 | Acc: 54.000% (1967/3589)      29/29 \n",
      "\n",
      "Epoch: 123\n",
      " [=============================>] | Loss: 0.137 | Acc: 95.204% (27332/28709)    225/225 \n",
      " [============================>.] | Loss: 2.411 | Acc: 53.000% (1905/3589)      29/29 \n",
      " [============================>.] | Loss: 2.288 | Acc: 54.000% (1969/3589)      29/29 \n",
      "\n",
      "Epoch: 124\n",
      " [=============================>] | Loss: 0.135 | Acc: 95.378% (27382/28709)    225/225 \n",
      " [============================>.] | Loss: 2.391 | Acc: 52.000% (1891/3589)      29/29 \n",
      " [============================>.] | Loss: 2.398 | Acc: 54.000% (1964/3589)      29/29 \n",
      "\n",
      "Epoch: 125\n",
      " [=============================>] | Loss: 0.123 | Acc: 95.691% (27472/28709)    225/225 \n",
      " [============================>.] | Loss: 2.417 | Acc: 53.000% (1923/3589)      29/29 \n",
      " [============================>.] | Loss: 2.336 | Acc: 55.000% (1985/3589)      29/29 \n",
      "\n",
      "Epoch: 126\n",
      " [=============================>] | Loss: 0.113 | Acc: 95.987% (27557/28709)    225/225 \n",
      " [============================>.] | Loss: 2.390 | Acc: 53.000% (1922/3589)      29/29 \n",
      " [============================>.] | Loss: 2.307 | Acc: 55.000% (1980/3589)      29/29 \n",
      "\n",
      "Epoch: 127\n",
      " [=============================>] | Loss: 0.103 | Acc: 96.538% (27715/28709)    225/225 \n",
      " [============================>.] | Loss: 2.467 | Acc: 53.000% (1909/3589)      29/29 \n",
      " [============================>.] | Loss: 2.344 | Acc: 54.000% (1954/3589)      29/29 \n",
      "\n",
      "Epoch: 128\n",
      " [=============================>] | Loss: 0.111 | Acc: 96.256% (27634/28709)    225/225 \n",
      " [============================>.] | Loss: 2.462 | Acc: 52.000% (1896/3589)      29/29 \n",
      " [============================>.] | Loss: 2.366 | Acc: 54.000% (1950/3589)      29/29 \n",
      "\n",
      "Epoch: 129\n",
      " [=============================>] | Loss: 0.107 | Acc: 96.256% (27634/28709)    225/225 \n",
      " [============================>.] | Loss: 2.438 | Acc: 53.000% (1919/3589)      29/29 \n",
      " [============================>.] | Loss: 2.355 | Acc: 55.000% (1992/3589)      29/29 \n",
      "\n",
      "Epoch: 130\n",
      " [=============================>] | Loss: 0.093 | Acc: 96.792% (27788/28709)    225/225 \n",
      " [============================>.] | Loss: 2.452 | Acc: 53.000% (1929/3589)      29/29 \n",
      " [============================>.] | Loss: 2.386 | Acc: 54.000% (1970/3589)      29/29 \n",
      "\n",
      "Epoch: 131\n",
      " [=============================>] | Loss: 0.088 | Acc: 97.029% (27856/28709)    225/225 \n",
      " [============================>.] | Loss: 2.494 | Acc: 52.000% (1887/3589)      29/29 \n",
      " [============================>.] | Loss: 2.391 | Acc: 55.000% (1979/3589)      29/29 \n",
      "\n",
      "Epoch: 132\n",
      " [=============================>] | Loss: 0.086 | Acc: 97.151% (27891/28709)    225/225 \n",
      " [============================>.] | Loss: 2.458 | Acc: 53.000% (1912/3589)      29/29 \n",
      " [============================>.] | Loss: 2.387 | Acc: 54.000% (1971/3589)      29/29 \n",
      "\n",
      "Epoch: 133\n",
      " [=============================>] | Loss: 0.090 | Acc: 96.924% (27826/28709)    225/225 \n",
      " [============================>.] | Loss: 2.507 | Acc: 53.000% (1914/3589)      29/29 \n",
      " [============================>.] | Loss: 2.402 | Acc: 55.000% (1985/3589)      29/29 \n",
      "\n",
      "Epoch: 134\n",
      " [=============================>] | Loss: 0.082 | Acc: 97.161% (27894/28709)    225/225 \n",
      " [============================>.] | Loss: 2.530 | Acc: 54.000% (1939/3589)      29/29 \n",
      " [============================>.] | Loss: 2.440 | Acc: 55.000% (1975/3589)      29/29 \n",
      "\n",
      "Epoch: 135\n",
      " [=============================>] | Loss: 0.076 | Acc: 97.384% (27958/28709)    225/225 \n",
      " [============================>.] | Loss: 2.549 | Acc: 53.000% (1914/3589)      29/29 \n",
      " [============================>.] | Loss: 2.474 | Acc: 54.000% (1954/3589)      29/29 \n",
      "\n",
      "Epoch: 136\n",
      " [=============================>] | Loss: 0.073 | Acc: 97.617% (28025/28709)    225/225 \n",
      " [============================>.] | Loss: 2.578 | Acc: 53.000% (1918/3589)      29/29 \n",
      " [============================>.] | Loss: 2.451 | Acc: 55.000% (1979/3589)      29/29 \n",
      "\n",
      "Epoch: 137\n",
      " [=============================>] | Loss: 0.075 | Acc: 97.485% (27987/28709)    225/225 \n",
      " [============================>.] | Loss: 2.576 | Acc: 53.000% (1915/3589)      29/29 \n",
      " [============================>.] | Loss: 2.482 | Acc: 55.000% (1995/3589)      29/29 \n",
      "\n",
      "Epoch: 138\n",
      " [=============================>] | Loss: 0.078 | Acc: 97.360% (27951/28709)    225/225 \n",
      " [============================>.] | Loss: 2.621 | Acc: 52.000% (1901/3589)      29/29 \n",
      " [============================>.] | Loss: 2.499 | Acc: 55.000% (1976/3589)      29/29 \n",
      "\n",
      "Epoch: 139\n",
      " [=============================>] | Loss: 0.074 | Acc: 97.461% (27980/28709)    225/225 \n",
      " [============================>.] | Loss: 2.575 | Acc: 53.000% (1937/3589)      29/29 \n",
      " [============================>.] | Loss: 2.474 | Acc: 56.000% (2022/3589)      29/29 \n",
      "\n",
      "Epoch: 140\n",
      " [=============================>] | Loss: 0.065 | Acc: 97.718% (28054/28709)    225/225 \n",
      " [============================>.] | Loss: 2.555 | Acc: 53.000% (1935/3589)      29/29 \n",
      " [============================>.] | Loss: 2.453 | Acc: 55.000% (2004/3589)      29/29 \n",
      "\n",
      "Epoch: 141\n",
      " [=============================>] | Loss: 0.061 | Acc: 97.921% (28112/28709)    225/225 \n",
      " [============================>.] | Loss: 2.494 | Acc: 54.000% (1952/3589)      29/29 \n",
      " [============================>.] | Loss: 2.451 | Acc: 55.000% (1995/3589)      29/29 \n",
      "\n",
      "Epoch: 142\n",
      " [=============================>] | Loss: 0.059 | Acc: 98.116% (28168/28709)    225/225 \n",
      " [============================>.] | Loss: 2.558 | Acc: 54.000% (1942/3589)      29/29 \n",
      " [============================>.] | Loss: 2.488 | Acc: 54.000% (1971/3589)      29/29 \n",
      "\n",
      "Epoch: 143\n",
      " [=============================>] | Loss: 0.054 | Acc: 98.220% (28198/28709)    225/225 \n",
      " [============================>.] | Loss: 2.592 | Acc: 54.000% (1939/3589)      29/29 \n",
      " [============================>.] | Loss: 2.485 | Acc: 56.000% (2010/3589)      29/29 \n",
      "\n",
      "Epoch: 144\n",
      " [=============================>] | Loss: 0.057 | Acc: 98.161% (28181/28709)    225/225 \n",
      " [============================>.] | Loss: 2.593 | Acc: 53.000% (1929/3589)      29/29 \n",
      " [============================>.] | Loss: 2.519 | Acc: 55.000% (2000/3589)      29/29 \n",
      "\n",
      "Epoch: 145\n",
      " [=============================>] | Loss: 0.055 | Acc: 98.304% (28222/28709)    225/225 \n",
      " [============================>.] | Loss: 2.615 | Acc: 53.000% (1919/3589)      29/29 \n",
      " [============================>.] | Loss: 2.501 | Acc: 54.000% (1972/3589)      29/29 \n",
      "\n",
      "Epoch: 146\n",
      " [=============================>] | Loss: 0.050 | Acc: 98.415% (28254/28709)    225/225 \n",
      " [============================>.] | Loss: 2.576 | Acc: 54.000% (1963/3589)      29/29 \n",
      " [============================>.] | Loss: 2.479 | Acc: 55.000% (2003/3589)      29/29 \n",
      "\n",
      "Epoch: 147\n",
      " [=============================>] | Loss: 0.045 | Acc: 98.492% (28276/28709)    225/225 \n",
      " [============================>.] | Loss: 2.625 | Acc: 53.000% (1919/3589)      29/29 \n",
      " [============================>.] | Loss: 2.530 | Acc: 54.000% (1966/3589)      29/29 \n",
      "\n",
      "Epoch: 148\n",
      " [=============================>] | Loss: 0.051 | Acc: 98.328% (28229/28709)    225/225 \n",
      " [============================>.] | Loss: 2.614 | Acc: 54.000% (1946/3589)      29/29 \n",
      " [============================>.] | Loss: 2.534 | Acc: 55.000% (2003/3589)      29/29 \n",
      "\n",
      "Epoch: 149\n",
      " [=============================>] | Loss: 0.051 | Acc: 98.332% (28230/28709)    225/225 \n",
      " [============================>.] | Loss: 2.606 | Acc: 53.000% (1935/3589)      29/29 \n",
      " [============================>.] | Loss: 2.572 | Acc: 55.000% (1999/3589)      29/29 \n",
      "\n",
      "Epoch: 150\n",
      " [=============================>] | Loss: 0.044 | Acc: 98.551% (28293/28709)    225/225 \n",
      " [============================>.] | Loss: 2.633 | Acc: 54.000% (1940/3589)      29/29 \n",
      " [============================>.] | Loss: 2.536 | Acc: 55.000% (2008/3589)      29/29 \n",
      "\n",
      "Epoch: 151\n",
      " [=============================>] | Loss: 0.040 | Acc: 98.638% (28318/28709)    225/225 \n",
      " [============================>.] | Loss: 2.645 | Acc: 53.000% (1936/3589)      29/29 \n",
      " [============================>.] | Loss: 2.585 | Acc: 55.000% (1997/3589)      29/29 \n",
      "\n",
      "Epoch: 152\n",
      " [=============================>] | Loss: 0.040 | Acc: 98.659% (28324/28709)    225/225 \n",
      " [============================>.] | Loss: 2.670 | Acc: 52.000% (1896/3589)      29/29 \n",
      " [============================>.] | Loss: 2.569 | Acc: 55.000% (1980/3589)      29/29 \n",
      "\n",
      "Epoch: 153\n",
      " [=============================>] | Loss: 0.038 | Acc: 98.833% (28374/28709)    225/225 \n",
      " [============================>.] | Loss: 2.624 | Acc: 54.000% (1946/3589)      29/29 \n",
      " [============================>.] | Loss: 2.609 | Acc: 55.000% (1988/3589)      29/29 \n",
      "\n",
      "Epoch: 154\n",
      " [=============================>] | Loss: 0.038 | Acc: 98.760% (28353/28709)    225/225 \n",
      " [============================>.] | Loss: 2.676 | Acc: 53.000% (1925/3589)      29/29 \n",
      " [============================>.] | Loss: 2.619 | Acc: 55.000% (1994/3589)      29/29 \n",
      "\n",
      "Epoch: 155\n",
      " [=============================>] | Loss: 0.039 | Acc: 98.756% (28352/28709)    225/225 \n",
      " [============================>.] | Loss: 2.684 | Acc: 54.000% (1946/3589)      29/29 \n",
      " [============================>.] | Loss: 2.611 | Acc: 55.000% (2003/3589)      29/29 \n",
      "\n",
      "Epoch: 156\n",
      " [=============================>] | Loss: 0.038 | Acc: 98.760% (28353/28709)    225/225 \n",
      " [============================>.] | Loss: 2.681 | Acc: 54.000% (1942/3589)      29/29 \n",
      " [============================>.] | Loss: 2.574 | Acc: 55.000% (2002/3589)      29/29 \n",
      "\n",
      "Epoch: 157\n",
      " [=============================>] | Loss: 0.039 | Acc: 98.638% (28318/28709)    225/225 \n",
      " [============================>.] | Loss: 2.701 | Acc: 53.000% (1931/3589)      29/29 \n",
      " [============================>.] | Loss: 2.589 | Acc: 56.000% (2012/3589)      29/29 \n",
      "\n",
      "Epoch: 158\n",
      " [=============================>] | Loss: 0.036 | Acc: 98.725% (28343/28709)    225/225 \n",
      " [============================>.] | Loss: 2.684 | Acc: 53.000% (1912/3589)      29/29 \n",
      " [============================>.] | Loss: 2.584 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 159\n",
      " [=============================>] | Loss: 0.033 | Acc: 98.927% (28401/28709)    225/225 \n",
      " [============================>.] | Loss: 2.697 | Acc: 53.000% (1931/3589)      29/29 \n",
      " [============================>.] | Loss: 2.567 | Acc: 56.000% (2022/3589)      29/29 \n",
      "\n",
      "Epoch: 160\n",
      " [=============================>] | Loss: 0.031 | Acc: 98.972% (28414/28709)    225/225 \n",
      " [============================>.] | Loss: 2.690 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.562 | Acc: 55.000% (1995/3589)      29/29 \n",
      "\n",
      "Epoch: 161\n",
      " [=============================>] | Loss: 0.029 | Acc: 99.073% (28443/28709)    225/225 \n",
      " [============================>.] | Loss: 2.702 | Acc: 54.000% (1943/3589)      29/29 \n",
      " [============================>.] | Loss: 2.595 | Acc: 56.000% (2017/3589)      29/29 \n",
      "\n",
      "Epoch: 162\n",
      " [=============================>] | Loss: 0.032 | Acc: 98.986% (28418/28709)    225/225 \n",
      " [============================>.] | Loss: 2.708 | Acc: 54.000% (1955/3589)      29/29 \n",
      " [============================>.] | Loss: 2.597 | Acc: 56.000% (2013/3589)      29/29 \n",
      "\n",
      "Epoch: 163\n",
      " [=============================>] | Loss: 0.029 | Acc: 99.035% (28432/28709)    225/225 \n",
      " [============================>.] | Loss: 2.743 | Acc: 53.000% (1932/3589)      29/29 \n",
      " [============================>.] | Loss: 2.594 | Acc: 55.000% (2007/3589)      29/29 \n",
      "\n",
      "Epoch: 164\n",
      " [=============================>] | Loss: 0.027 | Acc: 99.185% (28475/28709)    225/225 \n",
      " [============================>.] | Loss: 2.715 | Acc: 53.000% (1933/3589)      29/29 \n",
      " [============================>.] | Loss: 2.618 | Acc: 56.000% (2019/3589)      29/29 \n",
      "\n",
      "Epoch: 165\n",
      " [=============================>] | Loss: 0.027 | Acc: 99.087% (28447/28709)    225/225 \n",
      " [============================>.] | Loss: 2.725 | Acc: 54.000% (1952/3589)      29/29 \n",
      " [============================>.] | Loss: 2.611 | Acc: 56.000% (2036/3589)      29/29 \n",
      "\n",
      "Epoch: 166\n",
      " [=============================>] | Loss: 0.025 | Acc: 99.181% (28474/28709)    225/225 \n",
      " [============================>.] | Loss: 2.737 | Acc: 54.000% (1948/3589)      29/29 \n",
      " [============================>.] | Loss: 2.642 | Acc: 55.000% (2003/3589)      29/29 \n",
      "\n",
      "Epoch: 167\n",
      " [=============================>] | Loss: 0.027 | Acc: 99.108% (28453/28709)    225/225 \n",
      " [============================>.] | Loss: 2.758 | Acc: 54.000% (1956/3589)      29/29 \n",
      " [============================>.] | Loss: 2.647 | Acc: 56.000% (2022/3589)      29/29 \n",
      "\n",
      "Epoch: 168\n",
      " [=============================>] | Loss: 0.024 | Acc: 99.227% (28487/28709)    225/225 \n",
      " [============================>.] | Loss: 2.784 | Acc: 54.000% (1962/3589)      29/29 \n",
      " [============================>.] | Loss: 2.659 | Acc: 56.000% (2027/3589)      29/29 \n",
      "\n",
      "Epoch: 169\n",
      " [=============================>] | Loss: 0.023 | Acc: 99.241% (28491/28709)    225/225 \n",
      " [============================>.] | Loss: 2.775 | Acc: 54.000% (1941/3589)      29/29 \n",
      " [============================>.] | Loss: 2.631 | Acc: 55.000% (1997/3589)      29/29 \n",
      "\n",
      "Epoch: 170\n",
      " [=============================>] | Loss: 0.023 | Acc: 99.237% (28490/28709)    225/225 \n",
      " [============================>.] | Loss: 2.765 | Acc: 54.000% (1960/3589)      29/29 \n",
      " [============================>.] | Loss: 2.643 | Acc: 56.000% (2014/3589)      29/29 \n",
      "\n",
      "Epoch: 171\n",
      " [=============================>] | Loss: 0.022 | Acc: 99.213% (28483/28709)    225/225 \n",
      " [============================>.] | Loss: 2.737 | Acc: 54.000% (1967/3589)      29/29 \n",
      " [============================>.] | Loss: 2.642 | Acc: 56.000% (2015/3589)      29/29 \n",
      "\n",
      "Epoch: 172\n",
      " [=============================>] | Loss: 0.021 | Acc: 99.335% (28518/28709)    225/225 \n",
      " [============================>.] | Loss: 2.732 | Acc: 54.000% (1952/3589)      29/29 \n",
      " [============================>.] | Loss: 2.626 | Acc: 56.000% (2015/3589)      29/29 \n",
      "\n",
      "Epoch: 173\n",
      " [=============================>] | Loss: 0.021 | Acc: 99.262% (28497/28709)    225/225 \n",
      " [============================>.] | Loss: 2.732 | Acc: 54.000% (1959/3589)      29/29 \n",
      " [============================>.] | Loss: 2.641 | Acc: 56.000% (2013/3589)      29/29 \n",
      "\n",
      "Epoch: 174\n",
      " [=============================>] | Loss: 0.021 | Acc: 99.317% (28513/28709)    225/225 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================>.] | Loss: 2.772 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.657 | Acc: 56.000% (2016/3589)      29/29 \n",
      "\n",
      "Epoch: 175\n",
      " [=============================>] | Loss: 0.021 | Acc: 99.338% (28519/28709)    225/225 \n",
      " [============================>.] | Loss: 2.774 | Acc: 54.000% (1945/3589)      29/29 \n",
      " [============================>.] | Loss: 2.655 | Acc: 56.000% (2010/3589)      29/29 \n",
      "\n",
      "Epoch: 176\n",
      " [=============================>] | Loss: 0.020 | Acc: 99.366% (28527/28709)    225/225 \n",
      " [============================>.] | Loss: 2.782 | Acc: 54.000% (1940/3589)      29/29 \n",
      " [============================>.] | Loss: 2.647 | Acc: 55.000% (1992/3589)      29/29 \n",
      "\n",
      "Epoch: 177\n",
      " [=============================>] | Loss: 0.022 | Acc: 99.286% (28504/28709)    225/225 \n",
      " [============================>.] | Loss: 2.762 | Acc: 54.000% (1960/3589)      29/29 \n",
      " [============================>.] | Loss: 2.694 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 178\n",
      " [=============================>] | Loss: 0.019 | Acc: 99.411% (28540/28709)    225/225 \n",
      " [============================>.] | Loss: 2.752 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.656 | Acc: 56.000% (2015/3589)      29/29 \n",
      "\n",
      "Epoch: 179\n",
      " [=============================>] | Loss: 0.017 | Acc: 99.446% (28550/28709)    225/225 \n",
      " [============================>.] | Loss: 2.780 | Acc: 54.000% (1959/3589)      29/29 \n",
      " [============================>.] | Loss: 2.640 | Acc: 55.000% (2003/3589)      29/29 \n",
      "\n",
      "Epoch: 180\n",
      " [=============================>] | Loss: 0.020 | Acc: 99.342% (28520/28709)    225/225 \n",
      " [============================>.] | Loss: 2.780 | Acc: 54.000% (1945/3589)      29/29 \n",
      " [============================>.] | Loss: 2.647 | Acc: 55.000% (2006/3589)      29/29 \n",
      "\n",
      "Epoch: 181\n",
      " [=============================>] | Loss: 0.018 | Acc: 99.363% (28526/28709)    225/225 \n",
      " [============================>.] | Loss: 2.797 | Acc: 54.000% (1942/3589)      29/29 \n",
      " [============================>.] | Loss: 2.665 | Acc: 56.000% (2015/3589)      29/29 \n",
      "\n",
      "Epoch: 182\n",
      " [=============================>] | Loss: 0.017 | Acc: 99.436% (28547/28709)    225/225 \n",
      " [============================>.] | Loss: 2.732 | Acc: 55.000% (1976/3589)      29/29 \n",
      "Saving..\n",
      "best_PublicTest_acc: 55.000\n",
      " [============================>.] | Loss: 2.647 | Acc: 56.000% (2023/3589)      29/29 \n",
      "\n",
      "Epoch: 183\n",
      " [=============================>] | Loss: 0.017 | Acc: 99.432% (28546/28709)    225/225 \n",
      " [============================>.] | Loss: 2.736 | Acc: 54.000% (1970/3589)      29/29 \n",
      " [============================>.] | Loss: 2.641 | Acc: 55.000% (2008/3589)      29/29 \n",
      "\n",
      "Epoch: 184\n",
      " [=============================>] | Loss: 0.016 | Acc: 99.429% (28545/28709)    225/225 \n",
      " [============================>.] | Loss: 2.765 | Acc: 54.000% (1973/3589)      29/29 \n",
      " [============================>.] | Loss: 2.669 | Acc: 55.000% (2007/3589)      29/29 \n",
      "\n",
      "Epoch: 185\n",
      " [=============================>] | Loss: 0.015 | Acc: 99.516% (28570/28709)    225/225 \n",
      " [============================>.] | Loss: 2.769 | Acc: 54.000% (1970/3589)      29/29 \n",
      " [============================>.] | Loss: 2.687 | Acc: 56.000% (2017/3589)      29/29 \n",
      "\n",
      "Epoch: 186\n",
      " [=============================>] | Loss: 0.015 | Acc: 99.484% (28561/28709)    225/225 \n",
      " [============================>.] | Loss: 2.740 | Acc: 54.000% (1971/3589)      29/29 \n",
      " [============================>.] | Loss: 2.646 | Acc: 56.000% (2013/3589)      29/29 \n",
      "\n",
      "Epoch: 187\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.540% (28577/28709)    225/225 \n",
      " [============================>.] | Loss: 2.758 | Acc: 54.000% (1966/3589)      29/29 \n",
      " [============================>.] | Loss: 2.693 | Acc: 55.000% (1996/3589)      29/29 \n",
      "\n",
      "Epoch: 188\n",
      " [=============================>] | Loss: 0.015 | Acc: 99.526% (28573/28709)    225/225 \n",
      " [============================>.] | Loss: 2.750 | Acc: 54.000% (1966/3589)      29/29 \n",
      " [============================>.] | Loss: 2.678 | Acc: 55.000% (1999/3589)      29/29 \n",
      "\n",
      "Epoch: 189\n",
      " [=============================>] | Loss: 0.016 | Acc: 99.523% (28572/28709)    225/225 \n",
      " [============================>.] | Loss: 2.783 | Acc: 54.000% (1945/3589)      29/29 \n",
      " [============================>.] | Loss: 2.716 | Acc: 55.000% (2006/3589)      29/29 \n",
      "\n",
      "Epoch: 190\n",
      " [=============================>] | Loss: 0.015 | Acc: 99.516% (28570/28709)    225/225 \n",
      " [============================>.] | Loss: 2.748 | Acc: 54.000% (1973/3589)      29/29 \n",
      " [============================>.] | Loss: 2.671 | Acc: 56.000% (2018/3589)      29/29 \n",
      "\n",
      "Epoch: 191\n",
      " [=============================>] | Loss: 0.015 | Acc: 99.498% (28565/28709)    225/225 \n",
      " [============================>.] | Loss: 2.755 | Acc: 55.000% (1977/3589)      29/29 \n",
      " [============================>.] | Loss: 2.672 | Acc: 56.000% (2024/3589)      29/29 \n",
      "\n",
      "Epoch: 192\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.502% (28566/28709)    225/225 \n",
      " [============================>.] | Loss: 2.765 | Acc: 54.000% (1969/3589)      29/29 \n",
      " [============================>.] | Loss: 2.682 | Acc: 56.000% (2020/3589)      29/29 \n",
      "\n",
      "Epoch: 193\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.537% (28576/28709)    225/225 \n",
      " [============================>.] | Loss: 2.772 | Acc: 54.000% (1970/3589)      29/29 \n",
      " [============================>.] | Loss: 2.671 | Acc: 56.000% (2011/3589)      29/29 \n",
      "\n",
      "Epoch: 194\n",
      " [=============================>] | Loss: 0.015 | Acc: 99.530% (28574/28709)    225/225 \n",
      " [============================>.] | Loss: 2.754 | Acc: 53.000% (1934/3589)      29/29 \n",
      " [============================>.] | Loss: 2.661 | Acc: 56.000% (2013/3589)      29/29 \n",
      "\n",
      "Epoch: 195\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.530% (28574/28709)    225/225 \n",
      " [============================>.] | Loss: 2.760 | Acc: 54.000% (1969/3589)      29/29 \n",
      " [============================>.] | Loss: 2.676 | Acc: 56.000% (2015/3589)      29/29 \n",
      "\n",
      "Epoch: 196\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.568% (28585/28709)    225/225 \n",
      " [============================>.] | Loss: 2.774 | Acc: 54.000% (1972/3589)      29/29 \n",
      " [============================>.] | Loss: 2.708 | Acc: 56.000% (2016/3589)      29/29 \n",
      "\n",
      "Epoch: 197\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.544% (28578/28709)    225/225 \n",
      " [============================>.] | Loss: 2.743 | Acc: 54.000% (1969/3589)      29/29 \n",
      " [============================>.] | Loss: 2.658 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 198\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.568% (28585/28709)    225/225 \n",
      " [============================>.] | Loss: 2.745 | Acc: 54.000% (1965/3589)      29/29 \n",
      " [============================>.] | Loss: 2.666 | Acc: 56.000% (2023/3589)      29/29 \n",
      "\n",
      "Epoch: 199\n",
      " [=============================>] | Loss: 0.013 | Acc: 99.540% (28577/28709)    225/225 \n",
      " [============================>.] | Loss: 2.754 | Acc: 53.000% (1935/3589)      29/29 \n",
      " [============================>.] | Loss: 2.676 | Acc: 56.000% (2018/3589)      29/29 \n",
      "\n",
      "Epoch: 200\n",
      " [=============================>] | Loss: 0.013 | Acc: 99.579% (28588/28709)    225/225 \n",
      " [============================>.] | Loss: 2.772 | Acc: 54.000% (1968/3589)      29/29 \n",
      " [============================>.] | Loss: 2.682 | Acc: 56.000% (2022/3589)      29/29 \n",
      "\n",
      "Epoch: 201\n",
      " [=============================>] | Loss: 0.013 | Acc: 99.558% (28582/28709)    225/225 \n",
      " [============================>.] | Loss: 2.789 | Acc: 54.000% (1944/3589)      29/29 \n",
      " [============================>.] | Loss: 2.689 | Acc: 55.000% (1998/3589)      29/29 \n",
      "\n",
      "Epoch: 202\n",
      " [=============================>] | Loss: 0.012 | Acc: 99.655% (28610/28709)    225/225 \n",
      " [============================>.] | Loss: 2.797 | Acc: 54.000% (1946/3589)      29/29 \n",
      " [============================>.] | Loss: 2.686 | Acc: 56.000% (2018/3589)      29/29 \n",
      "\n",
      "Epoch: 203\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.582% (28589/28709)    225/225 \n",
      " [============================>.] | Loss: 2.804 | Acc: 54.000% (1963/3589)      29/29 \n",
      " [============================>.] | Loss: 2.712 | Acc: 55.000% (2005/3589)      29/29 \n",
      "\n",
      "Epoch: 204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.012 | Acc: 99.620% (28600/28709)    225/225 \n",
      " [============================>.] | Loss: 2.795 | Acc: 54.000% (1959/3589)      29/29 \n",
      " [============================>.] | Loss: 2.701 | Acc: 56.000% (2027/3589)      29/29 \n",
      "\n",
      "Epoch: 205\n",
      " [=============================>] | Loss: 0.012 | Acc: 99.582% (28589/28709)    225/225 \n",
      " [============================>.] | Loss: 2.818 | Acc: 54.000% (1953/3589)      29/29 \n",
      " [============================>.] | Loss: 2.686 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 206\n",
      " [=============================>] | Loss: 0.012 | Acc: 99.592% (28592/28709)    225/225 \n",
      " [============================>.] | Loss: 2.794 | Acc: 54.000% (1961/3589)      29/29 \n",
      " [============================>.] | Loss: 2.676 | Acc: 56.000% (2029/3589)      29/29 \n",
      "\n",
      "Epoch: 207\n",
      " [=============================>] | Loss: 0.012 | Acc: 99.579% (28588/28709)    225/225 \n",
      " [============================>.] | Loss: 2.785 | Acc: 54.000% (1961/3589)      29/29 \n",
      " [============================>.] | Loss: 2.664 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 208\n",
      " [=============================>] | Loss: 0.012 | Acc: 99.568% (28585/28709)    225/225 \n",
      " [============================>.] | Loss: 2.794 | Acc: 54.000% (1960/3589)      29/29 \n",
      " [============================>.] | Loss: 2.679 | Acc: 56.000% (2033/3589)      29/29 \n",
      "\n",
      "Epoch: 209\n",
      " [=============================>] | Loss: 0.014 | Acc: 99.596% (28593/28709)    225/225 \n",
      " [============================>.] | Loss: 2.794 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.674 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 210\n",
      " [=============================>] | Loss: 0.012 | Acc: 99.579% (28588/28709)    225/225 \n",
      " [============================>.] | Loss: 2.784 | Acc: 54.000% (1949/3589)      29/29 \n",
      " [============================>.] | Loss: 2.664 | Acc: 56.000% (2022/3589)      29/29 \n",
      "\n",
      "Epoch: 211\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.669% (28614/28709)    225/225 \n",
      " [============================>.] | Loss: 2.778 | Acc: 54.000% (1964/3589)      29/29 \n",
      " [============================>.] | Loss: 2.675 | Acc: 56.000% (2043/3589)      29/29 \n",
      "\n",
      "Epoch: 212\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.659% (28611/28709)    225/225 \n",
      " [============================>.] | Loss: 2.784 | Acc: 54.000% (1964/3589)      29/29 \n",
      " [============================>.] | Loss: 2.659 | Acc: 56.000% (2018/3589)      29/29 \n",
      "\n",
      "Epoch: 213\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.655% (28610/28709)    225/225 \n",
      " [============================>.] | Loss: 2.791 | Acc: 54.000% (1946/3589)      29/29 \n",
      " [============================>.] | Loss: 2.663 | Acc: 56.000% (2024/3589)      29/29 \n",
      "\n",
      "Epoch: 214\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.638% (28605/28709)    225/225 \n",
      " [============================>.] | Loss: 2.774 | Acc: 54.000% (1973/3589)      29/29 \n",
      " [============================>.] | Loss: 2.715 | Acc: 56.000% (2026/3589)      29/29 \n",
      "\n",
      "Epoch: 215\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.704% (28624/28709)    225/225 \n",
      " [============================>.] | Loss: 2.760 | Acc: 54.000% (1951/3589)      29/29 \n",
      " [============================>.] | Loss: 2.692 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 216\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.638% (28605/28709)    225/225 \n",
      " [============================>.] | Loss: 2.778 | Acc: 54.000% (1955/3589)      29/29 \n",
      " [============================>.] | Loss: 2.707 | Acc: 55.000% (2007/3589)      29/29 \n",
      "\n",
      "Epoch: 217\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.648% (28608/28709)    225/225 \n",
      " [============================>.] | Loss: 2.770 | Acc: 55.000% (1977/3589)      29/29 \n",
      " [============================>.] | Loss: 2.710 | Acc: 56.000% (2016/3589)      29/29 \n",
      "\n",
      "Epoch: 218\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.666% (28613/28709)    225/225 \n",
      " [============================>.] | Loss: 2.781 | Acc: 54.000% (1959/3589)      29/29 \n",
      " [============================>.] | Loss: 2.699 | Acc: 56.000% (2016/3589)      29/29 \n",
      "\n",
      "Epoch: 219\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.655% (28610/28709)    225/225 \n",
      " [============================>.] | Loss: 2.765 | Acc: 54.000% (1966/3589)      29/29 \n",
      " [============================>.] | Loss: 2.708 | Acc: 56.000% (2011/3589)      29/29 \n",
      "\n",
      "Epoch: 220\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.659% (28611/28709)    225/225 \n",
      " [============================>.] | Loss: 2.801 | Acc: 54.000% (1962/3589)      29/29 \n",
      " [============================>.] | Loss: 2.716 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 221\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.645% (28607/28709)    225/225 \n",
      " [============================>.] | Loss: 2.787 | Acc: 54.000% (1969/3589)      29/29 \n",
      " [============================>.] | Loss: 2.707 | Acc: 56.000% (2011/3589)      29/29 \n",
      "\n",
      "Epoch: 222\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.648% (28608/28709)    225/225 \n",
      " [============================>.] | Loss: 2.779 | Acc: 54.000% (1969/3589)      29/29 \n",
      " [============================>.] | Loss: 2.692 | Acc: 56.000% (2019/3589)      29/29 \n",
      "\n",
      "Epoch: 223\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.662% (28612/28709)    225/225 \n",
      " [============================>.] | Loss: 2.770 | Acc: 54.000% (1957/3589)      29/29 \n",
      " [============================>.] | Loss: 2.703 | Acc: 56.000% (2028/3589)      29/29 \n",
      "\n",
      "Epoch: 224\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.669% (28614/28709)    225/225 \n",
      " [============================>.] | Loss: 2.789 | Acc: 54.000% (1956/3589)      29/29 \n",
      " [============================>.] | Loss: 2.697 | Acc: 56.000% (2017/3589)      29/29 \n",
      "\n",
      "Epoch: 225\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.648% (28608/28709)    225/225 \n",
      " [============================>.] | Loss: 2.782 | Acc: 54.000% (1950/3589)      29/29 \n",
      " [============================>.] | Loss: 2.685 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 226\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.680% (28617/28709)    225/225 \n",
      " [============================>.] | Loss: 2.803 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.714 | Acc: 55.000% (2009/3589)      29/29 \n",
      "\n",
      "Epoch: 227\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.683% (28618/28709)    225/225 \n",
      " [============================>.] | Loss: 2.804 | Acc: 54.000% (1949/3589)      29/29 \n",
      " [============================>.] | Loss: 2.701 | Acc: 56.000% (2023/3589)      29/29 \n",
      "\n",
      "Epoch: 228\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.652% (28609/28709)    225/225 \n",
      " [============================>.] | Loss: 2.798 | Acc: 54.000% (1955/3589)      29/29 \n",
      " [============================>.] | Loss: 2.719 | Acc: 56.000% (2020/3589)      29/29 \n",
      "\n",
      "Epoch: 229\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.617% (28599/28709)    225/225 \n",
      " [============================>.] | Loss: 2.801 | Acc: 54.000% (1948/3589)      29/29 \n",
      " [============================>.] | Loss: 2.691 | Acc: 56.000% (2026/3589)      29/29 \n",
      "\n",
      "Epoch: 230\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.634% (28604/28709)    225/225 \n",
      " [============================>.] | Loss: 2.809 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.709 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 231\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.662% (28612/28709)    225/225 \n",
      " [============================>.] | Loss: 2.797 | Acc: 54.000% (1954/3589)      29/29 \n",
      " [============================>.] | Loss: 2.693 | Acc: 56.000% (2018/3589)      29/29 \n",
      "\n",
      "Epoch: 232\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.700% (28623/28709)    225/225 \n",
      " [============================>.] | Loss: 2.789 | Acc: 54.000% (1949/3589)      29/29 \n",
      " [============================>.] | Loss: 2.693 | Acc: 56.000% (2017/3589)      29/29 \n",
      "\n",
      "Epoch: 233\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.693% (28621/28709)    225/225 \n",
      " [============================>.] | Loss: 2.791 | Acc: 54.000% (1970/3589)      29/29 \n",
      " [============================>.] | Loss: 2.697 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 234\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.676% (28616/28709)    225/225 \n",
      " [============================>.] | Loss: 2.809 | Acc: 54.000% (1956/3589)      29/29 \n",
      " [============================>.] | Loss: 2.715 | Acc: 55.000% (2008/3589)      29/29 \n",
      "\n",
      "Epoch: 235\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.638% (28605/28709)    225/225 \n",
      " [============================>.] | Loss: 2.825 | Acc: 54.000% (1955/3589)      29/29 \n",
      " [============================>.] | Loss: 2.725 | Acc: 56.000% (2022/3589)      29/29 \n",
      "\n",
      "Epoch: 236\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.669% (28614/28709)    225/225 \n",
      " [============================>.] | Loss: 2.819 | Acc: 54.000% (1949/3589)      29/29 \n",
      " [============================>.] | Loss: 2.711 | Acc: 56.000% (2024/3589)      29/29 \n",
      "\n",
      "Epoch: 237\n",
      " [=============================>] | Loss: 0.011 | Acc: 99.707% (28625/28709)    225/225 \n",
      " [============================>.] | Loss: 2.804 | Acc: 54.000% (1945/3589)      29/29 \n",
      " [============================>.] | Loss: 2.699 | Acc: 56.000% (2014/3589)      29/29 \n",
      "\n",
      "Epoch: 238\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.655% (28610/28709)    225/225 \n",
      " [============================>.] | Loss: 2.819 | Acc: 54.000% (1940/3589)      29/29 \n",
      " [============================>.] | Loss: 2.718 | Acc: 56.000% (2014/3589)      29/29 \n",
      "\n",
      "Epoch: 239\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.676% (28616/28709)    225/225 \n",
      " [============================>.] | Loss: 2.812 | Acc: 54.000% (1951/3589)      29/29 \n",
      " [============================>.] | Loss: 2.717 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 240\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.700% (28623/28709)    225/225 \n",
      " [============================>.] | Loss: 2.801 | Acc: 54.000% (1965/3589)      29/29 \n",
      " [============================>.] | Loss: 2.716 | Acc: 56.000% (2020/3589)      29/29 \n",
      "\n",
      "Epoch: 241\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.700% (28623/28709)    225/225 \n",
      " [============================>.] | Loss: 2.807 | Acc: 54.000% (1965/3589)      29/29 \n",
      " [============================>.] | Loss: 2.720 | Acc: 56.000% (2025/3589)      29/29 \n",
      "\n",
      "Epoch: 242\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.690% (28620/28709)    225/225 \n",
      " [============================>.] | Loss: 2.819 | Acc: 54.000% (1942/3589)      29/29 \n",
      " [============================>.] | Loss: 2.721 | Acc: 56.000% (2013/3589)      29/29 \n",
      "\n",
      "Epoch: 243\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.676% (28616/28709)    225/225 \n",
      " [============================>.] | Loss: 2.785 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.695 | Acc: 56.000% (2021/3589)      29/29 \n",
      "\n",
      "Epoch: 244\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.753% (28638/28709)    225/225 \n",
      " [============================>.] | Loss: 2.806 | Acc: 54.000% (1964/3589)      29/29 \n",
      " [============================>.] | Loss: 2.704 | Acc: 56.000% (2024/3589)      29/29 \n",
      "\n",
      "Epoch: 245\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.697% (28622/28709)    225/225 \n",
      " [============================>.] | Loss: 2.829 | Acc: 54.000% (1958/3589)      29/29 \n",
      " [============================>.] | Loss: 2.724 | Acc: 56.000% (2016/3589)      29/29 \n",
      "\n",
      "Epoch: 246\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.690% (28620/28709)    225/225 \n",
      " [============================>.] | Loss: 2.826 | Acc: 54.000% (1959/3589)      29/29 \n",
      " [============================>.] | Loss: 2.733 | Acc: 56.000% (2032/3589)      29/29 \n",
      "\n",
      "Epoch: 247\n",
      " [=============================>] | Loss: 0.010 | Acc: 99.711% (28626/28709)    225/225 \n",
      " [============================>.] | Loss: 2.805 | Acc: 54.000% (1959/3589)      29/29 \n",
      " [============================>.] | Loss: 2.701 | Acc: 56.000% (2015/3589)      29/29 \n",
      "\n",
      "Epoch: 248\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.700% (28623/28709)    225/225 \n",
      " [============================>.] | Loss: 2.800 | Acc: 54.000% (1954/3589)      29/29 \n",
      " [============================>.] | Loss: 2.692 | Acc: 56.000% (2019/3589)      29/29 \n",
      "\n",
      "Epoch: 249\n",
      " [=============================>] | Loss: 0.009 | Acc: 99.697% (28622/28709)    225/225 \n",
      " [============================>.] | Loss: 2.811 | Acc: 54.000% (1964/3589)      29/29 \n",
      " [============================>.] | Loss: 2.718 | Acc: 56.000% (2013/3589)      29/29 \n",
      "best_PublicTest_acc: 55.000\n",
      "best_PublicTest_acc_epoch: 182\n",
      "best_PrivateTest_acc: 56.000\n",
      "best_PrivateTest_acc_epoch: 139\n"
     ]
    }
   ],
   "source": [
    "!python mainpro_FER_moblienetv2.py --model mobilenetv2 --bs 128 --lr 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/bb/771800366f41d66eef51e4b80515f8ef7edab234a3f244fdce3bafe89b39/scikit_image-0.16.2-cp36-cp36m-manylinux1_x86_64.whl (26.5MB)\n",
      "\u001b[K     || 26.5MB 20.4MB/s eta 0:00:01     |     | 22.1MB 20.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.1.1)\n",
      "Collecting imageio>=2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/de/f7f985018f462ceeffada7f6e609919fbcc934acd9301929cba14bc2c24a/imageio-2.6.1-py3-none-any.whl (3.3MB)\n",
      "\u001b[K     || 3.3MB 27.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "\u001b[K     || 1.6MB 14.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.3.0)\n",
      "Collecting PyWavelets>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/bb/d2b85265ec9fa3c1922210c9393d4cdf7075cc87cce6fe671d7455f80fbc/PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4MB)\n",
      "\u001b[K     || 4.4MB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (6.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (41.0.1)\n",
      "Installing collected packages: imageio, networkx, PyWavelets, scikit-image\n",
      "Successfully installed PyWavelets-1.1.1 imageio-2.6.1 networkx-2.4 scikit-image-0.16.2\n",
      "Traceback (most recent call last):\n",
      "  File \"test_visualization.py\", line 44, in <module>\n",
      "    net.load_state_dict(checkpoint['net'])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 845, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for MobileNet:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"dw2_1.conv_dw.weight\", \"dw2_1.bn_dw.weight\", \"dw2_1.bn_dw.bias\", \"dw2_1.bn_dw.running_mean\", \"dw2_1.bn_dw.running_var\", \"dw2_1.conv_sep.weight\", \"dw2_1.bn_sep.weight\", \"dw2_1.bn_sep.bias\", \"dw2_1.bn_sep.running_mean\", \"dw2_1.bn_sep.running_var\", \"dw2_2.conv_dw.weight\", \"dw2_2.bn_dw.weight\", \"dw2_2.bn_dw.bias\", \"dw2_2.bn_dw.running_mean\", \"dw2_2.bn_dw.running_var\", \"dw2_2.conv_sep.weight\", \"dw2_2.bn_sep.weight\", \"dw2_2.bn_sep.bias\", \"dw2_2.bn_sep.running_mean\", \"dw2_2.bn_sep.running_var\", \"dw3_1.conv_dw.weight\", \"dw3_1.bn_dw.weight\", \"dw3_1.bn_dw.bias\", \"dw3_1.bn_dw.running_mean\", \"dw3_1.bn_dw.running_var\", \"dw3_1.conv_sep.weight\", \"dw3_1.bn_sep.weight\", \"dw3_1.bn_sep.bias\", \"dw3_1.bn_sep.running_mean\", \"dw3_1.bn_sep.running_var\", \"dw3_2.conv_dw.weight\", \"dw3_2.bn_dw.weight\", \"dw3_2.bn_dw.bias\", \"dw3_2.bn_dw.running_mean\", \"dw3_2.bn_dw.running_var\", \"dw3_2.conv_sep.weight\", \"dw3_2.bn_sep.weight\", \"dw3_2.bn_sep.bias\", \"dw3_2.bn_sep.running_mean\", \"dw3_2.bn_sep.running_var\", \"dw4_1.conv_dw.weight\", \"dw4_1.bn_dw.weight\", \"dw4_1.bn_dw.bias\", \"dw4_1.bn_dw.running_mean\", \"dw4_1.bn_dw.running_var\", \"dw4_1.conv_sep.weight\", \"dw4_1.bn_sep.weight\", \"dw4_1.bn_sep.bias\", \"dw4_1.bn_sep.running_mean\", \"dw4_1.bn_sep.running_var\", \"dw4_2.conv_dw.weight\", \"dw4_2.bn_dw.weight\", \"dw4_2.bn_dw.bias\", \"dw4_2.bn_dw.running_mean\", \"dw4_2.bn_dw.running_var\", \"dw4_2.conv_sep.weight\", \"dw4_2.bn_sep.weight\", \"dw4_2.bn_sep.bias\", \"dw4_2.bn_sep.running_mean\", \"dw4_2.bn_sep.running_var\", \"dw5_1.conv_dw.weight\", \"dw5_1.bn_dw.weight\", \"dw5_1.bn_dw.bias\", \"dw5_1.bn_dw.running_mean\", \"dw5_1.bn_dw.running_var\", \"dw5_1.conv_sep.weight\", \"dw5_1.bn_sep.weight\", \"dw5_1.bn_sep.bias\", \"dw5_1.bn_sep.running_mean\", \"dw5_1.bn_sep.running_var\", \"dw5_2.conv_dw.weight\", \"dw5_2.bn_dw.weight\", \"dw5_2.bn_dw.bias\", \"dw5_2.bn_dw.running_mean\", \"dw5_2.bn_dw.running_var\", \"dw5_2.conv_sep.weight\", \"dw5_2.bn_sep.weight\", \"dw5_2.bn_sep.bias\", \"dw5_2.bn_sep.running_mean\", \"dw5_2.bn_sep.running_var\", \"dw5_3.conv_dw.weight\", \"dw5_3.bn_dw.weight\", \"dw5_3.bn_dw.bias\", \"dw5_3.bn_dw.running_mean\", \"dw5_3.bn_dw.running_var\", \"dw5_3.conv_sep.weight\", \"dw5_3.bn_sep.weight\", \"dw5_3.bn_sep.bias\", \"dw5_3.bn_sep.running_mean\", \"dw5_3.bn_sep.running_var\", \"dw5_4.conv_dw.weight\", \"dw5_4.bn_dw.weight\", \"dw5_4.bn_dw.bias\", \"dw5_4.bn_dw.running_mean\", \"dw5_4.bn_dw.running_var\", \"dw5_4.conv_sep.weight\", \"dw5_4.bn_sep.weight\", \"dw5_4.bn_sep.bias\", \"dw5_4.bn_sep.running_mean\", \"dw5_4.bn_sep.running_var\", \"dw5_5.conv_dw.weight\", \"dw5_5.bn_dw.weight\", \"dw5_5.bn_dw.bias\", \"dw5_5.bn_dw.running_mean\", \"dw5_5.bn_dw.running_var\", \"dw5_5.conv_sep.weight\", \"dw5_5.bn_sep.weight\", \"dw5_5.bn_sep.bias\", \"dw5_5.bn_sep.running_mean\", \"dw5_5.bn_sep.running_var\", \"dw5_6.conv_dw.weight\", \"dw5_6.bn_dw.weight\", \"dw5_6.bn_dw.bias\", \"dw5_6.bn_dw.running_mean\", \"dw5_6.bn_dw.running_var\", \"dw5_6.conv_sep.weight\", \"dw5_6.bn_sep.weight\", \"dw5_6.bn_sep.bias\", \"dw5_6.bn_sep.running_mean\", \"dw5_6.bn_sep.running_var\", \"dw6.conv_dw.weight\", \"dw6.bn_dw.weight\", \"dw6.bn_dw.bias\", \"dw6.bn_dw.running_mean\", \"dw6.bn_dw.running_var\", \"dw6.conv_sep.weight\", \"dw6.bn_sep.weight\", \"dw6.bn_sep.bias\", \"dw6.bn_sep.running_mean\", \"dw6.bn_sep.running_var\", \"fc.weight\", \"fc.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"features.0.0.weight\", \"features.0.1.weight\", \"features.0.1.bias\", \"features.0.1.running_mean\", \"features.0.1.running_var\", \"features.0.1.num_batches_tracked\", \"features.1.conv.0.weight\", \"features.1.conv.1.weight\", \"features.1.conv.1.bias\", \"features.1.conv.1.running_mean\", \"features.1.conv.1.running_var\", \"features.1.conv.1.num_batches_tracked\", \"features.1.conv.3.weight\", \"features.1.conv.4.weight\", \"features.1.conv.4.bias\", \"features.1.conv.4.running_mean\", \"features.1.conv.4.running_var\", \"features.1.conv.4.num_batches_tracked\", \"features.2.conv.0.weight\", \"features.2.conv.1.weight\", \"features.2.conv.1.bias\", \"features.2.conv.1.running_mean\", \"features.2.conv.1.running_var\", \"features.2.conv.1.num_batches_tracked\", \"features.2.conv.3.weight\", \"features.2.conv.4.weight\", \"features.2.conv.4.bias\", \"features.2.conv.4.running_mean\", \"features.2.conv.4.running_var\", \"features.2.conv.4.num_batches_tracked\", \"features.2.conv.6.weight\", \"features.2.conv.7.weight\", \"features.2.conv.7.bias\", \"features.2.conv.7.running_mean\", \"features.2.conv.7.running_var\", \"features.2.conv.7.num_batches_tracked\", \"features.3.conv.0.weight\", \"features.3.conv.1.weight\", \"features.3.conv.1.bias\", \"features.3.conv.1.running_mean\", \"features.3.conv.1.running_var\", \"features.3.conv.1.num_batches_tracked\", \"features.3.conv.3.weight\", \"features.3.conv.4.weight\", \"features.3.conv.4.bias\", \"features.3.conv.4.running_mean\", \"features.3.conv.4.running_var\", \"features.3.conv.4.num_batches_tracked\", \"features.3.conv.6.weight\", \"features.3.conv.7.weight\", \"features.3.conv.7.bias\", \"features.3.conv.7.running_mean\", \"features.3.conv.7.running_var\", \"features.3.conv.7.num_batches_tracked\", \"features.4.conv.0.weight\", \"features.4.conv.1.weight\", \"features.4.conv.1.bias\", \"features.4.conv.1.running_mean\", \"features.4.conv.1.running_var\", \"features.4.conv.1.num_batches_tracked\", \"features.4.conv.3.weight\", \"features.4.conv.4.weight\", \"features.4.conv.4.bias\", \"features.4.conv.4.running_mean\", \"features.4.conv.4.running_var\", \"features.4.conv.4.num_batches_tracked\", \"features.4.conv.6.weight\", \"features.4.conv.7.weight\", \"features.4.conv.7.bias\", \"features.4.conv.7.running_mean\", \"features.4.conv.7.running_var\", \"features.4.conv.7.num_batches_tracked\", \"features.5.conv.0.weight\", \"features.5.conv.1.weight\", \"features.5.conv.1.bias\", \"features.5.conv.1.running_mean\", \"features.5.conv.1.running_var\", \"features.5.conv.1.num_batches_tracked\", \"features.5.conv.3.weight\", \"features.5.conv.4.weight\", \"features.5.conv.4.bias\", \"features.5.conv.4.running_mean\", \"features.5.conv.4.running_var\", \"features.5.conv.4.num_batches_tracked\", \"features.5.conv.6.weight\", \"features.5.conv.7.weight\", \"features.5.conv.7.bias\", \"features.5.conv.7.running_mean\", \"features.5.conv.7.running_var\", \"features.5.conv.7.num_batches_tracked\", \"features.6.conv.0.weight\", \"features.6.conv.1.weight\", \"features.6.conv.1.bias\", \"features.6.conv.1.running_mean\", \"features.6.conv.1.running_var\", \"features.6.conv.1.num_batches_tracked\", \"features.6.conv.3.weight\", \"features.6.conv.4.weight\", \"features.6.conv.4.bias\", \"features.6.conv.4.running_mean\", \"features.6.conv.4.running_var\", \"features.6.conv.4.num_batches_tracked\", \"features.6.conv.6.weight\", \"features.6.conv.7.weight\", \"features.6.conv.7.bias\", \"features.6.conv.7.running_mean\", \"features.6.conv.7.running_var\", \"features.6.conv.7.num_batches_tracked\", \"features.7.conv.0.weight\", \"features.7.conv.1.weight\", \"features.7.conv.1.bias\", \"features.7.conv.1.running_mean\", \"features.7.conv.1.running_var\", \"features.7.conv.1.num_batches_tracked\", \"features.7.conv.3.weight\", \"features.7.conv.4.weight\", \"features.7.conv.4.bias\", \"features.7.conv.4.running_mean\", \"features.7.conv.4.running_var\", \"features.7.conv.4.num_batches_tracked\", \"features.7.conv.6.weight\", \"features.7.conv.7.weight\", \"features.7.conv.7.bias\", \"features.7.conv.7.running_mean\", \"features.7.conv.7.running_var\", \"features.7.conv.7.num_batches_tracked\", \"features.8.conv.0.weight\", \"features.8.conv.1.weight\", \"features.8.conv.1.bias\", \"features.8.conv.1.running_mean\", \"features.8.conv.1.running_var\", \"features.8.conv.1.num_batches_tracked\", \"features.8.conv.3.weight\", \"features.8.conv.4.weight\", \"features.8.conv.4.bias\", \"features.8.conv.4.running_mean\", \"features.8.conv.4.running_var\", \"features.8.conv.4.num_batches_tracked\", \"features.8.conv.6.weight\", \"features.8.conv.7.weight\", \"features.8.conv.7.bias\", \"features.8.conv.7.running_mean\", \"features.8.conv.7.running_var\", \"features.8.conv.7.num_batches_tracked\", \"features.9.conv.0.weight\", \"features.9.conv.1.weight\", \"features.9.conv.1.bias\", \"features.9.conv.1.running_mean\", \"features.9.conv.1.running_var\", \"features.9.conv.1.num_batches_tracked\", \"features.9.conv.3.weight\", \"features.9.conv.4.weight\", \"features.9.conv.4.bias\", \"features.9.conv.4.running_mean\", \"features.9.conv.4.running_var\", \"features.9.conv.4.num_batches_tracked\", \"features.9.conv.6.weight\", \"features.9.conv.7.weight\", \"features.9.conv.7.bias\", \"features.9.conv.7.running_mean\", \"features.9.conv.7.running_var\", \"features.9.conv.7.num_batches_tracked\", \"features.10.conv.0.weight\", \"features.10.conv.1.weight\", \"features.10.conv.1.bias\", \"features.10.conv.1.running_mean\", \"features.10.conv.1.running_var\", \"features.10.conv.1.num_batches_tracked\", \"features.10.conv.3.weight\", \"features.10.conv.4.weight\", \"features.10.conv.4.bias\", \"features.10.conv.4.running_mean\", \"features.10.conv.4.running_var\", \"features.10.conv.4.num_batches_tracked\", \"features.10.conv.6.weight\", \"features.10.conv.7.weight\", \"features.10.conv.7.bias\", \"features.10.conv.7.running_mean\", \"features.10.conv.7.running_var\", \"features.10.conv.7.num_batches_tracked\", \"features.11.conv.0.weight\", \"features.11.conv.1.weight\", \"features.11.conv.1.bias\", \"features.11.conv.1.running_mean\", \"features.11.conv.1.running_var\", \"features.11.conv.1.num_batches_tracked\", \"features.11.conv.3.weight\", \"features.11.conv.4.weight\", \"features.11.conv.4.bias\", \"features.11.conv.4.running_mean\", \"features.11.conv.4.running_var\", \"features.11.conv.4.num_batches_tracked\", \"features.11.conv.6.weight\", \"features.11.conv.7.weight\", \"features.11.conv.7.bias\", \"features.11.conv.7.running_mean\", \"features.11.conv.7.running_var\", \"features.11.conv.7.num_batches_tracked\", \"features.12.conv.0.weight\", \"features.12.conv.1.weight\", \"features.12.conv.1.bias\", \"features.12.conv.1.running_mean\", \"features.12.conv.1.running_var\", \"features.12.conv.1.num_batches_tracked\", \"features.12.conv.3.weight\", \"features.12.conv.4.weight\", \"features.12.conv.4.bias\", \"features.12.conv.4.running_mean\", \"features.12.conv.4.running_var\", \"features.12.conv.4.num_batches_tracked\", \"features.12.conv.6.weight\", \"features.12.conv.7.weight\", \"features.12.conv.7.bias\", \"features.12.conv.7.running_mean\", \"features.12.conv.7.running_var\", \"features.12.conv.7.num_batches_tracked\", \"features.13.conv.0.weight\", \"features.13.conv.1.weight\", \"features.13.conv.1.bias\", \"features.13.conv.1.running_mean\", \"features.13.conv.1.running_var\", \"features.13.conv.1.num_batches_tracked\", \"features.13.conv.3.weight\", \"features.13.conv.4.weight\", \"features.13.conv.4.bias\", \"features.13.conv.4.running_mean\", \"features.13.conv.4.running_var\", \"features.13.conv.4.num_batches_tracked\", \"features.13.conv.6.weight\", \"features.13.conv.7.weight\", \"features.13.conv.7.bias\", \"features.13.conv.7.running_mean\", \"features.13.conv.7.running_var\", \"features.13.conv.7.num_batches_tracked\", \"features.14.conv.0.weight\", \"features.14.conv.1.weight\", \"features.14.conv.1.bias\", \"features.14.conv.1.running_mean\", \"features.14.conv.1.running_var\", \"features.14.conv.1.num_batches_tracked\", \"features.14.conv.3.weight\", \"features.14.conv.4.weight\", \"features.14.conv.4.bias\", \"features.14.conv.4.running_mean\", \"features.14.conv.4.running_var\", \"features.14.conv.4.num_batches_tracked\", \"features.14.conv.6.weight\", \"features.14.conv.7.weight\", \"features.14.conv.7.bias\", \"features.14.conv.7.running_mean\", \"features.14.conv.7.running_var\", \"features.14.conv.7.num_batches_tracked\", \"features.15.conv.0.weight\", \"features.15.conv.1.weight\", \"features.15.conv.1.bias\", \"features.15.conv.1.running_mean\", \"features.15.conv.1.running_var\", \"features.15.conv.1.num_batches_tracked\", \"features.15.conv.3.weight\", \"features.15.conv.4.weight\", \"features.15.conv.4.bias\", \"features.15.conv.4.running_mean\", \"features.15.conv.4.running_var\", \"features.15.conv.4.num_batches_tracked\", \"features.15.conv.6.weight\", \"features.15.conv.7.weight\", \"features.15.conv.7.bias\", \"features.15.conv.7.running_mean\", \"features.15.conv.7.running_var\", \"features.15.conv.7.num_batches_tracked\", \"features.16.conv.0.weight\", \"features.16.conv.1.weight\", \"features.16.conv.1.bias\", \"features.16.conv.1.running_mean\", \"features.16.conv.1.running_var\", \"features.16.conv.1.num_batches_tracked\", \"features.16.conv.3.weight\", \"features.16.conv.4.weight\", \"features.16.conv.4.bias\", \"features.16.conv.4.running_mean\", \"features.16.conv.4.running_var\", \"features.16.conv.4.num_batches_tracked\", \"features.16.conv.6.weight\", \"features.16.conv.7.weight\", \"features.16.conv.7.bias\", \"features.16.conv.7.running_mean\", \"features.16.conv.7.running_var\", \"features.16.conv.7.num_batches_tracked\", \"features.17.conv.0.weight\", \"features.17.conv.1.weight\", \"features.17.conv.1.bias\", \"features.17.conv.1.running_mean\", \"features.17.conv.1.running_var\", \"features.17.conv.1.num_batches_tracked\", \"features.17.conv.3.weight\", \"features.17.conv.4.weight\", \"features.17.conv.4.bias\", \"features.17.conv.4.running_mean\", \"features.17.conv.4.running_var\", \"features.17.conv.4.num_batches_tracked\", \"features.17.conv.6.weight\", \"features.17.conv.7.weight\", \"features.17.conv.7.bias\", \"features.17.conv.7.running_mean\", \"features.17.conv.7.running_var\", \"features.17.conv.7.num_batches_tracked\", \"conv.0.weight\", \"conv.1.weight\", \"conv.1.bias\", \"conv.1.running_mean\", \"conv.1.running_var\", \"conv.1.num_batches_tracked\", \"classifier.weight\", \"classifier.bias\". \n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 3\n",
      "h: 32\n",
      "w: 32\n",
      "test_visualization.py:55: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(inputs, volatile=True)\n",
      "tensor([[ 1.8199, -3.2011, -0.0258,  1.0750,  0.2068, -0.0737,  0.0639]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "test_visualization.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  score = F.softmax(outputs[0])\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "!python test_visualization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (1.6.0)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.1)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.17.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (41.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%data : Float(1, 3, 32, 32),\n",
      "      %1 : Float(32, 3, 3, 3),\n",
      "      %2 : Float(32),\n",
      "      %3 : Float(32),\n",
      "      %4 : Float(32),\n",
      "      %5 : Float(32),\n",
      "      %6 : Long(),\n",
      "      %7 : Float(32, 1, 3, 3),\n",
      "      %8 : Float(32),\n",
      "      %9 : Float(32),\n",
      "      %10 : Float(32),\n",
      "      %11 : Float(32),\n",
      "      %12 : Long(),\n",
      "      %13 : Float(16, 32, 1, 1),\n",
      "      %14 : Float(16),\n",
      "      %15 : Float(16),\n",
      "      %16 : Float(16),\n",
      "      %17 : Float(16),\n",
      "      %18 : Long(),\n",
      "      %19 : Float(96, 16, 1, 1),\n",
      "      %20 : Float(96),\n",
      "      %21 : Float(96),\n",
      "      %22 : Float(96),\n",
      "      %23 : Float(96),\n",
      "      %24 : Long(),\n",
      "      %25 : Float(96, 1, 3, 3),\n",
      "      %26 : Float(96),\n",
      "      %27 : Float(96),\n",
      "      %28 : Float(96),\n",
      "      %29 : Float(96),\n",
      "      %30 : Long(),\n",
      "      %31 : Float(24, 96, 1, 1),\n",
      "      %32 : Float(24),\n",
      "      %33 : Float(24),\n",
      "      %34 : Float(24),\n",
      "      %35 : Float(24),\n",
      "      %36 : Long(),\n",
      "      %37 : Float(144, 24, 1, 1),\n",
      "      %38 : Float(144),\n",
      "      %39 : Float(144),\n",
      "      %40 : Float(144),\n",
      "      %41 : Float(144),\n",
      "      %42 : Long(),\n",
      "      %43 : Float(144, 1, 3, 3),\n",
      "      %44 : Float(144),\n",
      "      %45 : Float(144),\n",
      "      %46 : Float(144),\n",
      "      %47 : Float(144),\n",
      "      %48 : Long(),\n",
      "      %49 : Float(24, 144, 1, 1),\n",
      "      %50 : Float(24),\n",
      "      %51 : Float(24),\n",
      "      %52 : Float(24),\n",
      "      %53 : Float(24),\n",
      "      %54 : Long(),\n",
      "      %55 : Float(144, 24, 1, 1),\n",
      "      %56 : Float(144),\n",
      "      %57 : Float(144),\n",
      "      %58 : Float(144),\n",
      "      %59 : Float(144),\n",
      "      %60 : Long(),\n",
      "      %61 : Float(144, 1, 3, 3),\n",
      "      %62 : Float(144),\n",
      "      %63 : Float(144),\n",
      "      %64 : Float(144),\n",
      "      %65 : Float(144),\n",
      "      %66 : Long(),\n",
      "      %67 : Float(32, 144, 1, 1),\n",
      "      %68 : Float(32),\n",
      "      %69 : Float(32),\n",
      "      %70 : Float(32),\n",
      "      %71 : Float(32),\n",
      "      %72 : Long(),\n",
      "      %73 : Float(192, 32, 1, 1),\n",
      "      %74 : Float(192),\n",
      "      %75 : Float(192),\n",
      "      %76 : Float(192),\n",
      "      %77 : Float(192),\n",
      "      %78 : Long(),\n",
      "      %79 : Float(192, 1, 3, 3),\n",
      "      %80 : Float(192),\n",
      "      %81 : Float(192),\n",
      "      %82 : Float(192),\n",
      "      %83 : Float(192),\n",
      "      %84 : Long(),\n",
      "      %85 : Float(32, 192, 1, 1),\n",
      "      %86 : Float(32),\n",
      "      %87 : Float(32),\n",
      "      %88 : Float(32),\n",
      "      %89 : Float(32),\n",
      "      %90 : Long(),\n",
      "      %91 : Float(192, 32, 1, 1),\n",
      "      %92 : Float(192),\n",
      "      %93 : Float(192),\n",
      "      %94 : Float(192),\n",
      "      %95 : Float(192),\n",
      "      %96 : Long(),\n",
      "      %97 : Float(192, 1, 3, 3),\n",
      "      %98 : Float(192),\n",
      "      %99 : Float(192),\n",
      "      %100 : Float(192),\n",
      "      %101 : Float(192),\n",
      "      %102 : Long(),\n",
      "      %103 : Float(32, 192, 1, 1),\n",
      "      %104 : Float(32),\n",
      "      %105 : Float(32),\n",
      "      %106 : Float(32),\n",
      "      %107 : Float(32),\n",
      "      %108 : Long(),\n",
      "      %109 : Float(192, 32, 1, 1),\n",
      "      %110 : Float(192),\n",
      "      %111 : Float(192),\n",
      "      %112 : Float(192),\n",
      "      %113 : Float(192),\n",
      "      %114 : Long(),\n",
      "      %115 : Float(192, 1, 3, 3),\n",
      "      %116 : Float(192),\n",
      "      %117 : Float(192),\n",
      "      %118 : Float(192),\n",
      "      %119 : Float(192),\n",
      "      %120 : Long(),\n",
      "      %121 : Float(64, 192, 1, 1),\n",
      "      %122 : Float(64),\n",
      "      %123 : Float(64),\n",
      "      %124 : Float(64),\n",
      "      %125 : Float(64),\n",
      "      %126 : Long(),\n",
      "      %127 : Float(384, 64, 1, 1),\n",
      "      %128 : Float(384),\n",
      "      %129 : Float(384),\n",
      "      %130 : Float(384),\n",
      "      %131 : Float(384),\n",
      "      %132 : Long(),\n",
      "      %133 : Float(384, 1, 3, 3),\n",
      "      %134 : Float(384),\n",
      "      %135 : Float(384),\n",
      "      %136 : Float(384),\n",
      "      %137 : Float(384),\n",
      "      %138 : Long(),\n",
      "      %139 : Float(64, 384, 1, 1),\n",
      "      %140 : Float(64),\n",
      "      %141 : Float(64),\n",
      "      %142 : Float(64),\n",
      "      %143 : Float(64),\n",
      "      %144 : Long(),\n",
      "      %145 : Float(384, 64, 1, 1),\n",
      "      %146 : Float(384),\n",
      "      %147 : Float(384),\n",
      "      %148 : Float(384),\n",
      "      %149 : Float(384),\n",
      "      %150 : Long(),\n",
      "      %151 : Float(384, 1, 3, 3),\n",
      "      %152 : Float(384),\n",
      "      %153 : Float(384),\n",
      "      %154 : Float(384),\n",
      "      %155 : Float(384),\n",
      "      %156 : Long(),\n",
      "      %157 : Float(64, 384, 1, 1),\n",
      "      %158 : Float(64),\n",
      "      %159 : Float(64),\n",
      "      %160 : Float(64),\n",
      "      %161 : Float(64),\n",
      "      %162 : Long(),\n",
      "      %163 : Float(384, 64, 1, 1),\n",
      "      %164 : Float(384),\n",
      "      %165 : Float(384),\n",
      "      %166 : Float(384),\n",
      "      %167 : Float(384),\n",
      "      %168 : Long(),\n",
      "      %169 : Float(384, 1, 3, 3),\n",
      "      %170 : Float(384),\n",
      "      %171 : Float(384),\n",
      "      %172 : Float(384),\n",
      "      %173 : Float(384),\n",
      "      %174 : Long(),\n",
      "      %175 : Float(64, 384, 1, 1),\n",
      "      %176 : Float(64),\n",
      "      %177 : Float(64),\n",
      "      %178 : Float(64),\n",
      "      %179 : Float(64),\n",
      "      %180 : Long(),\n",
      "      %181 : Float(384, 64, 1, 1),\n",
      "      %182 : Float(384),\n",
      "      %183 : Float(384),\n",
      "      %184 : Float(384),\n",
      "      %185 : Float(384),\n",
      "      %186 : Long(),\n",
      "      %187 : Float(384, 1, 3, 3),\n",
      "      %188 : Float(384),\n",
      "      %189 : Float(384),\n",
      "      %190 : Float(384),\n",
      "      %191 : Float(384),\n",
      "      %192 : Long(),\n",
      "      %193 : Float(96, 384, 1, 1),\n",
      "      %194 : Float(96),\n",
      "      %195 : Float(96),\n",
      "      %196 : Float(96),\n",
      "      %197 : Float(96),\n",
      "      %198 : Long(),\n",
      "      %199 : Float(576, 96, 1, 1),\n",
      "      %200 : Float(576),\n",
      "      %201 : Float(576),\n",
      "      %202 : Float(576),\n",
      "      %203 : Float(576),\n",
      "      %204 : Long(),\n",
      "      %205 : Float(576, 1, 3, 3),\n",
      "      %206 : Float(576),\n",
      "      %207 : Float(576),\n",
      "      %208 : Float(576),\n",
      "      %209 : Float(576),\n",
      "      %210 : Long(),\n",
      "      %211 : Float(96, 576, 1, 1),\n",
      "      %212 : Float(96),\n",
      "      %213 : Float(96),\n",
      "      %214 : Float(96),\n",
      "      %215 : Float(96),\n",
      "      %216 : Long(),\n",
      "      %217 : Float(576, 96, 1, 1),\n",
      "      %218 : Float(576),\n",
      "      %219 : Float(576),\n",
      "      %220 : Float(576),\n",
      "      %221 : Float(576),\n",
      "      %222 : Long(),\n",
      "      %223 : Float(576, 1, 3, 3),\n",
      "      %224 : Float(576),\n",
      "      %225 : Float(576),\n",
      "      %226 : Float(576),\n",
      "      %227 : Float(576),\n",
      "      %228 : Long(),\n",
      "      %229 : Float(96, 576, 1, 1),\n",
      "      %230 : Float(96),\n",
      "      %231 : Float(96),\n",
      "      %232 : Float(96),\n",
      "      %233 : Float(96),\n",
      "      %234 : Long(),\n",
      "      %235 : Float(576, 96, 1, 1),\n",
      "      %236 : Float(576),\n",
      "      %237 : Float(576),\n",
      "      %238 : Float(576),\n",
      "      %239 : Float(576),\n",
      "      %240 : Long(),\n",
      "      %241 : Float(576, 1, 3, 3),\n",
      "      %242 : Float(576),\n",
      "      %243 : Float(576),\n",
      "      %244 : Float(576),\n",
      "      %245 : Float(576),\n",
      "      %246 : Long(),\n",
      "      %247 : Float(160, 576, 1, 1),\n",
      "      %248 : Float(160),\n",
      "      %249 : Float(160),\n",
      "      %250 : Float(160),\n",
      "      %251 : Float(160),\n",
      "      %252 : Long(),\n",
      "      %253 : Float(960, 160, 1, 1),\n",
      "      %254 : Float(960),\n",
      "      %255 : Float(960),\n",
      "      %256 : Float(960),\n",
      "      %257 : Float(960),\n",
      "      %258 : Long(),\n",
      "      %259 : Float(960, 1, 3, 3),\n",
      "      %260 : Float(960),\n",
      "      %261 : Float(960),\n",
      "      %262 : Float(960),\n",
      "      %263 : Float(960),\n",
      "      %264 : Long(),\n",
      "      %265 : Float(160, 960, 1, 1),\n",
      "      %266 : Float(160),\n",
      "      %267 : Float(160),\n",
      "      %268 : Float(160),\n",
      "      %269 : Float(160),\n",
      "      %270 : Long(),\n",
      "      %271 : Float(960, 160, 1, 1),\n",
      "      %272 : Float(960),\n",
      "      %273 : Float(960),\n",
      "      %274 : Float(960),\n",
      "      %275 : Float(960),\n",
      "      %276 : Long(),\n",
      "      %277 : Float(960, 1, 3, 3),\n",
      "      %278 : Float(960),\n",
      "      %279 : Float(960),\n",
      "      %280 : Float(960),\n",
      "      %281 : Float(960),\n",
      "      %282 : Long(),\n",
      "      %283 : Float(160, 960, 1, 1),\n",
      "      %284 : Float(160),\n",
      "      %285 : Float(160),\n",
      "      %286 : Float(160),\n",
      "      %287 : Float(160),\n",
      "      %288 : Long(),\n",
      "      %289 : Float(960, 160, 1, 1),\n",
      "      %290 : Float(960),\n",
      "      %291 : Float(960),\n",
      "      %292 : Float(960),\n",
      "      %293 : Float(960),\n",
      "      %294 : Long(),\n",
      "      %295 : Float(960, 1, 3, 3),\n",
      "      %296 : Float(960),\n",
      "      %297 : Float(960),\n",
      "      %298 : Float(960),\n",
      "      %299 : Float(960),\n",
      "      %300 : Long(),\n",
      "      %301 : Float(320, 960, 1, 1),\n",
      "      %302 : Float(320),\n",
      "      %303 : Float(320),\n",
      "      %304 : Float(320),\n",
      "      %305 : Float(320),\n",
      "      %306 : Long(),\n",
      "      %307 : Float(1280, 320, 1, 1),\n",
      "      %308 : Float(1280),\n",
      "      %309 : Float(1280),\n",
      "      %310 : Float(1280),\n",
      "      %311 : Float(1280),\n",
      "      %312 : Long(),\n",
      "      %313 : Float(7, 1280),\n",
      "      %314 : Float(7)):\n",
      "  %315 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%data, %1), scope: MobileNetV2/Sequential[features]/Sequential[0]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %316 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%315, %2, %3, %4, %5), scope: MobileNetV2/Sequential[features]/Sequential[0]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %317 : Float(1, 32, 16, 16) = onnx::Clip[max=6, min=0](%316), scope: MobileNetV2/Sequential[features]/Sequential[0]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %318 : Float(1, 32, 16, 16) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%317, %7), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %319 : Float(1, 32, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%318, %8, %9, %10, %11), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %320 : Float(1, 32, 16, 16) = onnx::Clip[max=6, min=0](%319), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %321 : Float(1, 16, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%320, %13), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %322 : Float(1, 16, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%321, %14, %15, %16, %17), scope: MobileNetV2/Sequential[features]/InvertedResidual[1]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %323 : Float(1, 96, 16, 16) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%322, %19), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %324 : Float(1, 96, 16, 16) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%323, %20, %21, %22, %23), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %325 : Float(1, 96, 16, 16) = onnx::Clip[max=6, min=0](%324), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %326 : Float(1, 96, 8, 8) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%325, %25), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %327 : Float(1, 96, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%326, %26, %27, %28, %29), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %328 : Float(1, 96, 8, 8) = onnx::Clip[max=6, min=0](%327), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %329 : Float(1, 24, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%328, %31), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %330 : Float(1, 24, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%329, %32, %33, %34, %35), scope: MobileNetV2/Sequential[features]/InvertedResidual[2]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %331 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%330, %37), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %332 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%331, %38, %39, %40, %41), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %333 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%332), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %334 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%333, %43), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %335 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%334, %44, %45, %46, %47), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %336 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%335), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %337 : Float(1, 24, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%336, %49), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %338 : Float(1, 24, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%337, %50, %51, %52, %53), scope: MobileNetV2/Sequential[features]/InvertedResidual[3]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %339 : Float(1, 24, 8, 8) = onnx::Add(%330, %338), scope: MobileNetV2/Sequential[features]/InvertedResidual[3] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %340 : Float(1, 144, 8, 8) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%339, %55), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %341 : Float(1, 144, 8, 8) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%340, %56, %57, %58, %59), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %342 : Float(1, 144, 8, 8) = onnx::Clip[max=6, min=0](%341), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %343 : Float(1, 144, 4, 4) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%342, %61), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %344 : Float(1, 144, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%343, %62, %63, %64, %65), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %345 : Float(1, 144, 4, 4) = onnx::Clip[max=6, min=0](%344), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %346 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%345, %67), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %347 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%346, %68, %69, %70, %71), scope: MobileNetV2/Sequential[features]/InvertedResidual[4]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %348 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%347, %73), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %349 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%348, %74, %75, %76, %77), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %350 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%349), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %351 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%350, %79), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %352 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%351, %80, %81, %82, %83), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %353 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%352), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %354 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%353, %85), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %355 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%354, %86, %87, %88, %89), scope: MobileNetV2/Sequential[features]/InvertedResidual[5]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %356 : Float(1, 32, 4, 4) = onnx::Add(%347, %355), scope: MobileNetV2/Sequential[features]/InvertedResidual[5] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %357 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%356, %91), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %358 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%357, %92, %93, %94, %95), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %359 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%358), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %360 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%359, %97), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %361 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%360, %98, %99, %100, %101), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %362 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%361), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %363 : Float(1, 32, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%362, %103), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %364 : Float(1, 32, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%363, %104, %105, %106, %107), scope: MobileNetV2/Sequential[features]/InvertedResidual[6]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %365 : Float(1, 32, 4, 4) = onnx::Add(%356, %364), scope: MobileNetV2/Sequential[features]/InvertedResidual[6] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %366 : Float(1, 192, 4, 4) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %109), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %367 : Float(1, 192, 4, 4) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%366, %110, %111, %112, %113), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %368 : Float(1, 192, 4, 4) = onnx::Clip[max=6, min=0](%367), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %369 : Float(1, 192, 2, 2) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%368, %115), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %370 : Float(1, 192, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%369, %116, %117, %118, %119), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %371 : Float(1, 192, 2, 2) = onnx::Clip[max=6, min=0](%370), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %372 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%371, %121), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %373 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%372, %122, %123, %124, %125), scope: MobileNetV2/Sequential[features]/InvertedResidual[7]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %374 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%373, %127), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %375 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%374, %128, %129, %130, %131), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %376 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%375), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %377 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%376, %133), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %378 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%377, %134, %135, %136, %137), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %379 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%378), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %380 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%379, %139), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %381 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%380, %140, %141, %142, %143), scope: MobileNetV2/Sequential[features]/InvertedResidual[8]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %382 : Float(1, 64, 2, 2) = onnx::Add(%373, %381), scope: MobileNetV2/Sequential[features]/InvertedResidual[8] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %383 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%382, %145), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %384 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%383, %146, %147, %148, %149), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %385 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%384), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %386 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%385, %151), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %387 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%386, %152, %153, %154, %155), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %388 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%387), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %389 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%388, %157), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %390 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%389, %158, %159, %160, %161), scope: MobileNetV2/Sequential[features]/InvertedResidual[9]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %391 : Float(1, 64, 2, 2) = onnx::Add(%382, %390), scope: MobileNetV2/Sequential[features]/InvertedResidual[9] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %392 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%391, %163), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %393 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%392, %164, %165, %166, %167), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %394 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%393), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %395 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%394, %169), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %396 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%395, %170, %171, %172, %173), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %397 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%396), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %398 : Float(1, 64, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%397, %175), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %399 : Float(1, 64, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%398, %176, %177, %178, %179), scope: MobileNetV2/Sequential[features]/InvertedResidual[10]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %400 : Float(1, 64, 2, 2) = onnx::Add(%391, %399), scope: MobileNetV2/Sequential[features]/InvertedResidual[10] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %401 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%400, %181), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %402 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%401, %182, %183, %184, %185), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %403 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%402), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %404 : Float(1, 384, 2, 2) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%403, %187), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %405 : Float(1, 384, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%404, %188, %189, %190, %191), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %406 : Float(1, 384, 2, 2) = onnx::Clip[max=6, min=0](%405), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %407 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%406, %193), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %408 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%407, %194, %195, %196, %197), scope: MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %409 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%408, %199), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %410 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%409, %200, %201, %202, %203), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %411 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%410), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %412 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%411, %205), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %413 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%412, %206, %207, %208, %209), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %414 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%413), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %415 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%414, %211), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %416 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%415, %212, %213, %214, %215), scope: MobileNetV2/Sequential[features]/InvertedResidual[12]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %417 : Float(1, 96, 2, 2) = onnx::Add(%408, %416), scope: MobileNetV2/Sequential[features]/InvertedResidual[12] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %418 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%417, %217), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %419 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%418, %218, %219, %220, %221), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %420 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%419), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %421 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%420, %223), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %422 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%421, %224, %225, %226, %227), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %423 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%422), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %424 : Float(1, 96, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%423, %229), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %425 : Float(1, 96, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%424, %230, %231, %232, %233), scope: MobileNetV2/Sequential[features]/InvertedResidual[13]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %426 : Float(1, 96, 2, 2) = onnx::Add(%417, %425), scope: MobileNetV2/Sequential[features]/InvertedResidual[13] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %427 : Float(1, 576, 2, 2) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%426, %235), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %428 : Float(1, 576, 2, 2) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%427, %236, %237, %238, %239), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %429 : Float(1, 576, 2, 2) = onnx::Clip[max=6, min=0](%428), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %430 : Float(1, 576, 1, 1) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%429, %241), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %431 : Float(1, 576, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%430, %242, %243, %244, %245), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %432 : Float(1, 576, 1, 1) = onnx::Clip[max=6, min=0](%431), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %433 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%432, %247), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %434 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%433, %248, %249, %250, %251), scope: MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %435 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%434, %253), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %436 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%435, %254, %255, %256, %257), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %437 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%436), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %438 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%437, %259), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %439 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%438, %260, %261, %262, %263), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %440 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%439), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %441 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%440, %265), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %442 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%441, %266, %267, %268, %269), scope: MobileNetV2/Sequential[features]/InvertedResidual[15]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %443 : Float(1, 160, 1, 1) = onnx::Add(%434, %442), scope: MobileNetV2/Sequential[features]/InvertedResidual[15] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %444 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%443, %271), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %445 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%444, %272, %273, %274, %275), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %446 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%445), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %447 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%446, %277), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %448 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%447, %278, %279, %280, %281), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %449 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%448), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %450 : Float(1, 160, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %283), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %451 : Float(1, 160, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%450, %284, %285, %286, %287), scope: MobileNetV2/Sequential[features]/InvertedResidual[16]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %452 : Float(1, 160, 1, 1) = onnx::Add(%443, %451), scope: MobileNetV2/Sequential[features]/InvertedResidual[16] # /storage/expression_recognition/mobilenetv2.py:86:0\n",
      "  %453 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%452, %289), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %454 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%453, %290, %291, %292, %293), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %455 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%454), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %456 : Float(1, 960, 1, 1) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%455, %295), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %457 : Float(1, 960, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%456, %296, %297, %298, %299), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %458 : Float(1, 960, 1, 1) = onnx::Clip[max=6, min=0](%457), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/ReLU6[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %459 : Float(1, 320, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%458, %301), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/Conv2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %460 : Float(1, 320, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%459, %302, %303, %304, %305), scope: MobileNetV2/Sequential[features]/InvertedResidual[17]/Sequential[conv]/BatchNorm2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %461 : Float(1, 1280, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%460, %307), scope: MobileNetV2/Sequential[conv]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %462 : Float(1, 1280, 1, 1) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%461, %308, %309, %310, %311), scope: MobileNetV2/Sequential[conv]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %463 : Float(1, 1280, 1, 1) = onnx::Clip[max=6, min=0](%462), scope: MobileNetV2/Sequential[conv]/ReLU6[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:957:0\n",
      "  %464 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%463), scope: MobileNetV2/AvgPool2d[avgpool]\n",
      "  %465 : Float(1, 1280, 1, 1) = onnx::AveragePool[kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%464), scope: MobileNetV2/AvgPool2d[avgpool] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py:551:0\n",
      "  %466 : Long() = onnx::Constant[value={0}](), scope: MobileNetV2\n",
      "  %467 : Tensor = onnx::Shape(%465), scope: MobileNetV2\n",
      "  %468 : Long() = onnx::Gather[axis=0](%467, %466), scope: MobileNetV2 # /storage/expression_recognition/mobilenetv2.py:132:0\n",
      "  %469 : Long() = onnx::Constant[value={-1}](), scope: MobileNetV2\n",
      "  %470 : Tensor = onnx::Unsqueeze[axes=[0]](%468)\n",
      "  %471 : Tensor = onnx::Unsqueeze[axes=[0]](%469)\n",
      "  %472 : Tensor = onnx::Concat[axis=0](%470, %471)\n",
      "  %473 : Float(1, 1280) = onnx::Reshape(%465, %472), scope: MobileNetV2 # /storage/expression_recognition/mobilenetv2.py:132:0\n",
      "  %outTensor : Float(1, 7) = onnx::Gemm[alpha=1, beta=1, transB=1](%473, %313, %314), scope: MobileNetV2/Linear[classifier] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1369:0\n",
      "  return (%outTensor)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#onnx\n",
    "\"\"\"\n",
    "visualize results for test image\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "raw_img = io.imread('images/1.jpg')\n",
    "gray = rgb2gray(raw_img)\n",
    "gray = resize(gray, (48,48), mode='symmetric').astype(np.uint8)\n",
    "\n",
    "img = gray[:, :, np.newaxis]\n",
    "\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "#\n",
    "net = mobilenetv2(num_classes=7,input_size=32)\n",
    "checkpoint = torch.load(os.path.join('FER2013_mobilenetv2', 'PublicTest_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.train(False)\n",
    "\n",
    "#input\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "#\n",
    "torch_out = torch.onnx._export(net,  # model being run\n",
    "                               inputs,  # model input (or a tuple for multiple inputs)\n",
    "                               \"FER2013_mobilenetv2_privateTest.onnx\",  # where to save the model\n",
    "                               verbose=True,\n",
    "                               input_names=['data'], \n",
    "                               output_names=['outTensor'], \n",
    "                               export_params=True, \n",
    "                               training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n",
      "[[ 1.977627   -3.144173   -0.0559044   1.0978713   0.11394721 -0.25046518\n",
      "   0.13199198]]\n",
      "     0.507\n",
      "     0.003\n",
      "     0.066\n",
      "     0.210\n",
      "     0.079\n",
      "     0.055\n",
      "     0.080\n",
      "tensor(0.5070)\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "# onnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"FER2013_mobilenetv2_privateTest.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "raw_img = io.imread('images/1_gray.jpg')\n",
    "img = Image.fromarray(raw_img)\n",
    "inputs = transform_test(img)\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "print(inputs.numpy().shape)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "predicted = torch.max(score.data)\n",
    "print(predicted)\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[int(predicted.cpu().numpy())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx-simplifier in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (3.9.1)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.6.0)\n",
      "Requirement already satisfied: onnxruntime>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.0.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.7.0->onnx-simplifier) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.7.0->onnx-simplifier) (41.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (3.7.4.1)\n",
      "Simplifying...\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx-simplifier\n",
    "# onnx\n",
    "!python -m onnxsim \"FER2013_mobilenetv2_privateTest.onnx\" \"FER2013_mobilenetv2_privateTest_sim.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n",
      "[[ 1.9776292  -3.1441724  -0.05590643  1.0978684   0.11394835 -0.25046366\n",
      "   0.13199115]]\n",
      "     0.507\n",
      "     0.003\n",
      "     0.066\n",
      "     0.210\n",
      "     0.079\n",
      "     0.055\n",
      "     0.080\n",
      "tensor(0.5070)\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "#  sim onnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "from mobilenetv2 import mobilenetv2\n",
    "\n",
    "cut_size = 32\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(cut_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"FER2013_mobilenetv2_privateTest_sim.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "raw_img = io.imread('images/1_gray.jpg')\n",
    "img = Image.fromarray(raw_img)\n",
    "inputs = transform_test(img)\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "print(inputs.numpy().shape)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "predicted = torch.max(score.data)\n",
    "print(predicted)\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[int(predicted.cpu().numpy())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: onnx-coreml in /usr/local/lib/python3.6/dist-packages (1.0)\n",
      "Requirement already satisfied, skipping upgrade: onnx==1.5.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: coremltools==3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.0)\n",
      "Requirement already satisfied, skipping upgrade: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx==1.5.0->onnx-coreml) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from onnx==1.5.0->onnx-coreml) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx==1.5.0->onnx-coreml) (41.0.1)\n",
      "1/100: Converting Node Type Conv\n",
      "2/100: Converting Node Type Clip\n",
      "3/100: Converting Node Type Conv\n",
      "4/100: Converting Node Type Clip\n",
      "5/100: Converting Node Type Conv\n",
      "6/100: Converting Node Type Conv\n",
      "7/100: Converting Node Type Clip\n",
      "8/100: Converting Node Type Conv\n",
      "9/100: Converting Node Type Clip\n",
      "10/100: Converting Node Type Conv\n",
      "11/100: Converting Node Type Conv\n",
      "12/100: Converting Node Type Clip\n",
      "13/100: Converting Node Type Conv\n",
      "14/100: Converting Node Type Clip\n",
      "15/100: Converting Node Type Conv\n",
      "16/100: Converting Node Type Add\n",
      "17/100: Converting Node Type Conv\n",
      "18/100: Converting Node Type Clip\n",
      "19/100: Converting Node Type Conv\n",
      "20/100: Converting Node Type Clip\n",
      "21/100: Converting Node Type Conv\n",
      "22/100: Converting Node Type Conv\n",
      "23/100: Converting Node Type Clip\n",
      "24/100: Converting Node Type Conv\n",
      "25/100: Converting Node Type Clip\n",
      "26/100: Converting Node Type Conv\n",
      "27/100: Converting Node Type Add\n",
      "28/100: Converting Node Type Conv\n",
      "29/100: Converting Node Type Clip\n",
      "30/100: Converting Node Type Conv\n",
      "31/100: Converting Node Type Clip\n",
      "32/100: Converting Node Type Conv\n",
      "33/100: Converting Node Type Add\n",
      "34/100: Converting Node Type Conv\n",
      "35/100: Converting Node Type Clip\n",
      "36/100: Converting Node Type Conv\n",
      "37/100: Converting Node Type Clip\n",
      "38/100: Converting Node Type Conv\n",
      "39/100: Converting Node Type Conv\n",
      "40/100: Converting Node Type Clip\n",
      "41/100: Converting Node Type Conv\n",
      "42/100: Converting Node Type Clip\n",
      "43/100: Converting Node Type Conv\n",
      "44/100: Converting Node Type Add\n",
      "45/100: Converting Node Type Conv\n",
      "46/100: Converting Node Type Clip\n",
      "47/100: Converting Node Type Conv\n",
      "48/100: Converting Node Type Clip\n",
      "49/100: Converting Node Type Conv\n",
      "50/100: Converting Node Type Add\n",
      "51/100: Converting Node Type Conv\n",
      "52/100: Converting Node Type Clip\n",
      "53/100: Converting Node Type Conv\n",
      "54/100: Converting Node Type Clip\n",
      "55/100: Converting Node Type Conv\n",
      "56/100: Converting Node Type Add\n",
      "57/100: Converting Node Type Conv\n",
      "58/100: Converting Node Type Clip\n",
      "59/100: Converting Node Type Conv\n",
      "60/100: Converting Node Type Clip\n",
      "61/100: Converting Node Type Conv\n",
      "62/100: Converting Node Type Conv\n",
      "63/100: Converting Node Type Clip\n",
      "64/100: Converting Node Type Conv\n",
      "65/100: Converting Node Type Clip\n",
      "66/100: Converting Node Type Conv\n",
      "67/100: Converting Node Type Add\n",
      "68/100: Converting Node Type Conv\n",
      "69/100: Converting Node Type Clip\n",
      "70/100: Converting Node Type Conv\n",
      "71/100: Converting Node Type Clip\n",
      "72/100: Converting Node Type Conv\n",
      "73/100: Converting Node Type Add\n",
      "74/100: Converting Node Type Conv\n",
      "75/100: Converting Node Type Clip\n",
      "76/100: Converting Node Type Conv\n",
      "77/100: Converting Node Type Clip\n",
      "78/100: Converting Node Type Conv\n",
      "79/100: Converting Node Type Conv\n",
      "80/100: Converting Node Type Clip\n",
      "81/100: Converting Node Type Conv\n",
      "82/100: Converting Node Type Clip\n",
      "83/100: Converting Node Type Conv\n",
      "84/100: Converting Node Type Add\n",
      "85/100: Converting Node Type Conv\n",
      "86/100: Converting Node Type Clip\n",
      "87/100: Converting Node Type Conv\n",
      "88/100: Converting Node Type Clip\n",
      "89/100: Converting Node Type Conv\n",
      "90/100: Converting Node Type Add\n",
      "91/100: Converting Node Type Conv\n",
      "92/100: Converting Node Type Clip\n",
      "93/100: Converting Node Type Conv\n",
      "94/100: Converting Node Type Clip\n",
      "95/100: Converting Node Type Conv\n",
      "96/100: Converting Node Type Conv\n",
      "97/100: Converting Node Type Clip\n",
      "98/100: Converting Node Type AveragePool\n",
      "99/100: Converting Node Type Reshape\n",
      "100/100: Converting Node Type Gemm\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U onnx-coreml\n",
    "#corml\n",
    "import onnx;\n",
    "from onnx_coreml import convert\n",
    "\n",
    "onnx_model = onnx.load(\"FER2013_mobilenetv2_privateTest_sim.onnx\")\n",
    "cml_model= convert(onnx_model,image_input_names='data',target_ios='13')\n",
    "cml_model.save(\"FER2013_mobilenetv2_privateTest_sim_13.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
